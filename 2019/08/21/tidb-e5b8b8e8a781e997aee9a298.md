---
title: 'TiDB 常见问题'
date: '2019-08-21T00:08:18+00:00'
status: publish
permalink: /2019/08/21/tidb-%e5%b8%b8%e8%a7%81%e9%97%ae%e9%a2%98
author: 毛巳煜
excerpt: ''
type: post
id: 5003
category:
    - TiDB
tag: []
post_format: []
hestia_layout_select:
    - sidebar-right
---
#### **`常见问题`**

##### 1. 查看TiDB版本 查看使用的Golang版本

```sql
MySQL [(none)]> select tidb_version()\G
*************************** 1. row ***************************
tidb_version(): Release Version: v3.0.5
Git Commit Hash: c9000abdc216b6a02efbcc578af8be1f98ba280d
Git Branch: HEAD
UTC Build Time: 2019-10-25 03:25:32
GoVersion: go version go1.13 linux/amd64
Race Enabled: false
TiKV Min Version: v3.0.0-60965b006877ca7234adaced7890d7b029ed1306
Check Table Before Drop: false
1 row in set (0.00 sec)

MySQL [(none)]>

```

- - - - - -

- - - - - -

- - - - - -

##### 2. region leader 也就是raft leader 这么理解是对的吧

**`答：`** 对的

- - - - - -

- - - - - -

- - - - - -

##### 3. PD 不可以两台吗

**`答：`** 少于两台PD是可以正常运行的，但是 PD 本身也有一个raft选举过程，两个节点没法实现; 滚动更新时会引发`HTTP Error 503 : Service Unavailable`

- - - - - -

- - - - - -

- - - - - -

##### 4. 修改TiDB默认端口

`修改端口`

```ruby
[tidb@dev10 tidb-ansible]<span class="katex math inline">vim inventory.ini
[tidb_servers]
172.160.180.33
172.160.180.34
# 加入新端口号
172.160.180.53 tidb_port=4600
......
[tidb@dev10 tidb-ansible]</span>

# 修改配置
[tidb@dev10 tidb-ansible]<span class="katex math inline">ansible-playbook deploy.yml --tags=tidb -l 172.160.180.53
# 滚动更新
[tidb@dev10 tidb-ansible]</span> ansible-playbook rolling_update.yml --tags=tidb -l 172.160.180.53

```

- - - - - -

- - - - - -

- - - - - -

##### 5. 配置TiDB 单机多KV

```ruby
[tidb@dev10 tidb-ansible]$ vim inventory.ini

......
[tikv_servers]
# 部署 3.0 版本的 TiDB 集群时，多实例场景需要额外配置 status 端口，示例如下：
TiKV1-1 ansible_host=172.160.180.52 deploy_dir=/home/tidb/data1/deploy tikv_port=20171 tikv_status_port=20181 labels="host=tikv1"
TiKV1-2 ansible_host=172.160.180.52 deploy_dir=/home/tidb/data2/deploy tikv_port=20172 tikv_status_port=20182 labels="host=tikv1"
TiKV1-3 ansible_host=172.160.180.52 deploy_dir=/home/tidb/data3/deploy tikv_port=20173 tikv_status_port=20183 labels="host=tikv1"
......


```

- - - - - -

- - - - - -

- - - - - -

###### 6. 还需要设置 tikv 配置文件

**路径：`/home/tidb/tidb-ansible/conf/tikv.yml`**  
修改配置文件需要重新 deploy

```yml
<br></br>......

readpool:
  storage:
  coprocessor:
    # TiKV 实例数量 * 参数值 = CPU 核心数量 * 0.8
    #
    # 8Core * 0.8 / 3TiKV
    #
    # high-concurrency: 8
    # normal-concurrency: 8
    # low-concurrency: 8
    #
    high-concurrency: 2
    normal-concurrency: 2
    low-concurrency: 2

......

storage:
  block-cache:
    # capacity = MEM_TOTAL * 0.5 / TiKV 实例数量
    # 16G * 0.5 / 3台TiKV
    # capacity: "1GB"
    capacity: "2GB"

......

raftstore:
  # capacity = 磁盘总容量 / TiKV 实例数量，例如：capacity: "100GB"
  #
  # 1024G / 3TiKV
  #
  # capacity: 0
  capacity: "340GB"

......


```

- - - - - -

- - - - - -

- - - - - -

##### 7. 垃圾回收时间

tikv\_gc\_run\_interval 是 GC 运行时间间隔。  
tikv\_gc\_life\_time 是历史版本的保留时间，每次进行 GC 时，会清理超过该时间的历史数据。  
官方建议：这两项配置不应低于 10 分钟，默认值均为 10 分钟。  
垃圾回收十分钟执行一次，回收的是十分钟之前的数据，要跑第二次才会回收刚删掉的数据，这么算来每次至少都需要等 20分钟才会释放硬盘空间。

可能过如下命令动态设置回收时间

```sql
update mysql.tidb set variable_value='10m0s' where variable_name='tikv_gc_life_time';
SELECT variable_value FROM mysql.tidb  where variable_name='tikv_gc_life_time';

```

- - - - - -

- - - - - -

- - - - - -

###### 8. **TiDB3.0.2同步 MariaDB 数据超长引发 binglog同步失败，解决方案如下：**

**注意：**

- 任何情况下都不要直接 kill Pump或Drainer  
  -一但数据同步失败 Drainer就会进入 paused (暂停状态)

**8.1** 修改TiDB提示的错误  
例如：删除某个超长的列，然后重新创建一个与MySQL长度合理的列。 删除表时不要忘记备份数据  
查看 `drainer.log` 找到发生异常的时间点

```ruby
[tidb@dev11 ~]$ tail -1000f /home/tidb/deploy/log/drainer.log
......
[2019/08/22 11:27:21.586 +08:00] [INFO] [syncer.go:254] ["write save point"] [ts=410634011353284609]
[2019/08/22 11:27:26.397 +08:00] [INFO] [syncer.go:254] ["write save point"] [ts=410634012205252609]
[2019/08/22 11:27:30.608 +08:00] [ERROR] [executor.go:85] ["exec fail"] [query="DELETE FROM `dev2_pfizer_activiti`.`ACT_RU_JOB` WHERE `ID_` = ? LIMIT 1"] [args="[\"ZTA4MjMyZDAtYzNmYS0xMWU5LWJkZjEtMDY4YWEwOWI2NzQy\"]"] [error="Error 1213: Deadlock found when trying to get lock; try restarting transaction"]
......
# 异常的发生时间为  [2019/08/22 11:27:30.608 +08:00]

```

- - - - - -

**8.2** 根据 异常的发生时间 **\[2019/08/22 11:27:3**0.608 +08:00\] 到 `pump.log` 找到这个时间相近的 `下一个 MaxCommitTS的值`

```ruby
[tidb@dev11 ~]$ vim /home/tidb/deploy/log/pump.log
# 在vim中查找
/[2019/08/22 11:27:3
# 根据时间的定位，最相近的下一个 TS值是  [MaxCommitTS=410634015285444609]
[2019/08/22 11:27:36.411 +08:00] [INFO] [storage.go:381] [DBStats] [DBStats="{\"WriteDelayCount\":0,\"WriteDelayDuration\":0,\"WritePaused\":false,\"AliveSnapshots\":0,\"AliveIterators\":0,\"IOWrite\":5790744,\"IORead\":0,\"BlockCacheSize\":0,\"OpenedTablesCount\":0,\"LevelSizes\":null,\"LevelTablesCounts\":null,\"LevelRead\":null,\"LevelWrite\":null,\"LevelDurations\":null}"]
[2019/08/22 11:27:36.445 +08:00] [INFO] [server.go:522] ["server info tick"] [writeBinlogCount=8932] [alivePullerCount=3] [MaxCommitTS=410634015285444609]
[2019/08/22 11:27:46.411 +08:00] [INFO] [storage.go:381] [DBStats] [DBStats="{\"WriteDelayCount\":0,\"WriteDelayDuration\":0,\"WritePaused\":false,\"AliveSnapshots\":0,\"AliveIterators\":0,\"IOWrite\":5791236,\"IORead\":0,\"BlockCacheSize\":0,\"OpenedTablesCount\":0,\"LevelSizes\":null,\"LevelTablesCounts\":null,\"LevelRead\":null,\"LevelWrite\":null,\"LevelDurations\":null}"]
[2019/08/22 11:27:46.445 +08:00] [INFO] [server.go:522] ["server info tick"] [writeBinlogCount=8932] [alivePullerCount=3] [MaxCommitTS=410634017644740612]

```

- - - - - -

**8.3** 修改下游数据库中的CommitTS

```ruby
MariaDB [(none)]> show databases;
+-----------------------------+
| Database                    |
+-----------------------------+ |
| information_schema          |
| mysql                       |
| performance_schema          |
| tidb_binlog                 |
| tidb_loader                 |
+-----------------------------+
22 rows in set (0.00 sec)

MariaDB [(none)]>
MariaDB [(none)]> use tidb_binlog;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
MariaDB [tidb_binlog]>
MariaDB [tidb_binlog]>
MariaDB [tidb_binlog]> show tables;
+-----------------------+
| Tables_in_tidb_binlog |
+-----------------------+
| checkpoint            |
+-----------------------+
1 row in set (0.00 sec)

# 查询发生异常时的最后一个 commitTS值
MariaDB [tidb_binlog]> select * from checkpoint;
+---------------------+---------------------------------------------+
| clusterID           | checkPoint                                  |
+---------------------+---------------------------------------------+
| 6697082055159617667 | {"commitTS":410636899695198209,"ts-map":{}} |
+---------------------+---------------------------------------------+
1 row in set (0.00 sec)

MariaDB [tidb_binlog]>

# 修改commitTS值，跳过会引发异常的 binlog
MariaDB [tidb_binlog]>
    UPDATE checkpoint
      SET checkPoint = '{"commitTS":410634015285444609,"ts-map":{}}'
      WHERE
      clusterID = '6697082055159617667';
Query OK, 1 row affected (0.10 sec)
Rows matched: 1  Changed: 1  Warnings: 0

MariaDB [tidb_binlog]>


```

- - - - - -

**8.4** 启动 drainer

```ruby
[tidb@test1 tidb-ansible]<span class="katex math inline">ansible-playbook start_drainer.yml

Congrats! All goes well. :-)
[tidb@test1 tidb-ansible]</span>

```

- - - - - -

`注意：`

```
如果因为操作pump不得当导致 TiDB客户端无法连接，需要重新执行如下语句进行无感知更新
ansible-playbook rolling_update.yml --tags=tidb

```

`注意：`

```
任何情况下都不要直接  kill  Pump或Drainer

```

`注意：`

```
drainer发生异常后会自动切换为 paused 状态，要恢复为 online状态只需要重新执行一次语句即可
nohup ./bin/drainer -config config/drainer.toml &

```

- - - - - -

- - - - - -

- - - - - -

###### 9. 重新部署容器后异常 这个问题是 pump 没启动 就设置了 开启binlog导致的

```ruby
[tidb@test1 tidb-ansible]$ ansible-playbook start.yml
fatal: [172.160.180.46]: FAILED! => {"changed": false, "elapsed": 300, "msg": "the TiDB port 4000 is not up"}
PLAY [grafana_servers] **************************************************************************************************************************************************************************
        to retry, use: --limit @/home/tidb/tidb-ansible/retry_files/start.retry

PLAY RECAP **************************************************************************************************************************************************************************************
172.160.180.46             : ok=27   changed=0    unreachable=0    failed=1
172.160.180.47             : ok=10   changed=0    unreachable=0    failed=1
172.160.180.48             : ok=13   changed=0    unreachable=0    failed=1
localhost                  : ok=7    changed=4    unreachable=0    failed=0


ERROR MESSAGE SUMMARY ***************************************************************************************************************************************************************************
[172.160.180.47]: Ansible FAILED! => playbook: start.yml; TASK: wait until the PD port is up; message: {"changed": false, "elapsed": 300, "msg": "the PD port 2379 is not up"}

[172.160.180.48]: Ansible FAILED! => playbook: start.yml; TASK: wait until the TiKV port is up; message: {"changed": false, "elapsed": 300, "msg": "the TiKV port 20160 is not up"}

[172.160.180.46]: Ansible FAILED! => playbook: start.yml; TASK: wait until the TiDB port is up; message: {"changed": false, "elapsed": 300, "msg": "the TiDB port 4000 is not up"}

```

- - - - - -

- - - - - -

- - - - - -

##### 10. tidb log 异常信息 listener stopped, waiting for manual kill

```ruby
[ERROR] [server.go:364] ["listener stopped, waiting for manual kill."]  # 原因是 开启binlog必须先开启 pump, 这个错误是因为 pump下线了导致的
[stack="github.com/pingcap/tidb/server.(*Server).Run\n\t/home/jenkins/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb/server/server.go:364\nmain.runServer\n\t/home/jenkins/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb/tidb-server/main.go:568\nmain.main\n\t/home/jenkins/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb/tidb-server/main.go:174\nruntime.main\n\t/usr/local/go/src/runtime/proc.go:200"]

```

- - - - - -

- - - - - -

- - - - - -

##### 11. drainer log 异常信息 got signal to exit

```ruby
[2019/08/20 20:56:27.850 +08:00] [INFO] [syncer.go:254] ["write save point"] [ts=410597664302825480]
[2019/08/20 20:56:32.301 +08:00] [INFO] [syncer.go:254] ["write save point"] [ts=410597665574223873]
[2019/08/20 20:56:36.537 +08:00] [INFO] [syncer.go:254] ["write save point"] [ts=410597666688335873]
[2019/08/20 20:56:40.817 +08:00] [INFO] [syncer.go:254] ["write save point"] [ts=410597667802447873]
[2019/08/20 20:56:45.456 +08:00] [INFO] [syncer.go:254] ["write save point"] [ts=410597669047631873]
[2019/08/20 20:56:49.539 +08:00] [INFO] [main.go:63] ["got signal to exit."] [signal=hangup] # 原因是使用 nohup 启动，但是客户连接是非正常使用 exit 断开，导致系统杀掉了 drainer 进程。
[2019/08/20 20:56:49.539 +08:00] [INFO] [server.go:406] ["begin to close drainer server"]
[2019/08/20 20:56:49.539 +08:00] [ERROR] [pump.go:147] ["pump receive binlog failed"] [id=dev10:8250] [error=EOF]
[2019/08/20 20:56:49.543 +08:00] [INFO] [server.go:371] ["has already update status"] [id=dev10:8249]
[2019/08/20 20:56:49.543 +08:00] [INFO] [server.go:410] ["commit status done"]
[2019/08/20 20:56:49.543 +08:00] [INFO] [collector.go:130] ["publishBinlogs quit"]
[2019/08/20 20:56:49.544 +08:00] [INFO] [util.go:66] [Exit] [name=heartbeat]
[2019/08/20 20:56:49.544 +08:00] [INFO] [pump.go:72] ["pump is closing"] [id=dev10:8250]
[2019/08/20 20:56:49.544 +08:00] [INFO] [syncer.go:254] ["write save point"] [ts=410597669244239873]
[2019/08/20 20:56:49.544 +08:00] [INFO] [util.go:66] [Exit] [name=collect]
[2019/08/20 20:56:49.544 +08:00] [INFO] [load.go:434] ["Run()... in Loader quit"]
[2019/08/20 20:56:49.544 +08:00] [INFO] [mysql.go:114] ["Successes chan quit"]
[2019/08/20 20:56:49.633 +08:00] [INFO] [syncer.go:246] ["handleSuccess quit"]
[2019/08/20 20:56:49.633 +08:00] [INFO] [util.go:66] [Exit] [name=syncer]
[2019/08/20 20:56:49.633 +08:00] [INFO] [main.go:73] ["drainer exit"]

```

- - - - - -

- - - - - -

- - - - - -

##### 12. pump log 异常信息 got signal to exit

```ruby
[2019/08/20 20:56:05.316 +08:00] [INFO] [server.go:522] ["server info tick"] [writeBinlogCount=10016] [alivePullerCount=1] [MaxCommitTS=410597658037583873]
[2019/08/20 20:56:15.309 +08:00] [INFO] [storage.go:381] [DBStats] [DBStats="{\"WriteDelayCount\":0,\"WriteDelayDuration\":0,\"WritePaused\":false,\"AliveSnapshots\":0,\"AliveIterators\":0,\"IOWrite\":2225967,\"IORead\":0,\"BlockCacheSize\":0,\"OpenedTablesCount\":0,\"LevelSizes\":null,\"LevelTablesCounts\":null,\"LevelRead\":null,\"LevelWrite\":null,\"LevelDurations\":null}"]
[2019/08/20 20:56:15.317 +08:00] [INFO] [server.go:522] ["server info tick"] [writeBinlogCount=10016] [alivePullerCount=1] [MaxCommitTS=410597660396879873]
[2019/08/20 20:56:25.309 +08:00] [INFO] [storage.go:381] [DBStats] [DBStats="{\"WriteDelayCount\":0,\"WriteDelayDuration\":0,\"WritePaused\":false,\"AliveSnapshots\":0,\"AliveIterators\":0,\"IOWrite\":2226617,\"IORead\":0,\"BlockCacheSize\":0,\"OpenedTablesCount\":0,\"LevelSizes\":null,\"LevelTablesCounts\":null,\"LevelRead\":null,\"LevelWrite\":null,\"LevelDurations\":null}"]
[2019/08/20 20:56:25.316 +08:00] [INFO] [server.go:522] ["server info tick"] [writeBinlogCount=10018] [alivePullerCount=1] [MaxCommitTS=410597663542607875]
[2019/08/20 20:56:35.309 +08:00] [INFO] [storage.go:381] [DBStats] [DBStats="{\"WriteDelayCount\":0,\"WriteDelayDuration\":0,\"WritePaused\":false,\"AliveSnapshots\":0,\"AliveIterators\":0,\"IOWrite\":2229972,\"IORead\":0,\"BlockCacheSize\":0,\"OpenedTablesCount\":0,\"LevelSizes\":null,\"LevelTablesCounts\":null,\"LevelRead\":null,\"LevelWrite\":null,\"LevelDurations\":null}"]
[2019/08/20 20:56:35.317 +08:00] [INFO] [server.go:522] ["server info tick"] [writeBinlogCount=10054] [alivePullerCount=1] [MaxCommitTS=410597665587331079]
[2019/08/20 20:56:45.309 +08:00] [INFO] [storage.go:381] [DBStats] [DBStats="{\"WriteDelayCount\":0,\"WriteDelayDuration\":0,\"WritePaused\":false,\"AliveSnapshots\":0,\"AliveIterators\":0,\"IOWrite\":2230701,\"IORead\":0,\"BlockCacheSize\":0,\"OpenedTablesCount\":0,\"LevelSizes\":null,\"LevelTablesCounts\":null,\"LevelRead\":null,\"LevelWrite\":null,\"LevelDurations\":null}"]
[2019/08/20 20:56:45.317 +08:00] [INFO] [server.go:522] ["server info tick"] [writeBinlogCount=10058] [alivePullerCount=1] [MaxCommitTS=410597667815555073]
[2019/08/20 20:56:49.539 +08:00] [INFO] [main.go:65] ["got signal to exit."] [signal=hangup] # 原因是使用 nohup 启动，但是客户连接是非正常使用 exit 断开，导致系统杀掉了 pump 进程。
[2019/08/20 20:56:49.539 +08:00] [INFO] [server.go:839] ["begin to close pump server"]
[2019/08/20 20:56:49.539 +08:00] [INFO] [server.go:496] ["detect drainer checkpoint routine exit"]
[2019/08/20 20:56:49.539 +08:00] [INFO] [server.go:537] ["gcBinlogFile exit"]
[2019/08/20 20:56:49.539 +08:00] [INFO] [server.go:519] ["printServerInfo exit"]
[2019/08/20 20:56:49.539 +08:00] [INFO] [server.go:472] ["genFakeBinlog exit"]
[2019/08/20 20:56:49.539 +08:00] [INFO] [server.go:848] ["background goroutins are stopped"]
[2019/08/20 20:56:49.539 +08:00] [INFO] [node.go:216] ["Heartbeat goroutine exited"]
[2019/08/20 20:56:49.543 +08:00] [INFO] [server.go:832] ["update state success"] [NodeID=dev10:8250] [state=paused]
[2019/08/20 20:56:49.543 +08:00] [INFO] [server.go:851] ["commit status done"]
[2019/08/20 20:56:49.544 +08:00] [INFO] [server.go:855] ["grpc is stopped"]
[2019/08/20 20:56:49.544 +08:00] [INFO] [chaser.go:76] ["Slow chaser quits"]
[2019/08/20 20:56:49.546 +08:00] [INFO] [server.go:860] ["storage is closed"]
[2019/08/20 20:56:49.546 +08:00] [INFO] [server.go:865] ["pump node is closed"]
[2019/08/20 20:56:49.546 +08:00] [INFO] [server.go:871] ["has closed pdCli"]
[2019/08/20 20:56:49.546 +08:00] [INFO] [server.go:876] ["has closed tiStore"]
[2019/08/20 20:56:49.546 +08:00] [INFO] [main.go:68] ["pump is closed"]
[2019/08/20 20:56:49.546 +08:00] [INFO] [main.go:80] ["pump exit"]

```

- - - - - -

- - - - - -

- - - - - -

##### 13. **二进制部署的pump/drainer 转 配置文件部署的pump/drainer 注意事项**

**要按照如下步骤切换部署**  
1\. [加入pump/drainer集群](https://www.lemonit.cn/2019/08/15/tidb-%E4%BD%BF%E7%94%A8-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-%E9%83%A8%E7%BD%B2-tidb-binlog%E9%9B%86%E7%BE%A4/ "加入pump/drainer集群")  
2\. 暂停二进制pump/drainer  
3\. 下线进进制pump/drainer（注：状态必须是offline状态否则，切换失败）  
`tail -1000 drainer.log 异常信息如下：`

```ruby
[tidb@dev11 log]<span class="katex math inline">pwd
/home/tidb/deploy/log
[tidb@dev11 log]</span> tail -1000 drainer.log

[2019/08/21 08:46:07.005 +08:00] [INFO] [pump.go:133] ["pump create pull binlogs client"] [id=dev10:8250]
[2019/08/21 08:46:07.006 +08:00] [ERROR] [pump.go:217] ["pump create PullBinlogs client failed"] [id=dev10:8250] [error="rpc error: code = Unavailable desc = all SubConns are in TransientFailure, latest connection error: connection error: desc = \"transport: Error while dialing dial tcp 172.160.180.32:8250: connect: connection refused\""]
[2019/08/21 08:46:07.006 +08:00] [ERROR] [pump.go:135] ["pump create pull binlogs client failed"] [id=dev10:8250] [error="rpc error: code = Unavailable desc = all SubConns are in TransientFailure, latest connection error: connection error: desc = \"transport: Error while dialing dial tcp 172.160.180.32:8250: connect: connection refused\""] [errorVerbose="rpc error: code = Unavailable desc = all SubConns are in TransientFailure, latest connection error: connection error: desc = \"transport: Error while dialing dial tcp 172.160.180.32:8250: connect: connection refused\"\ngithub.com/pingcap/errors.AddStack\n\t/home/jenkins/workspace/release_tidb_3.0/go/pkg/mod/github.com/pingcap/errors@v0.11.4/errors.go:174\ngithub.com/pingcap/errors.Trace\n\t/home/jenkins/workspace/release_tidb_3.0/go/pkg/mod/github.com/pingcap/errors@v0.11.4/juju_adaptor.go:15\ngithub.com/pingcap/tidb-binlog/drainer.(*Pump).createPullBinlogsClient\n\t/home/jenkins/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb-binlog/drainer/pump.go:221\ngithub.com/pingcap/tidb-binlog/drainer.(*Pump).PullBinlog.func1\n\t/home/jenkins/workspace/release_tidb_3.0/go/src/github.com/pingcap/tidb-binlog/drainer/pump.go:134\nruntime.goexit\n\t/usr/local/go/src/runtime/asm_amd64.s:1337"]

```

- - - - - -

[Drainer 集群同步数据到 MariaDB 产生重复数据](https://asktug.com/t/drainer-mariadb/647 "Drainer 集群同步数据到 MariaDB 产生重复数据")

- - - - - -

- - - - - -

- - - - - -

##### 14. Region 的默认数量

[官方文档](https://pingcap.com/blog-cn/tidb-internal-3/ "官方文档")

[一个 Region 的 Replica 数量](https://github.com/pingcap/pd/blob/master/conf/config.toml#L54 "一个 Region 的 Replica 数量")

```ruby
[tidb@test1 ~]<span class="katex math inline">cat /home/tidb/tidb-ansible/conf/pd.yml
---
# default configuration file for pd in yaml format

global:
  # lease: 3
  # tso-save-interval: "3s"

  # namespace-classifier: "table"

  # enable-prevote: true

security:
  # Path of file that contains list of trusted SSL CAs. if set, following four settings shouldn't be empty
  # cacert-path: ""
  # Path of file that contains X509 certificate in PEM format.
  # cert-path: ""
  # Path of file that contains X509 key in PEM format.
  # key-path: ""

log:
  #level: "info"

  # log format, one of json, text, console
  # format: "text"

  # disable automatic timestamps in output
  # disable-timestamp: false

  # file logging
  file:
    # max log file size in MB
    # max-size: 300
    # max log file keep days
    # max-days: 28
    # maximum number of old log files to retain
    # max-backups: 7
    # rotate log by day
    # log-rotate: true

metric:

schedule:
  # max-merge-region-size: 20
  # max-merge-region-keys: 200000
  # split-merge-interval: "1h"
  # max-snapshot-count: 3
  # max-pending-peer-count: 16
  # max-store-down-time: "30m"
  # leader-schedule-limit: 4
  # region-schedule-limit: 4
  # replica-schedule-limit: 8
  # merge-schedule-limit: 8
  # tolerant-size-ratio: 5.0

replication:
  # 因为region默认值是 3，而相同的region每台节点机上只能有一个，所以默认TiKV节点的数量最少是三台，是用来保证实际存活的region副本数量
  # The number of replicas for each region.
  # max-replicas: 3
  # The label keys specified the location of a store.
  # The placement priorities is implied by the order of label keys.
  # For example, ["zone", "rack"] means that we should place replicas to
  # different zones first, then to different racks if we don't have enough zones.
  # location-labels: []
[tidb@test1 ~]</span>


```

- - - - - -

- - - - - -

- - - - - -

##### 15. INFORMATION\_SCHEMA.SLOW\_QUERY, 慢查询日志里的query语句太长，不能显示完整的 SQL语句

```ruby
[tidb@test1 tidb-ansible]<span class="katex math inline">vim /home/tidb/tidb-ansible/conf/tidb.yml
......
log:
  # Log level: debug, info, warn, error, fatal.
  # level: "info"

  # Log format, one of json, text, console.
  # format: "text"

  # Disable automatic timestamps in output
  # disable-timestamp: false

  # Queries with execution time greater than this value will be logged. (Milliseconds)
  # slow-threshold: 300

  # Queries with internal result greater than this value will be logged.
  # expensive-threshold: 10000

  # Maximum query length recorded in log.
  # query-log-max-len: 2048
  query-log-max-len: 20480
......
[tidb@test1 tidb-ansible]</span>
[tidb@test1 tidb-ansible]<span class="katex math inline"># 滚动更新以后，后续收集的语句就可以显示全部了
[tidb@test1 tidb-ansible]</span> ansible-playbook rolling_update.yml

Congrats! All goes well. :-)
[tidb@test1 tidb-ansible]$

```

- - - - - -

- - - - - -

- - - - - -

##### 16. RECOVER 导致 TiDB Binlog 同步错误处理

当使用 TiDB Binlog 同步工具时，上游 TiDB 使用 RECOVER TABLE 后，TiDB Binlog 可能会因为下面几个原因造成同步中断：

- 下游数据库不支持 RECOVER TABLE 语句。  
  类似错误：`check the manual that corresponds to your MySQL server version for the right syntax to use near 'RECOVER TABLE table_name'`。
- 上下游数据库的 GC lifetime 不一样。  
  类似错误：`snapshot is older than GC safe point 2019-07-10 13:45:57 +0800 CST`。
- 上下游数据库的同步延迟。  
  类似错误：`snapshot is older than GC safe point 2019-07-10 13:45:57 +0800 CST`。

**`只能通过重新全量导入被删除的表来恢复 TiDB Binlog 的数据同步。`**

1. 先使用 mydumper 将恢复的数据保存到本地
2. 删除恢复表
3. 让drainer 恢复正常运行
4. 重新导入被删除的表
5. 查看TiDB Binlog 数据同步是否正常

- - - - - -

- - - - - -

- - - - - -

##### 17. TxnLockNotFound

1. 这个是事务提交的慢了，超过了`事务TTL`被其他事务给 Rollback 掉了
2. 事务`TTL`默认是 3s
3. 乐观和悲观在提交阶段都可能有这个报错

 因此这个报错是因为第一个事务一直没有提交，在等待一定时间之后后面的事务会清理掉之前事务的锁信息，等到第一个事务再去提交的时候发现锁没了，就会报错 `TxnLockNotFound`。

**事务TTL=(`Time To Live`), 事务的生存时间**

**[问题解决资料](https://asktug.com/t/topic/1815/10?u=mao_siyu "问题解决资料")**

- - - - - -

- - - - - -

- - - - - -

##### 18. 修改单个事务允许的最大执行时间

 **修改 `max-txn-time-use` 必须要相应的调整 `tikv_gc_life_time`** `tikv_gc_life_time` 的值`必须大于` TiDB 的配置文件(`/home/tidb/tidb-ansible/conf/tidb.yml`)中的 max-txn-time-use 的值至少 10 秒，且不低于 10 分钟。

**GC时间改为 `20分钟`**

```sql
UPDATE mysql.tidb SET VARIABLE_VALUE = '20m' WHERE VARIABLE_NAME = 'tikv_gc_life_time';
SELECT VARIABLE_VALUE FROM mysql.tidb WHERE VARIABLE_NAME = 'tikv_gc_life_time';

```

**单个事务允许的最大执行时间改为 20分钟`-`10秒**

```ruby
[tidb@dev10 tidb-ansible]<span class="katex math inline">pwd
/home/tidb/tidb-ansible
[tidb@dev10 tidb-ansible]</span> vim /home/tidb/tidb-ansible/conf/tidb.yml
......
125 tikv_client:
126   # 单个事务允许的最大执行时间。
127   # 默认值: 590
128   # 单位：秒
129   # 此处修改为: 1190
130   max-txn-time-use: 1190
......

# 使配置滚动生效
[tidb@dev10 tidb-ansible]$ ansible-playbook rolling_update.yml -t tidb

```

- - - - - -

- - - - - -

- - - - - -

##### 19. 悲观锁事务冲突，日志报错

`pessimistic write conflict, retry statement` 属于悲观锁事务冲突的报错日志  
**[处理办法](https://asktug.com/t/pessimistic-write-conflict-retry-statement/563/6 "处理办法")**

- - - - - -

- - - - - -

- - - - - -

##### 20. 测试写写冲突，解析日志

**执行SQL的客户端地址**: 172.160.180.32

**执行SQL的客户端地址**: 172.160.180.46

**TiDB-Server**: 172.160.180.33

**开启了乐观锁事务重试**

- - - - - -

###### 创建数据库

```sql
CREATE TABLE `table1` (
  `id` bigint(20) NOT NULL,
  `name` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin;

```

- - - - - -

- - - - - -

##### 20.1 测试插入数据冲突

<table><thead><tr><th align="left">事务一</th><th align="center">Step</th><th align="left">事务二</th></tr></thead><tbody><tr><td align="left">172.160.180.32</td><td align="center">客户端</td><td align="left">172.160.180.46</td></tr><tr><td align="left">BEGIN;   
 Query OK, 0 rows affected (0.01 sec)</td><td align="center">**1**</td><td align="left">BEGIN;   
 Query OK, 0 rows affected (0.01 sec)</td></tr><tr><td align="left">`INSERT INTO table1(id,name) VALUES (1000, '张三');`   
 Query OK, 1 row affected (0.00 sec)</td><td align="center">**2**</td><td align="left">`INSERT INTO table1(id,name) VALUES (1000, 'PingCAP');`   
 Query OK, 1 row affected (0.00 sec)</td></tr><tr><td align="left"></td><td align="center">**3**</td><td align="left">COMMIT;   
 Query `OK`, 0 rows affected (0.02 sec)</td></tr><tr><td align="left">COMMIT;   
 `ERROR 1062 (23000): Duplicate entry '1000' for key 'PRIMARY'`</td><td align="center">**4**</td><td align="left"></td></tr></tbody></table>

- - - - - -

###### 20.1.1 查看最终结果

```sql
MySQL [eric]> SELECT * FROM table1;
+------+---------+
| id   | name    |
+------+---------+
| 1000 | PingCAP |
+------+---------+
1 row in set (0.01 sec)

MySQL [eric]>

```

- - - - - -

###### 20.1.2 查看TiDB.log 冲突日志

```ruby
[2020/01/13 09:04:12.352 +08:00] [INFO] [set.go:192] ["set session var"] [conn=32186] [name=transaction_read_only] [val=0]
[2020/01/13 09:04:14.914 +08:00] [WARN] [session.go:419] [sql] [conn=32148] [label=general] [error="[kv:9007]Write conflict, txnStartTS=413893216290471948, conflictStartTS=413893242190299137, conflictCommitTS=413893250919694337, key={tableID=15314, handle=1000} primary={tableID=15314, handle=1000} [try again later]"] [txn="Txn{state=invalid}"]
[2020/01/13 09:04:14.914 +08:00] [WARN] [session.go:611] [retrying] [conn=32148] [schemaVersion=13707] [retryCnt=0] [queryNum=0] [sql=BEGIN]
[2020/01/13 09:04:14.915 +08:00] [WARN] [session.go:611] [retrying] [conn=32148] [schemaVersion=13707] [retryCnt=0] [queryNum=1] [sql="INSERT INTO table1(id,name) VALUES (1000, '张三')"]
[2020/01/13 09:04:14.915 +08:00] [WARN] [session.go:611] [retrying] [conn=32148] [schemaVersion=13707] [retryCnt=0] [queryNum=2] [sql=COMMIT]
[2020/01/13 09:04:14.915 +08:00] [WARN] [session.go:632] ["transaction association"] [conn=32148] ["retrying txnStartTS"=413893251535732737] ["original txnStartTS"=413893216290471948]
[2020/01/13 09:04:14.919 +08:00] [INFO] [2pc.go:1039] ["2PC clean up done"] [conn=32148] [txnStartTS=413893216290471948]
[2020/01/13 09:04:14.919 +08:00] [WARN] [session.go:646] [sql] [conn=32148] [label=general] [session="{\n  \"currDBName\": \"eric\",\n  \"id\": 32148,\n  \"status\": 2,\n  \"strictMode\": true,\n  \"user\": {\n    \"Username\": \"root\",\n    \"Hostname\": \"172.160.180.32\",\n    \"CurrentUser\": false,\n    \"AuthUsername\": \"root\",\n    \"AuthHostname\": \"%\"\n  }\n}"] [error="[kv:1062]Duplicate entry '1000' for key 'PRIMARY'"]

```

- - - - - -

###### 20.1.3 拆解日志

```json
[2020/01/13 09:04:14.914 +08:00] [WARN] [session.go:419] [sql] [conn=32148] [label=general]
[error="[kv:9007]                           // 写冲突
Write conflict,
 txnStartTS=413893216290471948,             // 原事务(事务A)
 conflictStartTS=413893242190299137,        // 与原事务相冲突的事务(事务B)
 conflictCommitTS=413893250919694337,       // 相冲突的事务已经提交(事务B已经提交)
 key={tableID=15314, handle=1000}
 primary={tableID=15314, handle=1000}
 [try again later]"]
 [txn="Txn{state=invalid}"]

// 最后提交的事务
[2020/01/13 09:04:14.914 +08:00] [WARN] [session.go:611] [retrying] [conn=32148] [schemaVersion=13707] [retryCnt=0]
 [queryNum=0] [sql=BEGIN]
[2020/01/13 09:04:14.915 +08:00] [WARN] [session.go:611] [retrying] [conn=32148] [schemaVersion=13707] [retryCnt=0]
 [queryNum=1] [sql="INSERT INTO table1(id,name) VALUES (1000, '张三')"]
[2020/01/13 09:04:14.915 +08:00] [WARN] [session.go:611] [retrying] [conn=32148] [schemaVersion=13707] [retryCnt=0]
 [queryNum=2] [sql=COMMIT]


[2020/01/13 09:04:14.915 +08:00] [WARN] [session.go:632]
 ["transaction association"] [conn=32148]    //
 ["retrying txnStartTS"=413893251535732737]  // 原事务(事务A) 被重试
 ["original txnStartTS"=413893216290471948]  // 原事务(事务A)

// 被回滚的事务(被清理的事务)
[2020/01/13 09:04:14.919 +08:00] [INFO] [2pc.go:1039] ["2PC clean up done"] [conn=32148] [txnStartTS=413893216290471948]

// 被回滚的事务，是由哪个客户端发起
[2020/01/13 09:04:14.919 +08:00] [WARN] [session.go:646] [sql] [conn=32148] [label=general]
 [session="{
    "currDBName": "eric",
    "id": 32148,
    "status": 2,
    "strictMode": true,
    "user": {
      "Username": "root",
      "Hostname": "172.160.180.32",
      "CurrentUser": false,
      "AuthUsername": "root",
      "AuthHostname": "%"
    }
 }"]
 // 事务冲突原因
 [error="[kv:1062]Duplicate entry '1000' for key 'PRIMARY'"]

```

- - - - - -

- - - - - -

##### 20.2 测试更新数据冲突

<table><thead><tr><th align="left">事务一</th><th align="center">Step</th><th align="left">事务二</th></tr></thead><tbody><tr><td align="left">172.160.180.32</td><td align="center">客户端</td><td align="left">172.160.180.46</td></tr><tr><td align="left">BEGIN;   
 Query OK, 0 rows affected (0.01 sec)</td><td align="center">**1**</td><td align="left">BEGIN;   
 Query OK, 0 rows affected (0.00 sec)</td></tr><tr><td align="left">`UPDATE table1 SET name='TiDB' WHERE id=1000;`   
 Query OK, 1 row affected (0.00 sec)   
 Rows matched: 1 Changed: 1 Warnings: 0</td><td align="center">**2**</td><td align="left">`UPDATE table1 SET name='TUG' WHERE id=1000;`   
 Query OK, 1 row affected (0.00 sec)   
 Rows matched: 1 Changed: 1 Warnings: 0</td></tr><tr><td align="left">COMMIT;   
 Query OK, 0 rows affected (0.01 sec)</td><td align="center">**3**</td><td align="left"></td></tr><tr><td align="left"></td><td align="center">**4**</td><td align="left">COMMIT;   
 Query OK, 0 rows affected (0.01 sec)</td></tr></tbody></table>

- - - - - -

###### 20.2.1 查看最终结果

```sql
MySQL [eric]> SELECT * FROM table1;
+------+------+
| id   | name |
+------+------+
| 1000 | TUG  |
+------+------+
1 row in set (0.01 sec)

MySQL [eric]>

```

- - - - - -

###### 20.2.2 冲突日志

```ruby
[2020/01/13 10:08:32.340 +08:00] [INFO] [set.go:192] ["set session var"] [conn=32560] [name=sql_select_limit] [val=1]
[2020/01/13 10:08:36.988 +08:00] [WARN] [session.go:419] [sql] [conn=32152] [label=general] [error="[kv:9007]Write conflict, txnStartTS=413894257093902337, conflictStartTS=413894221350567937, conflictCommitTS=413894262690676738, key={tableID=15314, handle=1000} primary={tableID=15314, handle=1000} [try again later]"] [txn="Txn{state=invalid}"]
[2020/01/13 10:08:36.988 +08:00] [WARN] [session.go:611] [retrying] [conn=32152] [schemaVersion=13712] [retryCnt=0] [queryNum=0] [sql=BEGIN]
[2020/01/13 10:08:36.989 +08:00] [WARN] [session.go:611] [retrying] [conn=32152] [schemaVersion=13712] [retryCnt=0] [queryNum=1] [sql="UPDATE table1 SET name='TUG' WHERE id=1000"]
[2020/01/13 10:08:36.989 +08:00] [WARN] [session.go:611] [retrying] [conn=32152] [schemaVersion=13712] [retryCnt=0] [queryNum=2] [sql=COMMIT]
[2020/01/13 10:08:36.989 +08:00] [WARN] [session.go:632] ["transaction association"] [conn=32152] ["retrying txnStartTS"=413894263948967942] ["original txnStartTS"=413894257093902337]
[2020/01/13 10:08:36.991 +08:00] [INFO] [2pc.go:1039] ["2PC clean up done"] [conn=32152] [txnStartTS=413894257093902337]
[2020/01/13 10:08:37.663 +08:00] [INFO] [set.go:192] ["set session var"] [conn=24502] [name=tx_read_only] [val=1]

```

- - - - - -

###### 20.2.3 拆解日志

```json
[2020/01/13 10:08:36.988 +08:00] [WARN] [session.go:419] [sql] [conn=32152] [label=general]
[error="[kv:9007]
 Write conflict,
 txnStartTS=413894257093902337,
 conflictStartTS=413894221350567937,
 conflictCommitTS=413894262690676738,
 key={tableID=15314, handle=1000}
 primary={tableID=15314, handle=1000}
 [try again later]"] [txn="Txn{state=invalid}"]

// 最后提交的事务
[2020/01/13 10:08:36.988 +08:00] [WARN] [session.go:611] [retrying] [conn=32152] [schemaVersion=13712] [retryCnt=0]
 [queryNum=0] [sql=BEGIN]
[2020/01/13 10:08:36.989 +08:00] [WARN] [session.go:611] [retrying] [conn=32152] [schemaVersion=13712] [retryCnt=0]
 [queryNum=1] [sql="UPDATE table1 SET name='TUG' WHERE id=1000"]
[2020/01/13 10:08:36.989 +08:00] [WARN] [session.go:611] [retrying] [conn=32152] [schemaVersion=13712] [retryCnt=0]
 [queryNum=2] [sql=COMMIT]


[2020/01/13 10:08:36.989 +08:00] [WARN] [session.go:632]
 ["transaction association"] [conn=32152]
 ["retrying txnStartTS"=413894263948967942]
 ["original txnStartTS"=413894257093902337]

// 被回滚的事务(被清理的事务)
[2020/01/13 10:08:36.991 +08:00] [INFO] [2pc.go:1039] ["2PC clean up done"] [conn=32152] [txnStartTS=413894257093902337]

```

- - - - - -

- - - - - -

- - - - - -

##### 21. `ansible-playbook bootstrap.yml --extra-vars "dev_mode=True"` 失败

```ruby
Ansible FAILED! => playbook: bootstrap.yml; TASK: check_system_static : Preflight check - Check if the operating system supports EPOLLEXCLUSIVE; message: {"changed": true, "cmd": "/home/tidb/deploy/epollexclusive", "delta": "0:00:00.353218", "end": "2020-01-15 18:44:32.660547", "msg": "non-zero return code", "rc": 1, "start": "2020-01-15 18:44:32.307329", "stderr": "", "stderr_lines": [], "stdout": "epoll_ctl with EPOLLEXCLUSIVE | EPOLLONESHOT succeeded. This is evidence of no EPOLLEXCLUSIVE support. Not using epollex polling engine.False: epollexclusive is not available", "stdout_lines": ["epoll_ctl with EPOLLEXCLUSIVE | EPOLLONESHOT succeeded. This is evidence of no EPOLLEXCLUSIVE support. Not using epollex polling engine.False: epollexclusive is not available"]}

```

**`原因：`操作系统内核版本太低**  
uname -a 检查一下操作系统内核版本，正常需要4.5以上版本才支持**EPOLLEXCLUSIVE**  
[官方解释](https://asktug.com/t/topic/1412/5?u=mao_siyu "官方解释")

- - - - - -

- - - - - -

- - - - - -

##### 22. `fio: randread iops of tikv_data_dir disk is too low`

```ruby
[tidb@dev10 tidb-ansible]$ ansible-playbook bootstrap.yml
......
FAILED! => {"changed": false, "msg": "fio: randread iops of tikv_data_dir disk is too low:
......

```

原因是 官方强烈要求使用 Ansible 方式部署时，TiKV 及 PD 节点数据目录所在磁盘请使用 SSD 磁盘，否则无法通过检测。  
**`跳过磁盘性能检查`**

```ruby
[tidb@dev10 tidb-ansible]<span class="katex math inline">ansible-playbook bootstrap.yml --extra-vars "dev_mode=True"
......
Congrats! All goes well. :-)
[tidb@dev10 tidb-ansible]</span>

```

- - - - - -

- - - - - -

- - - - - -

##### 23. `Swap is on, for best performance, turn swap off`

```ruby
[tidb@dev10 tidb-ansible]$ ansible-playbook deploy.yml
......
FAILED! => {"changed": false, "msg": "Swap is on, for best performance, turn swap off"}

```

**关闭swap**

```ruby
[tidb@dev10 tidb-ansible]$ ansible -i hosts.ini all -m shell -a 'swapoff -a' -u tidb -b

```

- - - - - -

- - - - - -

- - - - - -

##### 24. `hostnames of all nodes in cluster`

```ruby
[tidb@dev10 tidb-ansible]$ ansible-playbook deploy.yml
......
fatal: [192.168.192.31]: FAILED! => {"changed": false, "msg": "hostnames of all nodes in cluster: [IT, IT, IT, IT, IT, IT]\n"}

```

**所有节点机的hostname都相同导致的， 要修改主机名**

```ruby
[tidb@dev10 tidb-ansible]$ hostnamectl set-hostname 主机名

```

- - - - - -

- - - - - -

- - - - - -

##### 25. 因TiDB 3.0.7 添加联合索引长度限制，导致TiDB 3.0.7以前的数据库，数据迁移到高版本数据库时，联合索引超长问题

**[TiDB3.0.9 转库导表，异常](https://asktug.com/t/topic/32838/14 "TiDB3.0.9 转库导表，异常")**

- - - - - -

- - - - - -

- - - - - -

##### 26. 注意事项

1. 不要使用Navicat 直接操作大表，例如：`添加索引`、`修改表名`等操作，如果机器的性能不够，TiKV处理的速度根不上，会发生请求的消息堆积； 因为TiDB本身的保护机制，一项修改操作没有执行完，后续所有的操作都会进入等待状态，我尝试过野蛮操作直接杀死TiDB进程, 直接将TiDB集群强制停机后在重新启动, 最后发现, 之前队列中的请求依然存在, 直到所有的请求都处理完，才会继续执行后续操作，所以尽量不要这样操作大表，因为一但操作了就`没有后悔药`可吃了;

- - - - - -

- - - - - -

- - - - - -

##### 27. TiDB 备份数据异常

**`原因`** ：因为备份数据时，数据量过大，导致超出 TiDB-Server 内存使用最大上限而引发，操作系统将内存保护机制，将进程Kill掉，导致连接失：因为备份数据时，数据量过大，导致超出 TiDB-Server 内存使用最大上限而引发，操作系统将内存保护机制，将进程Kill掉，导致连接失败  
`** (mydumper:9228): CRITICAL **: 14:22:56.733: Error dumping schemas (dev2_order.order_info): MySQL server has gone away`  
**`解决办法`** : 调整mydumper参数，将原来的 将表按大小分块，改为按行分块

```sql
# 原来的
./mydumper -h 172.160.192.33 -u root -p 123456 -P 4600 -c -t 16 -F 64 -x '^(?!(mysql|test|INFORMATION_SCHEMA|PERFORMANCE_SCHEMA|tidb_loader))' --skip-tz-utc -o 2020-03-28-eric_mao/

# 改变后的
./mydumper -h 172.160.192.33 -u root -p 123456 -P 4600 -c -t 16 -r 5000 -x '^(?!(mysql|test|INFORMATION_SCHEMA|PERFORMANCE_SCHEMA|tidb_loader))' --skip-tz-utc -o 2020-03-28-eric_mao/


```

- - - - - -

- - - - - -

- - - - - -

##### 28. 调优TiKV配置 滚动更新异常

```ruby
[tidb@test ~]$ ansible-playbook rolling_update.yml -t tikv

```

###### 滚动更新 有可能引发的 **`异常`**

**`异常一`：**  
 tikv 节点修改配置时滚动更新异常

```ruby
......省略

TASK:    remove evict-leader-scheduler;
message: {
    "access_control_allow_headers": "accept, content-type, authorization",
    "access_control_allow_methods": "POST, GET, OPTIONS, PUT, DELETE",
    "access_control_allow_origin": "*",
    "changed": false,
    "connection": "close",
    "content": "\"scheduler not found\"\n",
    "content_length": "22",
    "content_type": "application/json; charset=UTF-8",
    "date": "Thu, 07 May 2020 07:33:41 GMT",
    "json": "scheduler not found",
    "msg": "Status code was 500 and not [200]: HTTP Error 500: Internal Server Error",
    "redirected": false,
    "status": 500,
    "url": "http://192.168.180.33:2379/pd/api/v1/schedulers/evict-leader-scheduler-72015"
}

```

**`异常二`：**  
 leader都跑到一个节点上了

- - - - - -

###### 查看 scheduler

```ruby
[tidb@test ~]<span class="katex math inline">/home/tidb/tidb-ansible/resources/bin/pd-ctl -u  http://192.168.180.33:2379 -d scheduler show
[
  "evict-leader-scheduler-72015",
  "evict-leader-scheduler-33067",
  "balance-region-scheduler",
  "balance-leader-scheduler",
  "balance-hot-region-scheduler",
  "label-scheduler"
]

[tidb@test ~]</span>

```

###### 删除 evict-leader-scheduler-\*

```ruby
[tidb@test ~]<span class="katex math inline">/home/tidb/tidb-ansible/resources/bin/pd-ctl -u  http://192.168.180.33:2379 -d scheduler remove evict-leader-scheduler-72015
[tidb@test ~]</span>
[tidb@test ~]$ /home/tidb/tidb-ansible/resources/bin/pd-ctl -u  http://192.168.180.33:2379 -d scheduler remove evict-leader-scheduler-33067


```

- - - - - -

- - - - - -

- - - - - -
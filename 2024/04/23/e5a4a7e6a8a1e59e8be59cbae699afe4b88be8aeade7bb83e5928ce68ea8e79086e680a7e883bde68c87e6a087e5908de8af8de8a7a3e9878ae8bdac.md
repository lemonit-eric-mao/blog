---
title: '大模型场景下训练和推理性能指标名词解释[转]'
date: '2024-04-23T02:43:27+00:00'
status: publish
permalink: /2024/04/23/%e5%a4%a7%e6%a8%a1%e5%9e%8b%e5%9c%ba%e6%99%af%e4%b8%8b%e8%ae%ad%e7%bb%83%e5%92%8c%e6%8e%a8%e7%90%86%e6%80%a7%e8%83%bd%e6%8c%87%e6%a0%87%e5%90%8d%e8%af%8d%e8%a7%a3%e9%87%8a%e8%bd%ac
author: 毛巳煜
excerpt: ''
type: post
id: 10828
category:
    - 人工智能
tag: []
post_format: []
wb_sst_seo:
    - 'a:3:{i:0;s:0:"";i:1;s:0:"";i:2;s:0:"";}'
hestia_layout_select:
    - sidebar-right
---
<table><thead><tr><th>指标</th><th>含义</th></tr></thead><tbody><tr><td>token</td><td>token是文本的最小单位。 在英文中，token 往往代表一个单词或一个标点符号；

在中文中，token 往往代表一个字或词。

</td></tr><tr><td>samples per second</td><td>每秒样本数，是指模型在训练或推理过程中每秒处理的样本数量，即训练时实际吞吐量。 计算公式为：**`samples/s = BS * N / step time`**

其中，BS为batch size，N为GPU/NPU的数量，step time是在分布式集群中执行完一个BS的时间（秒）。

</td></tr><tr><td>tokens per second</td><td>NLP中常用的吞吐量指标，表示在单位时间内模型能够处理的token数量。用于评估模型的推理或训练性能。 计算公式为：**`tokens/s = token len / cost time`**

其中，token len为处理的文本中token的数量，cost time 为处理时间。

</td></tr><tr><td>TFLOPs</td><td>FLOPs是Floating-point Operations Per Second的缩写，代表每秒所执行的浮点运算次数。 往往用TFLOPs衡量计算能力，即每秒执行万亿次浮点运算。

</td></tr><tr><td>TP</td><td>大模型训练时的并行策略，张量并行。 可以将大模型的张量拆分为多个小块，分散到多个设备上，从而加快训练和推理的速度。

详细可参考：[张量并行](https://zhuanlan.zhihu.com/p/581677880)。

</td></tr><tr><td>PP</td><td>大模型训练时的并行策略，流水线并行。 可以将模型的层或模块划分为多个阶段，并在不同的设备上并行执行这些阶段，从而提高计算效率和吞吐量。

详细可参考：[流水线并行](https://zhuanlan.zhihu.com/p/581677880)。

</td></tr><tr><td>prompt</td><td>Prompt是一种由模型使用方提供的文本片段，用于在推理时引导大模型生成特定的输出。</td></tr><tr><td>cost time (或Inference time)</td><td>推理时间，指完成一次推理过程所需的总时间。 包括加载模型、预处理输入、模型推理计算和后处理等步骤。

耗时越短，意味着模型推理速度越快。

</td></tr></tbody></table>

[原文](https://bbs.huaweicloud.com/blogs/416186 "原文")

- - - - - -

Python 代码示例
===========

<div style="overflow:hidden; clear:both; width: 100%; height: 40px; position: relative;">- - - - - -

 <span style="position: absolute;top: 50%;left: 50%; transform: translate(-50%, -50%); background-color: white;">以下为隐藏内容</span> </div> ```python
import time
from transformers import AutoTokenizer, AutoModel

# 假设的参数
N = 1  # GPU/NPU的数量，这里假设只有1个
BS = 1  # batch size，这里假设每次处理一个样本

model_path = "/data/LLM/THUDM/chatglm3-6b-32k"

# 加载分词器和模型
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda().eval()


# 进行对话交互
def chat_with_model(input_text, history):
    # 获取推理前的时间戳
    start_time = time.time()

    # 执行模型推理
    response, history = model.chat(tokenizer, input_text, history=history)

    # 获取推理后的时间戳
    end_time = time.time()

    # 计算推理时间
    cost_time = end_time - start_time

    # 从模型输出中获取生成的token数量
    output_tokens = tokenizer.encode(response, add_special_tokens=True)
    output_token_count = len(output_tokens)

    # 计算每秒token数（tokens/s）
    tokens_per_second = output_token_count / cost_time

    # 输出结果
    output = {
        "samples/s": round(BS * N / cost_time, 2),  # 每秒样本数
        "tokens/s": int(tokens_per_second),  # 每秒token数
        "step_time": round(cost_time, 2),  # 在分布式集群中执行完一个BS的时间（秒）
        "cost_time": round(cost_time, 2),  # 完成一次推理过程所需的总时间（秒）
        "N": N,  # GPU/NPU的数量
        "BS": BS,  # 在一次训练或推理过程中同时处理的数据样本的数量
        "output_token_count": output_token_count,  # 模型生成的 token 总数
    }
    print(output)

    output_str = f"""
    "samples/s": {output["samples/s"]}/秒,
    "tokens/s": {output["tokens/s"]}/秒,
    "N": {output["N"]},
    "BS": {output["BS"]},
    "step_time": {output["step_time"]}/秒,
    "cost_time": {output["cost_time"]}/秒,
    "output_token_count": {output["output_token_count"]},
    """
    print(output_str)

    return response, history


# 对话
response, history = chat_with_model(input_text="讲个小故事", history=[])
print(response)


```

**输出结果**

```
{'samples/s': 0.14, 'tokens/s': 41, 'step_time': 7.2, 'cost_time': 7.2, 'N': 1, 'BS': 1, 'output_token_count': 297}


    "samples/s": 0.14/秒,
    "tokens/s": 41/秒,
    "N": 1,
    "BS": 1,
    "step_time": 7.2/秒,
    "cost_time": 7.2/秒,
    "output_token_count": 297,

有一个叫小明的男孩,他很喜欢看书。有一天,他走进了一个图书馆,里面有各种各样的书。小明看着这些书,感到非常兴奋。

他开始挑选自己喜欢的书,然后坐下来开始阅读。他看了一会儿,突然听到了一个声音,好像是从一本书里传出来的。

小明好奇地走到书架旁,发现是一本魔法书。他打开魔法书,发现里面写着:“只要你念出咒语,就可以变成任何东西。”

小明非常兴奋,他开始念咒语:“变身,变身,变身!”

突然,小明变成了一个巨大的老虎。他跑出了图书馆,开始在街上寻找猎物。

他遇到了一只小鸟,问它:“你愿意和我一起玩吗?”小鸟吓坏了,飞走了。

小明又遇到了一只兔子,问它:“你愿意和我一起玩吗?”兔子也吓坏了,逃跑了。

小明感到非常孤单,他不知道该怎么办。他想变回原来的样子,但他不知道该念什么咒语。

他开始四处寻找,最终找到了魔法书。他念出了咒语:“解除变身,解除变身,解除变身!”

突然,小明变回了原来的样子,他感到非常高兴。他决定以后不再玩魔法书了,因为他知道魔法书是有危险的。

小明回家后,把魔法书放在了自己的书架上,从此再也没有看过它。


```
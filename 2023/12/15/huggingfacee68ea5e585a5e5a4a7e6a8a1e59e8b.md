---
title: HuggingFaceæ¥å…¥å¤§æ¨¡å‹
date: '2023-12-15T02:32:23+00:00'
status: private
permalink: /2023/12/15/huggingface%e6%8e%a5%e5%85%a5%e5%a4%a7%e6%a8%a1%e5%9e%8b
author: æ¯›å·³ç…œ
excerpt: ''
type: post
id: 10621
category:
    - äººå·¥æ™ºèƒ½
tag: []
post_format: []
wb_sst_seo:
    - 'a:3:{i:0;s:0:"";i:1;s:0:"";i:2;s:0:"";}'
hestia_layout_select:
    - sidebar-right
---
HF(HuggingFace)æ¥å…¥å¤§æ¨¡å‹ã€[ä»£ç è°ƒç”¨](https://huggingface.co/THUDM/chatglm3-6b-32k#%E4%BB%A3%E7%A0%81%E8%B0%83%E7%94%A8-code-usage)ã€‘
==========================================================================================================================

### å‡†å¤‡æµ‹è¯•

```python
(vllm-0.2.2) [cloud@New-test1 (19:16:08) /mnt/data/siyu.mao/vllm-0.2.2]
â””â”€$ python
Python 3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>>


```

1. ```python
  # å¯¼å…¥æ‰€éœ€çš„åº“
  from transformers import AutoTokenizer, AutoModel
  
  ```
2. ```python
  # ä»é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„åŠ è½½åˆ†è¯å™¨ï¼ˆtokenizerï¼‰å’Œæ¨¡å‹ï¼ˆmodelï¼‰è¿™ä¸ªè¿‡ç¨‹ä¼šç¨å¾®æ…¢ä¸€ç‚¹
  tokenizer = AutoTokenizer.from_pretrained("/mnt/data/NewLLM/THUDM/chatglm3-6b-32k", trust_remote_code=True)
  model = AutoModel.from_pretrained("/mnt/data/NewLLM/THUDM/chatglm3-6b-32k", trust_remote_code=True).half().cuda()
  
  ```
3. ```python
  # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
  model = model.eval()
  
  ```
4. ```python
  # è¿›è¡Œå¯¹è¯äº¤äº’
  # ç¬¬ä¸€è½®å¯¹è¯
  response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
  print(response)
  # è¾“å‡º
  ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯ ChatGLM3-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
  
  ```
5. ```python
  # ç¬¬äºŒè½®å¯¹è¯ï¼Œä½¿ç”¨ä¸Šä¸€è½®çš„å†å²å¯¹è¯
  response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", history=history)
  print(response)
  # è¾“å‡º
  æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©äººæ„Ÿåˆ°ç„¦è™‘å’Œæ²®ä¸§ï¼Œä½†ä»¥ä¸‹æ–¹æ³•å¯èƒ½ä¼šå¸®åŠ©ä½ æ”¹å–„ç¡çœ ï¼š
  
  1. å»ºç«‹è§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨ï¼šæ¯å¤©å°½é‡åœ¨ç›¸åŒçš„æ—¶é—´ä¸ŠåºŠå’Œèµ·åºŠï¼ŒåŒ…æ‹¬å‘¨æœ«ã€‚
  2. åˆ›å»ºèˆ’é€‚çš„ç¡çœ ç¯å¢ƒï¼šç¡®ä¿ä½ çš„ç¡çœ ç¯å¢ƒå®‰é™ã€é»‘æš—ã€å‡‰çˆ½ä¸”èˆ’é€‚ï¼ŒåºŠå«å’Œæ•å¤´ä¹Ÿåº”è¯¥èˆ’é€‚ã€‚
  3. é¿å…åˆºæ¿€æ€§ç‰©è´¨ï¼šé¿å…åœ¨ç¡å‰é¥®ç”¨å’–å•¡ã€èŒ¶æˆ–å…¶ä»–å«å’–å•¡å› çš„é¥®æ–™ï¼Œä»¥åŠé¿å…é£Ÿç”¨è¿‡å¤šçš„ç³–åˆ†å’Œè„‚è‚ªã€‚
  4. æ”¾æ¾æŠ€å·§ï¼šå°è¯•ä½¿ç”¨ä¸€äº›æ”¾æ¾æŠ€å·§ï¼Œå¦‚æ·±å‘¼å¸ã€å†¥æƒ³ã€æ¸è¿›æ€§è‚Œè‚‰æ¾å¼›æˆ–ç‘œä¼½ï¼Œè¿™äº›æ–¹æ³•å¯ä»¥å¸®åŠ©ä½ æ”¾æ¾èº«å¿ƒå¹¶å…¥ç¡ã€‚
  5. é¿å…åœ¨åºŠä¸Šçœ‹ç”µè§†æˆ–ä½¿ç”¨ç”µè„‘ï¼šåœ¨åºŠä¸Šçœ‹ç”µè§†æˆ–ä½¿ç”¨ç”µè„‘å¯èƒ½ä¼šå½±å“ç¡çœ è´¨é‡ï¼Œå› æ­¤æœ€å¥½åœ¨åºŠä¸Šçœ‹ä¹¦æˆ–å¬ä¸€äº›æŸ”å’Œçš„éŸ³ä¹ã€‚
  
  å¦‚æœä»¥ä¸Šæ–¹æ³•éƒ½æ— æ³•å¸®åŠ©ä½ æ”¹å–„ç¡çœ ï¼Œå»ºè®®ä½ å’¨è¯¢åŒ»ç”Ÿæˆ–ä¸“ä¸šå¿ƒç†åŒ»ç”Ÿçš„æ„è§ã€‚
  
  ```

- - - - - -

- - - - - -

- - - - - -

æ¨¡å‹è¿”å›çš„æ•°æ®æ ¼å¼
=========

æœªè½¬ç 
---

```bash
{"text": "", "error_code": 0, "usage": {"prompt_tokens": 10, "completion_tokens": 1, "total_tokens": 11}, "finish_reason": null}
{"text": "", "error_code": 0, "usage": {"prompt_tokens": 10, "completion_tokens": 2, "total_tokens": 12}, "finish_reason": null}
{"text": "", "error_code": 0, "usage": {"prompt_tokens": 10, "completion_tokens": 3, "total_tokens": 13}, "finish_reason": null}
{"text": "\u6211\u662f", "error_code": 0, "usage": {"prompt_tokens": 10, "completion_tokens": 4, "total_tokens": 14}, "finish_reason": null}
{"text": "\u6211\u662f Chat", "error_code": 0, "usage": {"prompt_tokens": 10, "completion_tokens": 5, "total_tokens": 15}, "finish_reason": null}
{"text": "\u6211\u662f ChatGL", "error_code": 0, "usage": {"prompt_tokens": 10, "completion_tokens": 6, "total_tokens": 16}, "finish_reason": null}
{"text": "\u6211\u662f ChatGLM", "error_code": 0, "usage": {"prompt_tokens": 10, "completion_tokens": 7, "total_tokens": 17}, "finish_reason": null}
{"text": "\u6211\u662f ChatGLM3", "error_code": 0, "usage": {"prompt_tokens": 10, "completion_tokens": 8, "total_tokens": 18}, "finish_reason": null}
{"text": "\u6211\u662f ChatGLM3-", "error_code": 0, "usage": {"prompt_tokens": 10, "completion_tokens": 9, "total_tokens": 19}, "finish_reason": null}
{"text": "\u6211\u662f ChatGLM3-6", "error_code": 0, "usage": {"prompt_tokens": 10, "completion_tokens": 10, "total_tokens": 20}, "finish_reason": null}
{"text": "\u6211\u662f ChatGLM3-6B", "error_code": 0, "usage": {"prompt_tokens": 10, "completion_tokens": 11, "total_tokens": 21}, "finish_reason": null}
{"text": "\u6211\u662f ChatGLM3-6B\uff0c", "error_code": 0, "usage": {"prompt_tokens": 10, "completion_tokens": 12, "total_tokens": 22}, "finish_reason": null}
{"text": "\u6211\u662f ChatGLM3-6B\uff0c\u662f", "error_code": 0, "usage": {"prompt_tokens": 10, "completion_tokens": 13, "total_tokens": 23}, "finish_reason": null}
{"text": "\u6211\u662f ChatGLM3-6B\uff0c\u662f\u6e05\u534e\u5927\u5b66", "error_code": 0, "usage": {"prompt_tokens": 10, "completion_tokens": 14, "total_tokens": 24}, "finish_reason": null}
{"text": "\u6211\u662f ChatGLM3-6B\uff0c\u662f\u6e05\u534e\u5927\u5b66KE", "error_code": 0, "usage": {"prompt_tokens": 10, "completion_tokens": 15, "total_tokens": 25}, "finish_reason": null}
{"text": "\u6211\u662f ChatGLM3-6B\uff0c\u662f\u6e05\u534e\u5927\u5b66KEG", "error_code": 0, "usage": {"prompt_tokens": 10, "completion_tokens": 16, "total_tokens": 26}, "finish_reason": null}
{"text": "\u6211\u662f ChatGLM3-6B\uff0c\u662f\u6e05\u534e\u5927\u5b66KEG\u5b9e\u9a8c\u5ba4", "error_code": 0, "usage": {"prompt_tokens": 10, "completion_tokens": 17, "total_tokens": 27}, "finish_reason": "stop"}

```

**éœ€è¦åšå¤„ç†æ‰èƒ½è¢«ç”¨æˆ·ä½¿ç”¨**  
**FastChatè°ƒç”¨æ¨¡å‹çš„æºä»£ç **

```python
    total_len = 0
    for total_ids in model.stream_generate(**inputs, **gen_kwargs):
        total_ids = total_ids.tolist()[0]
        total_len = len(total_ids)
        if echo:
            output_ids = total_ids
        else:
            output_ids = total_ids[input_echo_len:]
        response = tokenizer.decode(output_ids)
        response = process_response(response)

        yield {
            "text": response,
            "usage": {
                "prompt_tokens": input_echo_len,
                "completion_tokens": total_len - input_echo_len,
                "total_tokens": total_len,
            },
            "finish_reason": None,
        }

```

**ä¸šåŠ¡å°è£…ï¼Œè°ƒç”¨FastChat Model Workerçš„ä»£ç **

```python
    # è¯·æ±‚ FastChat Worker å¹¶è¿›è¡Œæµå¼è¿”å›
    def generate_completion_stream(self, params):
        Logger.debug("è¯·æ±‚ FastChat Worker å¹¶è¿›è¡Œæµå¼è¿”å›")
        # è·å–å·¥ä½œåœ°å€
        worker_addr = self.controller.get_worker_address(self.model_name)
        # å‘é€POSTè¯·æ±‚åˆ°å·¥ä½œåœ°å€ä¸Šçš„worker_generate_streamç«¯ç‚¹ï¼Œä¼ å…¥å‚æ•°paramsï¼Œè®¾ç½®æµå¼ä¼ è¾“
        response = requests.post(
            worker_addr + "/worker_generate_stream",  # æ„é€ å®Œæ•´çš„å·¥ä½œåœ°å€
            json=params,  # å°†å‚æ•°paramsè½¬æ¢ä¸ºJSONæ ¼å¼å¹¶å‘é€
            stream=True,  # è®¾ç½®ä¸ºæµå¼ä¼ è¾“ï¼Œä»¥ä¾¿é€è¡Œæ¥æ”¶å“åº”
        )

        prev = 0  # åˆå§‹åŒ–å‰ä¸€æ¬¡è¾“å‡ºçš„é•¿åº¦
        # éå†å“åº”çš„æ•°æ®æµï¼Œä»¥'\0'ä½œä¸ºåˆ†éš”ç¬¦é€è¡Œå¤„ç†
        for chunk in response.iter_lines(delimiter=b"\0"):
            if chunk:  # å¦‚æœæ•°æ®å—éç©º
                data = json.loads(chunk)  # è§£æJSONæ•°æ®å—
                output = data["text"].strip()  # è·å–æ–‡æœ¬è¾“å‡ºå¹¶å»é™¤é¦–å°¾ç©ºç™½
                Logger.debug(output[prev:])  # è¾“å‡ºå½“å‰æ–‡æœ¬ç‰‡æ®µï¼ˆé¿å…é‡å¤è¾“å‡ºå·²ç»è¾“å‡ºè¿‡çš„éƒ¨åˆ†ï¼‰
                yield output[prev:]  # ä½¿ç”¨yieldè¿”å›å½“å‰æ–‡æœ¬ç‰‡æ®µï¼ˆé¿å…é‡å¤è¾“å‡ºå·²ç»è¾“å‡ºè¿‡çš„éƒ¨åˆ†ï¼‰
                prev = len(output)  # æ›´æ–°å‰ä¸€æ¬¡è¾“å‡ºçš„é•¿åº¦ï¼Œä»¥ä¾¿ä¸‹ä¸€æ¬¡è¾“å‡ºæ—¶æˆªå–æ­£ç¡®çš„éƒ¨åˆ†


```
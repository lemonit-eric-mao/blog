---
title: "ChatGLM2-6Bè°ƒç ”æ–‡æ¡£"
date: "2023-10-19"
categories: 
  - "äººå·¥æ™ºèƒ½"
---

# ChatGLM2-6Bè°ƒç ”æ–‡æ¡£

## **å‰ç½®æ¡ä»¶**

### å®‰è£…ç¯å¢ƒ

- Pythonç‰ˆæœ¬: >= 3.8.5, < 3.11
    
- Cudaç‰ˆæœ¬: >= 11.7, ä¸”èƒ½é¡ºåˆ©å®‰è£…Python
    

### **[å®‰è£…conda](http://www.dev-share.top/2023/10/26/%e5%ae%89%e8%a3%85miniconda/ "å®‰è£…conda")**

- è¿›å…¥condaç¯å¢ƒ
    
    ```bash
    (base) [cloud@gpuServer1 (15:17:54) ~]
    â””â”€# conda create -y -n chatglm2-6b-test python=3.10.13
    
    (base) [cloud@gpuServer1 (15:17:54) ~]
    â””â”€# conda activate chatglm2-6b-test
    
    ```
    

* * *

### **[å¼€å‘ç¯å¢ƒå‡†å¤‡](http://www.dev-share.top/2023/10/27/%e5%ae%89%e8%a3%85-jupyter/ "å¼€å‘ç¯å¢ƒå‡†å¤‡")**

* * *

## **[ChatGLM2-6B è¿è¡Œç¯å¢ƒå®‰è£…](https://github.com/THUDM/ChatGLM2-6B/tree/main#ç¯å¢ƒå®‰è£…)**

- é¦–å…ˆéœ€è¦ä¸‹è½½æœ¬ä»“åº“ï¼š
    
    ```bash
    (chatglm2-6b-test) [cloud@gpuServer1 (15:17:54) ~]
    â””â”€# git clone https://github.com/THUDM/ChatGLM2-6B
    
    (chatglm2-6b-test) [cloud@gpuServer1 (15:17:54) ~]
    â””â”€# cd ChatGLM2-6B
    ```
    
- ç„¶åä½¿ç”¨ pip å®‰è£…ä¾èµ–ï¼š
    
    ```bash
    (chatglm2-6b-test) [cloud@gpuServer1 (15:17:54) ~]
    â””â”€# python -m pip install -r requirements.txt
    
    ## ä¸‹è½½ä¸‹æ¥çš„ä¾èµ–æœ‰ 5.5G
    ```
    
- ä¸‹è½½å¤§æ¨¡å‹æƒé‡
    
    ```bash
    (chatglm2-6b-test) [cloud@gpuServer1 (19:42:46) /data/LLM]
    â””â”€# cd ChatGLM2-6B
    
    
    (chatglm2-6b-test) [cloud@gpuServer1 (19:42:46) /data/LLM/ChatGLM2-6B]
    â””â”€# mkdir THUDM && cd THUDM/
    
    
    ## ä¸‹è½½chatglm2-6båˆ°THUDM/ç›®å½•ä¸­
    git lfs install
    git clone https://huggingface.co/THUDM/chatglm2-6b
    
    ```
    
- å…¶ä¸­ `transformers` åº“ç‰ˆæœ¬æ¨èä¸º `4.30.2`ï¼Œ`torch` æ¨èä½¿ç”¨ 2.0 åŠä»¥ä¸Šçš„ç‰ˆæœ¬ï¼Œä»¥è·å¾—æœ€ä½³çš„æ¨ç†æ€§èƒ½ã€‚
    
- ![](images/LangChain03.png)
    

### å¯åŠ¨å¤§æ¨¡å‹WebæœåŠ¡

- #### `streamlit run web_demo2.py`
    
- ```bash
    (chatglm2-6b-test) [cloud@gpuServer1 (20:28:43) /data/LLM/ChatGLM2-6B]
    â””â”€$ streamlit run web_demo2.py
    
    You can now view your Streamlit app in your browser.
    
    Local URL: http://localhost:8501
    Network URL: http://10.10.0.2:8501
    
    ```
    
- #### ä½¿ç”¨æµè§ˆå™¨è®¿é—®
    
    - æ§åˆ¶å°æ—¥å¿—
        
    - ```bash
        ===================================BUG REPORT===================================
        Welcome to bitsandbytes. For bug reports, please run
        
        python -m bitsandbytes
        
        and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
        ================================================================================
        bin /home/cloud/.local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so
        /home/cloud/.local/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/cloud/miniconda3/envs/chatglm2-6b-tes1.0', 'libcudart.so.12.0'] as expected! Searching further paths...
        warn(msg)
        CUDA SETUP: CUDA runtime path found: /usr/local/cuda-12.1/lib64/libcudart.so
        CUDA SETUP: Highest compute capability among GPUs detected: 8.0
        CUDA SETUP: Detected CUDA version 121
        CUDA SETUP: Loading binary /home/cloud/.local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...
        Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
        
        ```
        
    - å‰ç«¯æ•ˆæœ
        
    - ![](images/LangChain04.png)
        
- > **`æ³¨æ„`**ï¼šChatGLM2-6Bé¡¹ç›®é»˜è®¤è¦æ±‚ä½¿ç”¨`CUDA`ï¼Œæˆ‘å°è¯•ä½¿ç”¨`CPU`ä½†`æ¨ç†å¤±è´¥`äº†ï¼Œç›®å‰åŸå› ä¸æ˜ï¼Œå› ä¸ºä½¿ç”¨`CPUæ¨ç†`ä¹Ÿä¸é‡è¦ï¼Œæ‰€ä»¥åæ¥æ”¹ä¸ºä½¿ç”¨`GPU`ã€‚
    

### åªå¯åŠ¨æœåŠ¡ç«¯

- ```bash
    (chatglm2-6b-test) [cloud@gpuServer1 (13:54:04) /data/LLM/ChatGLM2-6B]
    â””â”€$ python api.py
    
    ===================================BUG REPORT===================================
    Welcome to bitsandbytes. For bug reports, please run
    
    python -m bitsandbytes
    
    and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
    ================================================================================
    bin /home/cloud/.local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so
    /home/cloud/.local/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/cloud/miniconda3/envs/chatglm2-6b-test did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
    warn(msg)
    CUDA SETUP: CUDA runtime path found: /usr/local/cuda-12.1/lib64/libcudart.so
    CUDA SETUP: Highest compute capability among GPUs detected: 8.0
    CUDA SETUP: Detected CUDA version 121
    CUDA SETUP: Loading binary /home/cloud/.local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...
    Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  1.08s/it]
    INFO:     Started server process [175595]
    INFO:     Waiting for application startup.
    INFO:     Application startup complete.
    INFO:     Uvicorn running on http://0.0.0.0:7861 (Press CTRL+C to quit)
    
    
    ```
    
- #### ä½¿ç”¨å‘½ä»¤è¡Œæµ‹è¯•
    
- éƒ¨ç½²åœ¨æœ¬åœ°çš„`7861`ç«¯å£(é»˜è®¤ç«¯å£ä¸º8000)ï¼Œé€šè¿‡ POST æ–¹æ³•è¿›è¡Œè°ƒç”¨
    
    ```bash
    (chatglm2-6b-test) [cloud@gpuServer1 (15:18:48) /data/LLM/ChatGLM2-6B]
    â””â”€$ curl -sX POST "http://127.0.0.1:7861" \
       -H 'Content-Type: application/json' \
       -d '{"prompt": "ä½ å¥½", "history": []}' | jq
    
    
    ## è¿”å›å¦‚ä¸‹ä¿¡æ¯
    {
    "response": "ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚",
    "history": [
      [
        "ä½ å¥½",
        "ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"
      ]
    ],
    "status": 200,
    "time": "2023-10-23 14:28:07"
    }
    
    ```
    

* * *

### web\_demo2.pyæ–‡ä»¶è¯´æ˜(Streamlit)

- ```python
    # å¼•å…¥å¿…è¦çš„åº“
    from transformers import AutoModel, AutoTokenizer  # ç”¨äºå¤„ç†æ¨¡å‹å’Œåˆ†è¯
    import streamlit as st  # ç”¨äºæ„å»ºStreamlitåº”ç”¨
    
    # é…ç½®Streamlitåº”ç”¨çš„æ ‡é¢˜ã€å›¾æ ‡å’Œå¸ƒå±€
    st.set_page_config(
      page_title="ChatGLM2-6b æ¼”ç¤º",  # è®¾ç½®åº”ç”¨æ ‡é¢˜
      page_icon=":robot:",  # è®¾ç½®åº”ç”¨å›¾æ ‡
      layout='wide'  # è®¾ç½®å¸ƒå±€ä¸ºå®½å±
    )
    
    # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºä»è¿œç¨‹åŠ è½½æ¨¡å‹
    @st.cache_resource
    def get_model():
      tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)  # åŠ è½½åˆ†è¯å™¨
      model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True).cuda()  # åŠ è½½æ¨¡å‹å¹¶å°†å…¶æ”¾åœ¨GPUä¸Š
      # è‹¥è¦å¯ç”¨å¤šæ˜¾å¡æ”¯æŒï¼Œå¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä¸¤è¡Œä»£ç æ›¿ä»£ä¸Šä¸€è¡Œï¼Œå¹¶æ ¹æ®å®é™…æ˜¾å¡æ•°é‡æ›´æ”¹num_gpusçš„å€¼
      # from utils import load_model_on_gpus
      # model = load_model_on_gpus("THUDM/chatglm2-6b", num_gpus=2)
      model = model.eval()  # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
      return tokenizer, model
    
    # è·å–åˆ†è¯å™¨å’Œæ¨¡å‹
    tokenizer, model = get_model()
    
    # è®¾ç½®Streamlitåº”ç”¨çš„æ ‡é¢˜
    st.title("ChatGLM2-6B")
    
    # æ·»åŠ æ»‘åŠ¨æ¡æ¥è°ƒæ•´å‚æ•°
    max_length = st.sidebar.slider(
      'max_length', 0, 32768, 8192, step=1  # æœ€å¤§é•¿åº¦çš„æ»‘åŠ¨æ¡
    )
    top_p = st.sidebar.slider(
      'top_p', 0.0, 1.0, 0.8, step=0.01  # top_på‚æ•°çš„æ»‘åŠ¨æ¡
    )
    temperature = st.sidebar.slider(
      'temperature', 0.0, 1.0, 0.8, step=0.01  # temperatureå‚æ•°çš„æ»‘åŠ¨æ¡
    )
    
    # åˆå§‹åŒ–ä¼šè¯å†å²å’Œè¿‡å»çš„é”®å€¼
    if 'history' not in st.session_state:
      st.session_state.history = []
    
    if 'past_key_values' not in st.session_state:
      st.session_state.past_key_values = None
    
    # éå†å†å²æ¶ˆæ¯å¹¶å°†å…¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸­
    for i, (query, response) in enumerate(st.session_state.history):
      with st.chat_message(name="user", avatar="user"):
          st.markdown(query)  # æ˜¾ç¤ºç”¨æˆ·çš„æ¶ˆæ¯
      with st.chat_message(name="assistant", avatar="assistant"):
          st.markdown(response)  # æ˜¾ç¤ºåŠ©æ‰‹çš„å›å¤
    with st.chat_message(name="user", avatar="user"):
      input_placeholder = st.empty()  # æ˜¾ç¤ºç”¨æˆ·è¾“å…¥æ¶ˆæ¯çš„åŒºåŸŸ
    with st.chat_message(name="assistant", avatar="assistant"):
      message_placeholder = st.empty()  # æ˜¾ç¤ºåŠ©æ‰‹å›å¤çš„åŒºåŸŸ
    
    # è·å–ç”¨æˆ·è¾“å…¥
    prompt_text = st.text_area(label="ç”¨æˆ·å‘½ä»¤è¾“å…¥",
                             height=100,
                             placeholder="è¯·åœ¨è¿™å„¿è¾“å…¥æ‚¨çš„å‘½ä»¤")
    
    # æ·»åŠ â€œå‘é€â€æŒ‰é’®
    button = st.button("å‘é€", key="predict")
    
    # å½“æŒ‰é’®è¢«ç‚¹å‡»æ—¶æ‰§è¡Œä»¥ä¸‹æ“ä½œ
    if button:
      input_placeholder.markdown(prompt_text)  # åœ¨ç”¨æˆ·è¾“å…¥åŒºåŸŸä¸­æ˜¾ç¤ºç”¨æˆ·çš„æ¶ˆæ¯
      history, past_key_values = st.session_state.history, st.session_state.past_key_values
      # ä½¿ç”¨æ¨¡å‹ç”ŸæˆèŠå¤©å“åº”
      for response, history, past_key_values in model.stream_chat(tokenizer, prompt_text, history,
                                                                  past_key_values=past_key_values,
                                                                  max_length=max_length, top_p=top_p,
                                                                  temperature=temperature,
                                                                  return_past_key_values=True):
          message_placeholder.markdown(response)  # åœ¨åŠ©æ‰‹å›å¤åŒºåŸŸä¸­æ˜¾ç¤ºèŠå¤©å“åº”
    
      st.session_state.history = history  # æ›´æ–°å†å²è®°å½•
      st.session_state.past_key_values = past_key_values  # æ›´æ–°è¿‡å»çš„é”®å€¼
    
    ```
    

### api.pyæ–‡ä»¶è¯´æ˜(FastAPI)

- ```python
    # å¼•å…¥å¿…è¦çš„åº“
    from fastapi import FastAPI, Request  # ç”¨äºæ„å»ºFastAPIåº”ç”¨å’Œå¤„ç†HTTPè¯·æ±‚
    from transformers import AutoTokenizer, AutoModel  # ç”¨äºå¤„ç†æ¨¡å‹å’Œåˆ†è¯
    import uvicorn  # ç”¨äºè¿è¡ŒFastAPIåº”ç”¨
    import json  # ç”¨äºå¤„ç†JSONæ•°æ®
    import datetime  # ç”¨äºå¤„ç†æ—¥æœŸå’Œæ—¶é—´
    import torch  # ç”¨äºå¤„ç†PyTorchæ¨¡å‹
    
    # è®¾ç½®GPUè®¾å¤‡å’ŒID
    DEVICE = "cuda"
    DEVICE_ID = "0"
    CUDA_DEVICE = f"{DEVICE}:{DEVICE_ID}" if DEVICE_ID else DEVICE
    
    # æ¸…ç†GPUå†…å­˜
    def torch_gc():
      if torch.cuda.is_available():
          with torch.cuda.device(CUDA_DEVICE):
              torch.cuda.empty_cache()
              torch.cuda.ipc_collect()
    
    # åˆ›å»ºFastAPIåº”ç”¨
    app = FastAPI()
    
    # åˆ›å»ºPOSTè¯·æ±‚å¤„ç†å‡½æ•°
    @app.post("/")
    async def create_item(request: Request):
      global model, tokenizer
      json_post_raw = await request.json()  # ä»HTTPè¯·æ±‚ä¸­æå–JSONæ•°æ®
      json_post = json.dumps(json_post_raw)  # å°†JSONæ•°æ®è½¬æ¢ä¸ºå­—ç¬¦ä¸²
      json_post_list = json.loads(json_post)  # å°†JSONå­—ç¬¦ä¸²è§£æä¸ºPythonå­—å…¸
      prompt = json_post_list.get('prompt')  # è·å–ç”¨æˆ·è¾“å…¥çš„æç¤ºä¿¡æ¯
      history = json_post_list.get('history')  # è·å–èŠå¤©å†å²è®°å½•
      max_length = json_post_list.get('max_length')  # è·å–æœ€å¤§ç”Ÿæˆæ–‡æœ¬é•¿åº¦
      top_p = json_post_list.get('top_p')  # è·å–top_på‚æ•°
      temperature = json_post_list.get('temperature')  # è·å–temperatureå‚æ•°
    
      # ä½¿ç”¨æ¨¡å‹ç”ŸæˆèŠå¤©å“åº”
      response, history = model.chat(tokenizer,
                                     prompt,
                                     history=history,
                                     max_length=max_length if max_length else 2048,  # è®¾ç½®é»˜è®¤çš„æœ€å¤§é•¿åº¦ä¸º2048
                                     top_p=top_p if top_p else 0.7,  # è®¾ç½®é»˜è®¤çš„top_på€¼ä¸º0.7
                                     temperature=temperature if temperature else 0.95)  # è®¾ç½®é»˜è®¤çš„temperatureå€¼ä¸º0.95
    
      now = datetime.datetime.now()  # è·å–å½“å‰æ—¥æœŸå’Œæ—¶é—´
      time = now.strftime("%Y-%m-d %H:%M:%S")  # å°†æ—¥æœŸå’Œæ—¶é—´æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
    
      # æ„å»ºå“åº”æ•°æ®
      answer = {
          "response": response,  # èŠå¤©æ¨¡å‹ç”Ÿæˆçš„å“åº”
          "history": history,  # æ›´æ–°åçš„èŠå¤©å†å²è®°å½•
          "status": 200,  # HTTPå“åº”çŠ¶æ€ç ï¼ˆæˆåŠŸï¼‰
          "time": time  # ç”Ÿæˆå“åº”çš„æ—¶é—´æˆ³
      }
    
      # åˆ›å»ºæ—¥å¿—ä¿¡æ¯
      log = "[" + time + "] " + '", prompt:"' + prompt + '", response:"' + repr(response) + '"'
    
      print(log)  # æ‰“å°æ—¥å¿—ä¿¡æ¯åˆ°æ§åˆ¶å°
    
      torch_gc()  # æ‰§è¡ŒGPUå†…å­˜æ¸…ç†æ“ä½œ
    
      return answer  # è¿”å›å“åº”æ•°æ®
    
    # ä¸»ç¨‹åºå…¥å£
    if __name__ == '__main__':
      tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)  # åŠ è½½åˆ†è¯å™¨
      model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True).cuda()  # åŠ è½½æ¨¡å‹å¹¶å°†å…¶æ”¾åœ¨GPUä¸Š
      # è‹¥è¦å¯ç”¨å¤šæ˜¾å¡æ”¯æŒï¼Œå¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä¸‰è¡Œä»£ç æ›¿ä»£ä¸Šé¢ä¸¤è¡Œï¼Œå¹¶æ ¹æ®å®é™…æ˜¾å¡æ•°é‡æ›´æ”¹num_gpusçš„å€¼
      # model_path = "THUDM/chatglm2-6b"
      # tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
      # model = load_model_on_gpus(model_path, num_gpus=2)
      model.eval()  # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
      uvicorn.run(app, host='0.0.0.0', port=7861, workers=1)  # è¿è¡ŒFastAPIåº”ç”¨
    
    ```
    
- ![](images/LangChain05.png)

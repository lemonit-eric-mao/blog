# [ollama 简介与应用](https://ollama.com/)



## 简介

`ollama` 是一个用于加载大语言模型的工具。大多数大语言模型本身并不直接提供 `REST API` 的访问能力，这在实际应用中可能不太方便。因此，我们需要一个能够通过 HTTP 请求轻松调用模型计算资源的工具，而 `ollama` 就提供了这种便利，使得开发者可以更加便捷地访问和使用大语言模型。



---



## 应用

[官方安装文档](https://github.com/ollama/ollama/blob/main/docs/linux.md)

### 下载工具



#### 在线安装(可选)

``` bash
curl -fsSL https://ollama.com/install.sh | sh
```



#### [下载离线包(推荐)](https://github.com/ollama/ollama/releases/tag/v0.3.11)

``` bash
wget https://github.com/ollama/ollama/releases/download/v0.3.11/ollama-linux-amd64.tgz

[cloud@ecs-MIy69r (20:11:40) /data/siyu.mao/ollama-svc]
└─$ cd ollama-0.3.11/


# 包中包含了ollama执行需要的依赖一并解压到/usr目录中
[cloud@ecs-MIy69r (20:12:09) /data/siyu.mao/ollama-svc/ollama-0.3.11]
└─$ tar -C /usr -xzf ollama-linux-amd64.tgz


[cloud@ecs-MIy69r (20:18:12) ~]
└─$ which ollama
/usr/bin/ollama

```



### 配置服务



#### `ollama`可选环境变量

```bash
Environment Variables:
      OLLAMA_DEBUG               Show additional debug information (e.g. OLLAMA_DEBUG=1)  # 显示额外的调试信息（例如：OLLAMA_DEBUG=1）
      OLLAMA_HOST                IP Address for the ollama server (default 127.0.0.1:11434)  # ollama 服务器的 IP 地址（默认 127.0.0.1:11434）
      OLLAMA_KEEP_ALIVE          The duration that models stay loaded in memory (default "5m")  # 模型在内存中保持加载的持续时间（默认 "5m"）
      OLLAMA_MAX_LOADED_MODELS   Maximum number of loaded models per GPU  # 每个 GPU 最大加载模型数量
      OLLAMA_MAX_QUEUE           Maximum number of queued requests  # 最大排队请求数量
      OLLAMA_MODELS              The path to the models directory  # 模型目录的路径
      OLLAMA_NUM_PARALLEL        Maximum number of parallel requests  # 最大并行请求数量
      OLLAMA_NOPRUNE             Do not prune model blobs on startup  # 启动时不修剪模型数据
      OLLAMA_ORIGINS             A comma separated list of allowed origins  # 允许的来源列表，以逗号分隔
      OLLAMA_SCHED_SPREAD        Always schedule model across all GPUs  # 始终在所有 GPU 之间调度模型
      OLLAMA_TMPDIR              Location for temporary files  # 临时文件的存放位置
      OLLAMA_FLASH_ATTENTION     Enabled flash attention  # 启用闪存注意力机制
      OLLAMA_LLM_LIBRARY         Set LLM library to bypass autodetection  # 设置 LLM 库以绕过自动检测
      OLLAMA_GPU_OVERHEAD        Reserve a portion of VRAM per GPU (bytes)  # 每个 GPU 保留一定的显存（字节）
      OLLAMA_LOAD_TIMEOUT        How long to allow model loads to stall before giving up (default "5m")  # 允许模型加载停滞的最长时间，超过后放弃（默认 "5m"）
```
打印环境变量
``` bash
cat show_env.sh
#!/bin/bash

# 环境变量名称列表
variables=(
  "OLLAMA_DEBUG"
  "OLLAMA_HOST"
  "OLLAMA_KEEP_ALIVE"
  "OLLAMA_MAX_LOADED_MODELS"
  "OLLAMA_MAX_QUEUE"
  "OLLAMA_MODELS"
  "OLLAMA_NUM_PARALLEL"
  "OLLAMA_NOPRUNE"
  "OLLAMA_ORIGINS"
  "OLLAMA_SCHED_SPREAD"
  "OLLAMA_TMPDIR"
  "OLLAMA_FLASH_ATTENTION"
  "OLLAMA_LLM_LIBRARY"
  "OLLAMA_GPU_OVERHEAD"
  "OLLAMA_LOAD_TIMEOUT"
)

# 遍历并打印每个环境变量的值
for var in "${variables[@]}"; do
  value=${!var}
  if [ -z "$value" ]; then
    echo "$var: 未设置"
  else
    echo "$var: $value"
  fi
done


bash show_env.sh
```



#### 配置`ollama`服务端环境变量，修改默认下载路径

``` bash
mkdir -p /data/LLM/ollama/models

tee > /etc/ollama.env << ERIC

OLLAMA_HOST=0.0.0.0:11434
OLLAMA_MODELS=/data/LLM/ollama/models

ERIC

# 需要授权
sudo chmod 777 /etc/ollama.env
sudo chmod 777 /data/LLM/ollama/models
```

也可以使用`systemctl edit ollama.service`命令来添加环境变量



##### 然后将环境变量，添加服务自启动脚本

``` bash
tee > /etc/systemd/system/ollama.service << ERIC
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
EnvironmentFile=/etc/ollama.env
ExecStart=/usr/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="PATH=\$PATH"

[Install]
WantedBy=default.target

ERIC
```



##### 启动服务并添加自启动

``` bash
sudo systemctl daemon-reload
sudo systemctl enable ollama
sudo systemctl restart ollama
sudo systemctl status ollama
```



##### 查看

``` bash
(base) root@ecs-MIy69r:~# ss -lntp | grep ollama
LISTEN 0      4096               *:11434            *:*    users:(("ollama",pid=717136,fd=3))

```



---



由于`Ollama`默认不能适配所有模型，所以需要对模型进行转换，有两种方式：

1. 使用llama.cpp；
2. 从`ollama`官网直接下载



### `在线`加载模型

``` bash
[cloud@ecs-MIy69r (09:12:12) ~]
└─$ ollama pull vicuna:7b

[cloud@ecs-MIy69r (09:12:12) ~]
└─$ ollama pull llama3.2:1b

[cloud@ecs-MIy69r (09:12:12) ~]
└─$ ollama pull glm4:9b

pulling manifest
pulling b506a070d115... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 5.5 GB
pulling e7e7aebd710c... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  137 B
pulling e4f0dc83900a... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 6.5 KB
pulling 4134f3eb0516... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏   81 B
pulling ca0dd08dd282... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  489 B
verifying sha256 digest
writing manifest
success

```



#### 查看相关信息

``` bash
[cloud@ecs-MIy69r (11:13:51) ~]
└─$ ollama list
NAME           ID              SIZE      MODIFIED
llama3.2:1b    baf6a787fdff    1.3 GB    33 seconds ago
vicuna:7b      370739dc897b    3.8 GB    20 hours ago
glm4:9b        5b699761eca5    5.5 GB    22 hours ago
```

``` bash
# 模型的功能，会在模板中列出
[cloud@ecs-MIy69r (11:14:20) ~]
└─$ ollama show glm4:9b --template


[gMASK]<sop>{{ if .System }}<|system|>
{{ .System }}{{ end }}{{ if .Prompt }}<|user|>
{{ .Prompt }}{{ end }}<|assistant|>
{{ .Response }}

```

``` bash
# 模型的功能，会在模板中列出
[cloud@ecs-MIy69r (11:14:20) ~]
└─$ ollama show vicuna:7b --template


{{ .System }}
USER: {{ .Prompt }}
ASSISTANT:
```

``` bash
# 模型的功能，会在模板中列出
[cloud@ecs-MIy69r (11:14:20) ~]
└─$ ollama show llama3.2:1b --template


<|start_header_id|>system<|end_header_id|>

Cutting Knowledge Date: December 2023

{{ if .System }}{{ .System }}
{{- end }}
{{- if .Tools }}When you receive a tool call response, use the output to format an answer to the orginal user question.

You are a helpful assistant with tool calling capabilities.
{{- end }}<|eot_id|>
{{- range $i, $_ := .Messages }}
{{- $last := eq (len (slice $.Messages $i)) 1 }}
{{- if eq .Role "user" }}<|start_header_id|>user<|end_header_id|>
{{- if and $.Tools $last }}

Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.

Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}. Do not use variables.

{{ range $.Tools }}
{{- . }}
{{ end }}
{{ .Content }}<|eot_id|>
{{- else }}

{{ .Content }}<|eot_id|>
{{- end }}{{ if $last }}<|start_header_id|>assistant<|end_header_id|>

{{ end }}
{{- else if eq .Role "assistant" }}<|start_header_id|>assistant<|end_header_id|>
{{- if .ToolCalls }}
{{ range .ToolCalls }}
{"name": "{{ .Function.Name }}", "parameters": {{ .Function.Arguments }}}{{ end }}
{{- else }}

{{ .Content }}
{{- end }}{{ if not $last }}<|eot_id|>{{ end }}
{{- else if eq .Role "tool" }}<|start_header_id|>ipython<|end_header_id|>

{{ .Content }}<|eot_id|>{{ if $last }}<|start_header_id|>assistant<|end_header_id|>

{{ end }}
{{- end }}
{{- end }}

```









---



### 使用命令行启动模型



#### 命令介绍

``` bash
[cloud@ecs-MIy69r (11:11:14) ~]
└─$ ollama run -h
Run a model  # 运行模型

Usage:
  ollama run MODEL [PROMPT] [flags]  # 使用方法：调用 ollama run 命令，后面跟模型名称、提示和标志

Flags:
      --format string      Response format (e.g. json)  # 响应格式（例如：json）
  -h, --help               help for run  # 运行命令的帮助信息
      --insecure           Use an insecure registry  # 使用不安全的注册表
      --keepalive string   Duration to keep a model loaded (e.g. 5m)  # 保持模型加载的持续时间（例如：5m）
      --nowordwrap         Don't wrap words to the next line automatically  # 不自动换行
      --verbose            Show timings for response  # 显示响应的时间

Environment Variables:
      OLLAMA_HOST                IP Address for the ollama server (default 127.0.0.1:11434)  # ollama 服务器的 IP 地址（默认 127.0.0.1:11434）
      OLLAMA_NOHISTORY           Do not preserve readline history  # 不保留 readline 历史记录

```



#### 启动模型并测试

``` bash
(base) root@ecs-MIy69r:~# ollama run glm4:9b --keepalive='-1m'
>>> Send a message (/? for help)
# 可以开始对话了
```

``` bash
(base) root@ecs-MIy69r:~# ollama run vicuna:7b --keepalive='-1m'
>>> Send a message (/? for help)
# 可以开始对话了
```



#### 查看

``` bash
(base) root@ecs-MIy69r:~# ollama ps
NAME         ID              SIZE      PROCESSOR    UNTIL
vicuna:7b    370739dc897b    9.4 GB    100% GPU     Forever
glm4:9b      5b699761eca5    6.6 GB    100% GPU     Forever

```



#### 停止模型运行

``` bash
[cloud@ecs-MIy69r (10:45:16) /data/siyu.mao/ollama-svc]
└─$ ollama stop glm4:9b

```



#### 查看

``` bash
[cloud@ecs-MIy69r (10:45:30) /data/siyu.mao/ollama-svc]
└─$ ollama ps
NAME         ID              SIZE      PROCESSOR    UNTIL
vicuna:7b    370739dc897b    9.4 GB    100% GPU     Forever
```





---



### 使用API启动模型[官方文档](https://github.com/ollama/ollama/blob/main/docs/api.md)

``` bash
# 查看正在运行的模型
[cloud@ecs-MIy69r (11:01:37) /data/siyu.mao/ollama-svc]
└─$ ollama ps
NAME         ID              SIZE      PROCESSOR    UNTIL
vicuna:7b    370739dc897b    9.4 GB    100% GPU     Forever
```

``` bash
# 启动模型(流式返回)
curl -X POST http://localhost:11434/api/generate -d '{
  "model": "glm4:9b",
  "stream": true,
  "options": {
    "num_ctx": 4096
  }
}'

# 返回的提示信息
{"model":"glm4:9b","created_at":"2024-09-26T03:01:58.1143841Z","response":"","done":true,"done_reason":"load"}
```

默认情况下，`Ollama` 使用的上下文窗口大小为 2048 个令牌, 通过指定参数：`num_ctx`修改上下文长度

``` bash
# 查看正在运行的模型
[cloud@ecs-MIy69r (11:01:58) /data/siyu.mao/ollama-svc]
└─$ ollama ps
NAME         ID              SIZE      PROCESSOR    UNTIL
vicuna:7b    370739dc897b    9.4 GB    100% GPU     Forever
glm4:9b      5b699761eca5    6.6 GB    100% GPU     4 minutes from now

```



#### 聊天请求（流式处理）[官方文档](https://github.com/ollama/ollama/blob/main/docs/api.md#chat-request-streaming)

``` bash
curl -X POST http://localhost:11434/api/chat \
    -H "Content-Type: application/json" \
    -d '{
        "model": "glm4:9b",
        "stream": true,
        "messages": [
            {
                "role": "user",
                "content": "简短回答你是谁？"
            }
        ]
    }'
```

``` bash
{"model":"glm4:9b","created_at":"2024-09-26T03:22:50.382175762Z","message":{"role":"assistant","content":"我是一个"},"done":false}
{"model":"glm4:9b","created_at":"2024-09-26T03:22:50.382206434Z","message":{"role":"assistant","content":"人工智能"},"done":false}
{"model":"glm4:9b","created_at":"2024-09-26T03:22:50.382471659Z","message":{"role":"assistant","content":"助手"},"done":false}
{"model":"glm4:9b","created_at":"2024-09-26T03:22:50.390865545Z","message":{"role":"assistant","content":"。"},"done":false}
{
    "model": "glm4:9b","created_at": "2024-09-26T03:22:50.401195138Z","message": {"role": "assistant","content": ""},
    "done_reason": "stop",
    "done": true,
    "total_duration": 132544500,
    "load_duration": 26660036,
    "prompt_eval_count": 12,
    "prompt_eval_duration": 19620000,
    "eval_count": 5,
    "eval_duration": 42740000
}

```



#### 聊天请求（无流式处理）

``` bash
curl -X POST http://localhost:11434/api/chat \
    -H "Content-Type: application/json" \
    -d '{
        "model": "glm4:9b",
        "stream": false,
        "messages": [
            {
                "role": "user",
                "content": "简短回答你是谁？"
            }
        ]
    }'
```

``` bash
{
    "model": "glm4:9b","created_at": "2024-09-26T03:33:25.909537303Z","message": {"role": "assistant","content": "我是一个人工智能助手。"},
    "done_reason": "stop",
    "done": true,
    "total_duration": 3160826744,
    "load_duration": 3055524512,
    "prompt_eval_count": 12,
    "prompt_eval_duration": 18821000,
    "eval_count": 5,
    "eval_duration": 43814000
}
```



#### 聊天请求（带工具）[官方文档](https://github.com/ollama/ollama/blob/main/docs/api.md#chat-request-with-tools)

``` bash
curl -X POST http://localhost:11434/api/chat -d '{
  "model": "llama3.2:1b",
  "messages": [
    {
      "role": "user",
      "content": "今天巴黎的天气怎么样？"
    }
  ],
  "stream": false,
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_current_weather",
        "description": "获取某个地点的当前天气",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "获取天气的地点，例如：旧金山，加利福尼亚"
            },
            "format": {
              "type": "string",
              "description": "返回天气的格式，例如：'摄氏'或'华氏'",
              "enum": ["摄氏", "华氏"]
            }
          },
          "required": ["location", "format"]
        }
      }
    }
  ]
}'

```

``` bash
{
    "model": "llama3.2:1b",
    "created_at": "2024-09-26T04:17:58.13511971Z",
    "message": {
        "role": "assistant",
        "content": "",
        "tool_calls": [
            {
                "function": {
                    "name": "get_current_weather",
                    "arguments": {
                        "format": "华氏",
                        "location": "巴黎"
                    }
                }
            }
        ]
    },
    "done_reason": "stop",
    "done": true,
    "total_duration": 168079067,
    "load_duration": 17841522,
    "prompt_eval_count": 212,
    "prompt_eval_duration": 5746000,
    "eval_count": 24,
    "eval_duration": 102432000
}
```

glm4-9b模型暂不支持function功能`{"error":"glm4:9b does not support tools"}`



#### 停止模型运行

``` bash
# 停止模型
curl -X POST http://localhost:11434/api/generate -d '{
  "model": "glm4:9b",
  "keep_alive": 0
}'

# 返回的提示信息
{"model":"glm4:9b","created_at":"2024-09-26T03:10:02.910751886Z","response":"","done":true,"done_reason":"unload"}
```





---



### `本地`加载模型



#### 命令介绍

``` bash
[cloud@ecs-MIy69r (12:23:01) ~]
└─$ ollama create -h

Create a model from a Modelfile  # 从 Modelfile 创建模型

Usage:
  ollama create MODEL [flags]  # 使用方法：调用 ollama create 命令，后面跟模型名称和标志

Flags:
  -f, --file string       Name of the Modelfile (default "Modelfile")  # Modelfile 的名称（默认 "Modelfile"）
  -h, --help              help for create  # 创建命令的帮助信息
  -q, --quantize string   Quantize model to this level (e.g. q4_0)  # 将模型量化到指定级别（例如：q4_0）

Environment Variables:
      OLLAMA_HOST                IP Address for the ollama server (default 127.0.0.1:11434)  # ollama 服务器的 IP 地址（默认 127.0.0.1:11434）

```



#### 创建`Modelfile`文件来配置本地模型

``` bash
tee > Modelfile << ERIC

FROM /data/LLM/Qwen/Qwen2-1.5B-Instruct

ERIC

```



#### 加载模型

``` bash
ollama create Qwen2-1.5B-Instruct -f Modelfile
```



#### 启动模型

``` bash
ollama run Qwen2-1.5B-Instruct
```



---





### 列出本地模型[官方文档](https://github.com/ollama/ollama/blob/main/docs/api.md#list-local-models)

``` bash
curl http://localhost:11434/api/tags
```

``` bash
{
    "models": [
        {
            "name": "llama3.2:1b",
            "model": "llama3.2:1b",
            "modified_at": "2024-09-26T12:14:31.129442163+08:00",
            "size": 1321098329,
            "digest": "baf6a787fdffd633537aa2eb51cfd54cb93ff08e28040095462bb63daf552878",
            "details": {
                "parent_model": "",
                "format": "gguf",
                "family": "llama",
                "families": [
                    "llama"
                ],
                "parameter_size": "1.2B",
                "quantization_level": "Q8_0"
            }
        },
        {
            "name": "vicuna:7b",
            "model": "vicuna:7b",
            "modified_at": "2024-09-25T16:30:05.464799583+08:00",
            "size": 3825807726,
            "digest": "370739dc897bba0188d390bb7659e48d926cacc320875136d0115228023b1590",
            "details": {
                "parent_model": "",
                "format": "gguf",
                "family": "llama",
                "families": null,
                "parameter_size": "7B",
                "quantization_level": "Q4_0"
            }
        },
        {
            "name": "glm4:9b",
            "model": "glm4:9b",
            "modified_at": "2024-09-25T14:41:50.585731061+08:00",
            "size": 5455326235,
            "digest": "5b699761eca535dc55047ad9d2dbf54e3b8697709419ef78a70503ed4bfbcf44",
            "details": {
                "parent_model": "",
                "format": "gguf",
                "family": "chatglm",
                "families": [
                    "chatglm"
                ],
                "parameter_size": "9.4B",
                "quantization_level": "Q4_0"
            }
        }
    ]
}
```



### 显示模型信息

``` bash
curl -X POST http://localhost:11434/api/show -d '{
  "name": "glm4:9b"
}'
```



### 列出正在运行的模型

``` bash
curl http://localhost:11434/api/ps
```

``` bash
{
  "models": [
    {
      "name": "vicuna:7b",
      "model": "vicuna:7b",
      "size": 9360795648,
      "digest": "370739dc897bba0188d390bb7659e48d926cacc320875136d0115228023b1590",
      "details": {
        "parent_model": "",
        "format": "gguf",
        "family": "llama",
        "families": null,
        "parameter_size": "7B",
        "quantization_level": "Q4_0"
      },
      "expires_at": "2317-01-05T20:51:05.179693023+08:00",
      "size_vram": 9360795648
    }
  ]
}
```







---



## 卸载



#### 删除 ollama 服务：

```bash
sudo systemctl stop ollama
sudo systemctl disable ollama
sudo rm /etc/systemd/system/ollama.service
```



#### 从 bin 目录中删除 `ollama` 二进制文件（ `/usr/local/bin`, `/usr/bin`, 或`/bin` ）

```bash
sudo rm $(which ollama)
```



删除下载的模型以及 `Ollama` 服务用户和组：

```bash
sudo rm -r /usr/share/ollama
sudo userdel ollama
sudo groupdel ollama
```




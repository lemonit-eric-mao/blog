# 模板

`Ollama` 提供了一个强大的模板引擎，基于 Go 的内置模板系统，旨在为大型语言模型构建提示。这一特性可以充分发挥模型的潜力。

## 基本模板结构

一个基本的 Go 模板由三个主要部分组成：

* **布局**：模板的整体结构。
* **变量**：动态数据的占位符，在渲染模板时被实际值替换。
* **函数**：用于操作模板内容的自定义函数或逻辑。

以下是一个简单聊天模板的示例：

``` bash
{{- range .Messages }}
{{ .Role }}: {{ .Content }}
{{- end }}
```

在这个示例中，我们有：

* 一个基本的消息结构（布局）
* 三个变量：`Messages`、`Role` 和 `Content`
* 一个自定义函数，迭代 `Messages` 数组并显示每个项



## 将模板添加到模型

默认情况下，导入到 `Ollama` 的模型使用 `{{ .Prompt }}` 作为默认模板，即用户输入将逐字发送到 LLM。这适用于文本或代码补全模型，但对于聊天或指令模型，则缺乏必要的标记。

省略模板意味着用户需自行负责正确的输入格式。添加模板可以帮助用户更轻松地获得最佳结果。

要在模型中添加模板，需要在 `Modelfile` 中插入 `TEMPLATE` 命令。以下是一个使用 Meta 的 Llama 3 的示例：

``` bash
FROM llama3.2

TEMPLATE """{{- if .System }}<|start_header_id|>system<|end_header_id|>

{{ .System }}<|eot_id|>
{{- end }}
{{- range .Messages }}<|start_header_id|>{{ .Role }}<|end_header_id|>

{{ .Content }}<|eot_id|>
{{- end }}<|start_header_id|>assistant<|end_header_id|>

"""
```



## 变量

`System`（字符串）：系统提示

`Prompt`（字符串）：用户提示

`Response`（字符串）：助手响应

`Suffix`（字符串）：插入助手响应后的文本

`Messages`（列表）：消息列表

`Messages[].Role`（字符串）：角色，可以是 `system`、`user`、`assistant` 或 `tool` 之一

`Messages[].Content`（字符串）：消息内容

`Messages[].ToolCalls`（列表）：模型希望调用的工具列表

`Messages[].ToolCalls[].Function`（对象）：要调用的函数

`Messages[].ToolCalls[].Function.Name`（字符串）：函数名称

`Messages[].ToolCalls[].Function.Arguments`（映射）：参数名称到参数值的映射

`Tools`（列表）：模型可以访问的工具列表

`Tools[].Type`（字符串）：模式类型。`type` 总是 `function`

`Tools[].Function`（对象）：函数定义

`Tools[].Function.Name`（字符串）：函数名称

`Tools[].Function.Description`（字符串）：函数描述

`Tools[].Function.Parameters`（对象）：函数参数

`Tools[].Function.Parameters.Type`（字符串）：模式类型。`type` 总是 `object`

`Tools[].Function.Parameters.Required`（列表）：必需属性的列表

`Tools[].Function.Parameters.Properties`（映射）：属性名称到属性定义的映射

`Tools[].Function.Parameters.Properties[].Type`（字符串）：属性类型

`Tools[].Function.Parameters.Properties[].Description`（字符串）：属性描述

`Tools[].Function.Parameters.Properties[].Enum`（列表）：有效值的列表



## 提示和最佳实践

在使用 Go 模板时，请牢记以下提示和最佳实践：

* **注意点**：控制流结构如 `range` 和 `with` 会更改 `.` 的值
* **超出范围的变量**：使用 `$.` 引用当前不在范围内的变量，从根开始
* **空白控制**：使用 `-` 去除前导（`{{-`）和尾随（`-}}`）空白



## 示例

### 示例消息

#### `ChatML`

`ChatML` 是一种流行的模板格式。它可以用于如 `Databrick` 的 DBRX、Intel 的 Neural Chat 和 Microsoft 的 Orca 2 等模型。

``` bash
{{- range .Messages }}<|im_start|>{{ .Role }}
{{ .Content }}<|im_end|>
{{ end }}<|im_start|>assistant
```



### 示例工具

通过在模板中添加 `{{ .Tools }}` 节点，可以向模型添加工具支持。这一特性对于训练调用外部工具的模型非常有用，可以作为获取实时数据或执行复杂任务的强大工具。

#### `Mistral`

`Mistral` v0.3 和 `Mixtral` 8x22B 支持工具调用。

``` bash
{{- range $index, $_ := .Messages }}
{{- if eq .Role "user" }}
{{- if and (le (len (slice $.Messages $index)) 2) $.Tools }}[AVAILABLE_TOOLS] {{ json $.Tools }}[/AVAILABLE_TOOLS]
{{- end }}[INST] {{ if and (eq (len (slice $.Messages $index)) 1) $.System }}{{ $.System }}

{{ end }}{{ .Content }}[/INST]
{{- else if eq .Role "assistant" }}
{{- if .Content }} {{ .Content }}</s>
{{- else if .ToolCalls }}[TOOL_CALLS] [
{{- range .ToolCalls }}{"name": "{{ .Function.Name }}", "arguments": {{ json .Function.Arguments }}}
{{- end }}]</s>
{{- end }}
{{- else if eq .Role "tool" }}[TOOL_RESULTS] {"content": {{ .Content }}}[/TOOL_RESULTS]
{{- end }}
{{- end }}
```



### 示例中间填充

通过在模板中添加 `{{ .Suffix }}` 节点，可以向模型添加中间填充支持。这一特性对于训练在用户输入中间生成文本的模型（如代码补全模型）非常有用。



#### `CodeLlama`

`CodeLlama` [7B](https://ollama.com/library/codellama:7b-code) 和 [13B](https://ollama.com/library/codellama:13b-code) 代码补全模型支持中间填充。

``` bash
<PRE> {{ .Prompt }} <SUF>{{ .Suffix }} <MID>
```

> [!NOTE]
> `CodeLlama` 34B 和 70B 代码补全以及所有指令和 Python 微调模型不支持中间填充。



#### `Codestral`

`Codestral` [22B](https://ollama.com/library/codestral:22b) 支持中间填充。

``` bash
[SUFFIX]{{ .Suffix }}[PREFIX] {{ .Prompt }}
```



---



# 实战

## 为glm4-9b添加Function模板

### 模板示例[官方文档](https://ollama.com/library/llama3.2/blobs/966de95ca8a6)

``` bash
<|start_header_id|>system<|end_header_id|>

{{ if .System }}                                               # 如果存在系统信息，则渲染系统内容
    {{ .System }}
{{- end }}

{{- if .Tools }}                                                # 如果存在工具
    # 这里可以添加与工具相关的逻辑
{{- end }}

<|eot_id|>                                                      # 这是一个分隔符

{{- range $i, $_ := .Messages }}                               # 遍历所有消息


    {{- $last := eq (len (slice $.Messages $i)) 1 }}         # 检查当前消息是否为最后一条


    {{- if eq .Role "user" }}                                  # 如果角色是用户
        <|start_header_id|>user<|end_header_id|>              # 表示用户的消息部分
        {{- if and $.Tools $last }}                            # 如果存在工具并且是最后一条消息
            {{ range $.Tools }}                                # 遍历所有工具
                {{- . }}                                      # 渲染工具内容
            {{ end }}
            {{ .Content }}                                    # 渲染用户消息内容
            <|eot_id|>
            
        {{- else }}
            {{ .Content }}                                    # 直接渲染用户消息内容
            <|eot_id|>
        {{- end }}
        
        {{ if $last }}                                        # 如果是最后一条消息，则渲染助手标识
            <|start_header_id|>assistant<|end_header_id|>     # 表示助手的消息部分
        {{ end }}


    {{- else if eq .Role "assistant" }}                       # 如果角色是助手
        <|start_header_id|>assistant<|end_header_id|>
        {{- if .ToolCalls }}                                  # 如果存在工具调用
            {{ range .ToolCalls }}                            # 遍历所有工具调用
                {"name": "{{ .Function.Name }}", "parameters": {{ .Function.Arguments }}}  # 渲染工具调用的名称和参数
            {{ end }}
        {{- else }}
            {{ .Content }}                                    # 渲染助手消息内容
        {{- end }}
        
        {{ if not $last }}<|eot_id|>{{ end }}                 # 如果不是最后一条消息，则结束标识


    {{- else if eq .Role "tool" }}                             # 如果角色是工具
        <|start_header_id|>ipython<|end_header_id|>
        {{ .Content }}                                        # 渲染工具内容
        <|eot_id|>
        {{ if $last }}                                        # 如果是最后一条消息，则渲染助手标识
            <|start_header_id|>assistant<|end_header_id|>
        {{ end }}
    {{- end }}


{{- end }}

```



---



### 测试`Qwen2.5-0.5b`模型的`Function Call`能力



#### 拉取新模型

``` bash
[cloud@ecs-MIy69r (10:03:46) /data/siyu.mao/ollama-svc]
└─$ ollama pull qwen2.5:0.5b


# 查看模型的模板信息
[cloud@ecs-MIy69r (10:03:46) /data/siyu.mao/ollama-svc]
└─$ ollama show qwen2.5:0.5b --template

```

通过查看`Qwen2.5-0.5b`模型的信息，可以确定，这个模型支持`Function Call`能力。



#### 测试

``` bash
curl -X POST http://localhost:11434/api/chat -d '{
  "model": "qwen2.5:0.5b",
  "messages": [
    {
      "role": "user",
      "content": "今天巴黎的天气怎么样？"
    }
  ],
  "stream": false,
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_current_weather",
        "description": "获取某个地点的当前天气",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "获取天气的地点，例如：旧金山，加利福尼亚"
            },
            "format": {
              "type": "string",
              "description": "返回天气的格式，例如：'摄氏'或'华氏'",
              "enum": ["摄氏", "华氏"]
            }
          },
          "required": ["location", "format"]
        }
      }
    }
  ]
}' | jq
```

``` bash
{
  "model": "qwen2.5:0.5b",
  "created_at": "2024-09-27T02:41:05.802796365Z",
  "message": {
    "role": "assistant",
    "content": "",
    "tool_calls": [
      {
        "function": {
          "name": "get_current_weather",
          "arguments": {
            "location": "巴黎"
          }
        }
      }
    ]
  },
  "done_reason": "stop",
  "done": true,
  "total_duration": 268439224,
  "load_duration": 15985098,
  "prompt_eval_count": 206,
  "prompt_eval_duration": 12373000,
  "eval_count": 21,
  "eval_duration": 103055000
}

```



---



### 测试`glm4-9b`模型的`Function Call`能力

``` bash
curl -X POST http://localhost:11434/api/chat -d '{
  "model": "glm4:9b",
  "messages": [
    {
      "role": "user",
      "content": "今天巴黎的天气怎么样？"
    }
  ],
  "stream": false,
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_current_weather",
        "description": "获取某个地点的当前天气",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "获取天气的地点，例如：旧金山，加利福尼亚"
            },
            "format": {
              "type": "string",
              "description": "返回天气的格式，例如：'摄氏'或'华氏'",
              "enum": ["摄氏", "华氏"]
            }
          },
          "required": ["location", "format"]
        }
      }
    }
  ]
}' | jq

```

``` bash
{
  "error": "glm4:9b does not support tools"
}
```

模型中的模板没有设置支持`Function Call`能力的`布局`，所以默认它`不支持`工具的功能。



#### 尝试修改模型的`模板`来支持`Function Call`能力



为`glm4-9b`创建`Modelfile`

> 因为我选择的这个版本的模型没有明确说明是支持`Function Call`的能力，这里我将尝试`修改模板`来验证，是否可以通过`修改模板`来为模型增加`Function Call`的能力。

``` bash
tee > Modelfile << ERIC
FROM glm4:9b

TEMPLATE """
{{- if .Messages }}
    {{- if or .System .Tools }}<|im_start|>system
        {{- if .System }}
            {{ .System }}
        {{- end }}
        
        {{- if .Tools }}
            # Tools

            You may call one or more functions to assist with the user query.

            You are provided with function signatures within <tools></tools> XML tags:
            <tools>
                {{- range .Tools }}
                    {"type": "function", "function": {{ .Function }}}
                {{- end }}
            </tools>

            For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:
            <tool_call>
                {"name": <function-name>, "arguments": <args-json-object>}
            </tool_call>
        {{- end }}<|im_end|>
    {{ end }}
    
    {{- range \$i, \$_ := .Messages }}
        {{- \$last := eq (len (slice \$.Messages \$i)) 1 -}}
        
        {{- if eq .Role "user" }}<|im_start|>user
            {{ .Content }}<|im_end|>
        {{ else if eq .Role "assistant" }}<|im_start|>assistant
            {{ if .Content }}
                {{ .Content }}
            {{- else if .ToolCalls }}<tool_call>
                {{ range .ToolCalls }}
                    {"name": "{{ .Function.Name }}", "arguments": {{ .Function.Arguments }}}
                {{ end }}</tool_call>
            {{- end }}{{ if not \$last }}<|im_end|>{{ end }}
        {{- else if eq .Role "tool" }}<|im_start|>user
            <tool_response>
                {{ .Content }}
            </tool_response><|im_end|>
        {{ end }}
        
        {{- if and (ne .Role "assistant") \$last }}<|im_start|>assistant
        {{ end }}
    {{- end }}
{{- else }}
    {{- if .System }}<|im_start|>system
        {{ .System }}<|im_end|>
    {{ end }}
    
    {{ if .Prompt }}<|im_start|>user
        {{ .Prompt }}<|im_end|>
    {{ end }}<|im_start|>assistant
        {{ end }}{{ .Response }}{{ if .Response }}<|im_end|>{{ end }}

"""

ERIC

```



#### 重新创建模型

``` bash
[cloud@ecs-MIy69r (16:41:45) /data/siyu.mao/ollama-svc]
└─$ ollama create glm4-9b-chat -f Modelfile
```

``` bash
transferring model data
using existing layer sha256:b506a070d1152798d435ec4e7687336567ae653b3106f73b7b4ac7be1cbc4449
using existing layer sha256:e4f0dc83900aa17dfeb365c6e161e9d58d30f6f7a0ae9732abc800bd6837f51e
creating new layer sha256:645c1b4d345f6bc4149fe39be9be9fee5dfa88f46b08e32ab95d72d980b170ea
using existing layer sha256:4134f3eb051643ebac23e36a5927028f43889bb1f59a23a71212feaaa0aac3f3
creating new layer sha256:8fea62bcf93269590d00a325a472efcde2091e8034e07102d54f659aa4a5b910
writing manifest
success

```



#### 查看模型

``` bash
[cloud@ecs-MIy69r (16:45:10) /data/siyu.mao/ollama-svc]
└─$ ollama list
NAME                   ID              SIZE      MODIFIED
glm4-9b-chat:latest    8493f94ed9d3    5.5 GB    41 seconds ago
llama3.2:1b            baf6a787fdff    1.3 GB    5 hours ago
vicuna:7b              370739dc897b    3.8 GB    24 hours ago
glm4:9b                5b699761eca5    5.5 GB    26 hours ago

```



#### 查看重新创建后的模板

``` bash
[cloud@ecs-MIy69r (16:45:51) /data/siyu.mao/ollama-svc]
└─$ ollama show glm4-9b-chat:latest --template

```



#### 测试`Function Call`

``` bash
curl -X POST http://localhost:11434/api/chat -d '{
  "model": "glm4-9b-chat:latest",
  "messages": [
    {
      "role": "user",
      "content": "今天巴黎的天气怎么样？"
    }
  ],
  "stream": false,
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_current_weather",
        "description": "获取某个地点的当前天气",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "获取天气的地点，例如：旧金山，加利福尼亚"
            },
            "format": {
              "type": "string",
              "description": "返回天气的格式，例如：'摄氏'或'华氏'",
              "enum": ["摄氏", "华氏"]
            }
          },
          "required": ["location", "format"]
        }
      }
    }
  ]
}' | jq

```

``` bash
{
  "model": "glm4-9b-chat:latest",
  "created_at": "2024-09-27T02:49:52.964669053Z",
  "message": {
    "role": "assistant",
    "content": "",
    "tool_calls": [
      {
        "function": {
          "name": "get_current_weather",
          "arguments": {
            "format": "摄氏",
            "location": "巴黎，法国"
          }
        }
      }
    ]
  },
  "done_reason": "stop",
  "done": true,
  "total_duration": 1942253503,
  "load_duration": 22273345,
  "prompt_eval_count": 233,
  "prompt_eval_duration": 76534000,
  "eval_count": 173,
  "eval_duration": 1801220000
}

```

> 由此可见，之前的模型本身是支持`Function Call`的，问题出在模板编写不正确，导致该功能无法正常使用。
>
> 支持`Function Call`的前提条件有：
>
> 1. 模型本身必须支持此功能。
> 2. 模板需要编写`符合模型要求`的布局脚本。
>
> 目前我使用的是Qwen2.5的模板脚本给glm4-9b模型使用，但经过反复测试后发现，它对glm4-9b模型的适配并不稳定，具体原因尚不明确。
>

---


















# RAG问题统计归类-调研



### **常用聚类算法及对比**

| **算法**                         | **特点和优点**                                               | **缺点和限制**                                               | **适用场景**                                           |
| -------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------ |
| **K-means**                      | - 简单、高效，适合大数据 - 对球形簇表现良好 - 可扩展到大数据集（Mini-batch K-means） | - 需预设簇数 kkk - 对噪声和离群点敏感 - 无法处理非球形簇     | 数据分布均匀、球形簇，市场分群、图像压缩等             |
| **DBSCAN**                       | - 自动确定簇数 - 识别任意形状簇 - 对噪声和离群点敏感         | - 参数 ϵ,minPts\epsilon, \text{minPts}ϵ,minPts 难以选择 - 高维数据效率较低 | 含噪声的地理空间数据、复杂形状簇的识别，如地震热点分析 |
| **Hierarchical（层次聚类）**     | - 不需预设簇数 - 可生成聚类树（层次结构） - 可处理小数据集   | - 计算复杂度高 - 对噪声和离群点敏感 - 不擅长处理大规模数据   | 生物信息学（基因聚类）、社交网络分析                   |
| **Gaussian Mixture Model (GMM)** | - 基于概率分布 - 可表示不同形状簇 - 支持软聚类（概率归属）   | - 需预设簇数 - 对初始值敏感，容易陷入局部最优                | 模式识别、金融建模，如客户分群、信用评分               |
| **Spectral Clustering**          | - 可处理复杂形状的簇 - 使用图论方法，适合非欧几里得空间 - 适合高维数据 | - 计算开销大 - 需预设簇数                                    | 图像分割、社交网络聚类、推荐系统                       |
| **OPTICS**                       | - 是 DBSCAN 的扩展 - 可发现簇的层次结构 - 处理密度变化的簇   | - 参数调整复杂 - 高维数据的性能下降                          | 数据密度变化较大的场景，如地理信息分析                 |
| **Mean-Shift**                   | - 不需预设簇数 - 可自动确定簇数 - 能适应任意形状簇           | - 计算开销大，尤其是大数据集 - 对带宽参数敏感                | 图像处理（分割、目标跟踪）、模式识别                   |
| **Affinity Propagation**         | - 不需预设簇数 - 自动选择中心点                              | - 计算复杂度高 - 对输入数据和距离度量敏感                    | 数据点较少的情况下，如图像聚类、文本聚类               |
| **Agglomerative Clustering**     | - 层次聚类的一种 - 可适应各种距离度量 - 适合少量样本         | - 计算复杂度高 - 无法有效处理大数据集                        | 基因序列分析、文档分类                                 |
| **BIRCH**                        | - 面向大规模数据集 - 增量式聚类 - 对噪声鲁棒                 | - 不适合高维数据 - 对参数敏感                                | 大数据场景，如实时数据聚类                             |

------

### **详细比较**

| **属性**               | **K-means** | **DBSCAN** | **Hierarchical** | **GMM** | **Spectral Clustering** |
| ---------------------- | ----------- | ---------- | ---------------- | ------- | ----------------------- |
| **需预设簇数**         | 是          | 否         | 否               | 是      | 是                      |
| **处理噪声能力**       | 弱          | 强         | 弱               | 弱      | 一般                    |
| **簇形状限制**         | 球形        | 任意       | 任意             | 任意    | 任意                    |
| **对高维数据的适应性** | 较好        | 较差       | 较差             | 较好    | 较好                    |
| **计算效率**           | 高          | 一般       | 低               | 一般    | 低                      |
| **是否支持软聚类**     | 否          | 否         | 否               | 是      | 否                      |

------

### **选择建议**

1. **K-means**：数据分布均匀、簇近似球形，数据量大时首选。
2. **DBSCAN**：有噪声、簇形状复杂、不规则时优先选择。
3. **层次聚类**：适合小数据集或需要分层结构的场景。
4. **GMM**：需要软聚类（概率分配）或非球形分布时效果较好。
5. **Spectral Clustering**：适用于复杂的高维数据场景，如图像分割。



---

### **流程**

1. **文本预处理**
   - **分词**：将问题文本分成有意义的单词或词组（如中文可以使用 Jieba 分词）。
   - **去停用词**：去掉无意义的停用词（如 "的", "是", "了" 等）。
   - **降维表示**：将文本转换为数值形式（如向量），常用方法包括：
     - **TF-IDF 向量化**：适合对小到中型文本数据的表征。
     - **词嵌入（Word2Vec, GloVe, FastText）**：捕捉词语的语义信息。
     - **句向量（Sentence-BERT, Universal Sentence Encoder）**：针对问题级语义的更高级表征。
2. **计算相似度**
   - 使用余弦相似度衡量文本向量之间的相似性。
3. **聚类算法选择**
   - 根据数据特点选择合适的算法。

------

### **算法推荐**

#### **1. K-means**

- 优点：
  - 简单高效，适合中等规模的数据量（如 5000~10000 条）。
  - 如果问题数据分布较为均匀，能快速得到清晰的簇划分。
- 限制：
  - 需要预设簇数 kkk，但可以通过肘部法则（Elbow Method）或轮廓系数（Silhouette Score）来估计。
  - 对噪声数据敏感。
- **适用场景**：问题较为集中，分布相对均匀。

#### **2. DBSCAN**

- 优点：
  - 不需预设簇数，自动识别簇的数量。
  - 可识别任意形状的簇，并有效处理噪声数据。
- 限制：
  - 参数 ϵ\epsilonϵ 和 minPts\text{minPts}minPts 需要调整。
  - 处理高维数据效率较低，但可以结合降维方法（如 PCA）。
- **适用场景**：问题分布复杂、含噪声或离群点。

#### **3. 层次聚类（Hierarchical Clustering）**

- 优点：
  - 不需预设簇数，生成聚类树，可视化数据的层次结构。
  - 可灵活调整簇的数量或相似度阈值。
- 限制：
  - 对于大数据集（如 >5000 条）计算开销较大。
- **适用场景**：需要层次结构分析的小规模数据。

#### **4. Spectral Clustering**

- 优点：
  - 适合非线性边界的簇，能处理复杂的语义关系。
  - 能捕获高维数据中的簇结构。
- 限制：
  - 计算复杂度较高，尤其对于大规模数据。
  - 需要预设簇数。
- **适用场景**：问题的相似性较复杂，或需要处理非欧几里得距离。

#### **5. Mean-Shift**

- 优点：
  - 不需要指定簇数。
  - 能找到密度中心，适合任意形状的簇。
- 限制：
  - 计算开销较大，尤其是数据量大时。
  - 对带宽参数敏感。
- **适用场景**：对问题的语义密度分布感兴趣。

------

### **推荐方案**

**基于数据量和问题特点的建议**：

- **预处理**：采用 TF-IDF 或 Sentence-BERT 转换问题文本为向量。
- **算法选择**：
  - **优先选择 DBSCAN**：如果问题类别不固定，且数据可能有噪声。
  - **选择 K-means**：如果问题类别数量可以大致估计，且数据分布较均匀。
  - **层次聚类**：适合小规模数据并需要层次分析时。

**综合考虑**：

1. 如果问题类别较多且不确定，**Sentence-BERT + DBSCAN** 是一个强有力的组合，能够自动发现聚类结构。
2. 如果问题类别较明确，尝试 **TF-IDF + K-means**，通过调节 kkk 得到最优结果。



---



### **1. 什么是 Sentence-BERT？**

Sentence-BERT（简称 SBERT）可以分为两部分理解：

#### **(1) Sentence**

意思是“句子”。SBERT 是用来分析句子或者短文本的工具。比如：

- 句子 A：*“苹果是一种水果。”*
- 句子 B：*“香蕉也是水果。”*

这些句子看似不同，但其实它们有一定的相似性。这种相似性就是 SBERT 擅长捕捉的内容。

#### **(2) BERT**

BERT 是一种**机器学习模型**，它的全名是 **Bidirectional Encoder Representations from Transformers**，即“基于双向编码器和 Transformer 的文本模型”。
简单来说，BERT 是一种工具，帮助计算机“读懂”文本。

------

### **2. Sentence-BERT 做什么？**

**SBERT 的主要工作是：把句子变成数字。**

- 为什么要变成数字？
  - 计算机只会处理数字，而不能直接理解文字。
  - 比如，*“我喜欢苹果”* 会被转换成一个向量：1.2,−0.4,3.6,...1.2, -0.4, 3.6, ...1.2,−0.4,3.6,...（一个高维的数字列表）。
- 转换成数字后，句子之间的关系可以用数学方法计算：
  - **相似度**：两个句子的数字越接近，它们的意思越相似。
  - **分类**：把句子分成不同的组（类似聚类）。

------

### **3. SBERT 的作用举例**

#### **(1) 找相似的问题**

假如你有一个问题数据库，里面有这些问题：

- Q1: *“什么是苹果？”*
- Q2: *“苹果是种水果吗？”*
- Q3: *“如何种植苹果树？”*
- Q4: *“今天的天气如何？”*

当用户提问 *“苹果是一种什么？”* 时，SBERT 能够将这个问题与数据库中的问题对比，并找到**相似的问题**（比如 Q1 和 Q2）。

#### **(2) 数据归类**

如果你有很多问题，例如：

- 关于水果：*“苹果是什么？”*、*“如何种香蕉？”*
- 关于天气：*“明天天气如何？”*、*“现在是晴天吗？”*

SBERT 能把这些问题转换成数字，再根据数字的分布，把问题自动分成两类：

- 一类是水果相关问题。
- 另一类是天气相关问题。

------

### **4. 为什么要用 SBERT？**

SBERT 的核心优势是：

1. **高效**：相比其他工具，它计算句子间相似度速度快，尤其是大规模数据时效果更明显。
2. **语义理解**：它能“理解”句子背后的意思，而不仅仅是看字面。
   - *“苹果是一种水果。”* 和 *“水果有苹果。”* 意思相似，SBERT 可以识别这种关系。
3. **现成工具**：你可以直接使用开源的 SBERT 模型，几乎不需要重新训练。

------

### **5. 总结**

- **SBERT 是一个工具**，用来处理句子或者短文本。
- **它把句子变成数字向量**，让计算机可以计算相似性或者进行分类。
- **常见用途**：
  - 比如问题检索、相似问题匹配、问题分类、数据聚类等。





# POC 代码

``` python
import os
from collections import Counter

import jieba
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
from openai import OpenAI
from sentence_transformers import SentenceTransformer
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA
from sklearn.preprocessing import normalize

matplotlib.rcParams['font.sans-serif'] = ['SimHei']  # Windows 黑体
matplotlib.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题
matplotlib.use('TkAgg')  # 设置绘图后端

# 停用词表
STOPWORDS = {
    "的", "了", "怎么", "如何", "这个", "小程序", "请问",
    "吗", "呢", "啊", "哦", "吧", "呀",  # 语气词
    "是", "在", "有", "和", "与", "并", "对", "就", "或", "而", "但", "因为", "所以", "而且",  # 功能词
    "我们", "你们", "他们", "我", "你", "他", "她", "它", "自己",  # 代词
    "问题", "时候", "什么", "哪里", "哪个", "这种", "这样", "谁", "多少",  # 疑问词和泛指词
    "可以", "可能", "还是", "已经", "不是", "没有", "需要", "如果", "比如", "比如说",  # 语义不强
    "一个", "一些", "一份", "一种", "一类", "一次", "一天", "一样", "全部", "所有",  # 泛指数量
    "北京", "京城", "京", "首都", "上海", "沪", "魔都", "广州", "金投", "新疆", "深圳", "大连",  # 城市地名
}


class QuestionLoader:
    """负责加载和预处理问题的工具类"""

    def __init__(self, filepath):
        self.filepath = filepath

    def load_questions(self):
        """从本地文件加载问题列表"""
        if not os.path.exists(self.filepath):
            raise FileNotFoundError(f"文件 {self.filepath} 不存在")
        with open(self.filepath, "r", encoding="utf-8") as f:
            return f.read().strip().split("\n")

    def preprocess_questions(self, questions):
        """对问题进行分词和去停用词处理"""
        new_questions = [
            " ".join([word for word in jieba.lcut(question) if word not in STOPWORDS])
            for question in questions
        ]
        return new_questions


class QuestionCluster:
    """负责问题聚类的工具类"""

    def __init__(self):
        self.model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        # self.model = SentenceTransformer("/data/LLM/paraphrase-multilingual-MiniLM-L12-v2")
        self.client = OpenAI(base_url='http://221.180.141.96:37001/v1', api_key="empty")

    def embedding(self, processed_questions, is_local=True):
        if is_local:
            print("调用本地模型")
            # 将问题转换为嵌入向量
            embedding_response = self.model.encode(processed_questions)
            return embedding_response
        else:
            print("调用OpenAI的API")
            # 调用OpenAI的API进行嵌入
            embedding_response = self.client.embeddings.create(input=processed_questions, model='paraphrase-multilingual-MiniLM-L12-v2')
            # 将 1 维数组变成 2 维数组
            embedding_response = np.array([e.embedding for e in embedding_response.data])
            return embedding_response

    def cluster_questions(self, processed_questions, raw_questions, is_local):
        """
        基于语义相似度对问题进行聚类
        :param processed_questions: 处理后的问题列表
        :param raw_questions: 原始问题列表
        :param is_local: 是否使用本地模型
        :return: 聚类标签和按类别分组的问题
        """
        # 将问题转换为嵌入向量
        embedding_response = self.embedding(processed_questions, is_local)
        # 对嵌入向量归一化(归一化可能提高聚类精度)
        embeddings = normalize(embedding_response)

        # 使用 DBSCAN 进行聚类
        # DBSCAN 参数说明：
        # eps (邻域半径):
        # - 大：邻域范围更大，簇数量减少，噪声点减少。
        # - 小：邻域范围更小，簇数量增加，噪声点增多。
        #
        # min_samples (最小样本数):
        # - 大：需要更多点形成簇，簇减少，噪声点增多。
        # - 小：更容易形成簇，簇增多，但可能过于碎片化。
        #
        # metric (距离度量):
        # - "cosine"（余弦距离），适合高维嵌入数据。
        clustering = DBSCAN(eps=0.25, min_samples=3, metric="cosine")
        labels = clustering.fit_predict(embeddings)
        """
        DBSCAN 的核心思想就像“画圈圈”：
            1. **圈圈的大小由 `eps` 决定**  
               - `eps` 就是每个点周围的“圈的半径”。  
               - 圈大了（`eps`大），容易把更多点圈进来，形成大簇；圈小了，只能圈到更近的点，簇会更碎。
            
            2. **圈里的点数量由 `min_samples` 决定**  
               - 每个点如果圈内的点数（包括自己）达到 `min_samples`，就认为这个点是“核心点”（形成簇的关键点）。  
               - 如果不够 `min_samples`，这个点可能是噪声（孤立点）或者属于簇边缘（但不能引发新的簇）。
            
            3. **簇的形成**  
               - 从核心点开始，连锁地“扩展”簇：圈内有其他核心点，继续把它们的圈范围内的点加入簇。
               - 这样，簇会不断“生长”，直到没有新的核心点可圈进来。
        总结：  
            - DBSCAN 的确是**通过“画圈圈”（eps 半径）和“点的数量要求”（min_samples）来判断哪些点能形成簇**的。eps 决定圈的范围，min_samples 决定圈的密度。两者结合，能很好地处理密度不均的散点分布。
            - K-means 更适合用在你已经知道有多少个簇的情况下，且适应于“规则形状的簇”。
            - DBSCAN 则更适合在不规则形状、不均匀密度的数据中探索，尤其能标记出噪声点。
        """

        print(f"使用参数：eps={clustering.eps}, min_samples={clustering.min_samples}")
        print(f"类别分布：{Counter(labels)}")

        # 按类别归类，并将类别与中文进行对应
        clustered_questions = {}
        for question, label in zip(raw_questions, labels):
            clustered_questions.setdefault(label, []).append(question)

        return embeddings, labels, clustered_questions


class ClusterVisualizer:
    """负责可视化的工具类"""

    @staticmethod
    def display_top_clusters(labels, clustered_questions, top_n=None):
        """
        显示高频问题类别
        :param labels: 聚类标签
        :param clustered_questions: 按类别分组的问题
        :param top_n: 显示的类别数（不包括噪声点）
        """
        # 统计每个类别的问题数量
        label_counts: Counter[int] = Counter(labels)  # 类型提示

        # 分离噪声数据和非噪声数据
        noise_count = label_counts.get(-1, 0)  # 噪声点的数量
        non_noise_label_counts = {label: count for label, count in label_counts.items() if label != -1}

        # 获取前 top_n 个非噪声高频类别
        top_clusters = sorted(non_noise_label_counts.items(), key=lambda x: x[1], reverse=True)[:top_n]

        # 显示噪声数据
        print(f"\n噪声点数量：{noise_count}")
        if noise_count > 0:
            print("\n噪声点问题：")
            for question in clustered_questions.get(-1, []):
                print(f"  {question}")

        # 显示非噪声数据
        print(f"\n显示前 {top_n} 类问题（不包括噪声点）：")
        for cluster_id, count in top_clusters:
            print(f"\n类别 {cluster_id}（数量：{count}）:")
            for question in clustered_questions.get(cluster_id, []):
                print(f"  {question}")

        # 创建一个列表，用于存储噪声数据和非噪声数据
        result_list = []

        # 噪声点信息
        if noise_count > 0:
            result_list.append({
                'cluster_id': -1,
                'count': noise_count,
                'questions': clustered_questions.get(-1, [])
            })

        # 非噪声点信息
        for cluster_id, count in top_clusters:
            result_list.append({
                'cluster_id': cluster_id,
                'count': count,
                'questions': clustered_questions.get(cluster_id, [])
            })

        print("\n结果数据：")
        print(result_list)

        # 返回结果列表
        return result_list

    @staticmethod
    def plot_clusters_2d(embeddings, labels, top_n=None):
        """
        使用二维散点图展示聚类结果，显示噪声点并选取去掉噪声点后的 top_n 类别
        :param embeddings: 问题的嵌入向量
        :param labels: 聚类标签
        :param top_n: 显示的类别数（仅展示去掉噪声点的 top_n 类别，噪声点始终显示）
        """
        # 使用 PCA 将高维嵌入降至二维
        pca = PCA(n_components=2)
        reduced_embeddings = pca.fit_transform(embeddings)

        # 转换为 numpy 数组
        reduced_embeddings = np.array(reduced_embeddings)
        labels = np.array(labels)

        # 分离噪声点和非噪声点
        noise_points = reduced_embeddings[labels == -1]  # 噪声点
        # non_noise_points = reduced_embeddings[labels != -1]  # 非噪声点
        non_noise_labels = labels[labels != -1]  # 非噪声点的标签

        # 统计非噪声类别出现次数，选取 top_n 类别
        label_counts = Counter(non_noise_labels)
        most_common_labels = [
            label for label, _ in label_counts.most_common(top_n)
        ] if top_n else set(non_noise_labels)

        # 创建散点图
        plt.figure(figsize=(10, 7))

        # 绘制噪声点
        if noise_points.size > 0:
            plt.scatter(
                noise_points[:, 0],
                noise_points[:, 1],
                label=f"噪声点 （{len(noise_points)}）",
                alpha=0.6,
                edgecolors="k",
                c="darkgray",  # 固定颜色
            )

        # 绘制 Top_n 类别的非噪声点
        for label in most_common_labels:
            cluster_points = reduced_embeddings[labels == label]
            count = label_counts[label]  # 获取当前类别的数量
            plt.scatter(
                cluster_points[:, 0],
                cluster_points[:, 1],
                label=f"类别 {label} （{count}）",
                alpha=0.6,
                edgecolors="k",
                c=None,  # 使用 matplotlib 自动分配颜色
            )

        plt.title(f"问题聚类结果可视化 (前 {top_n} 类别，包含噪声点)")
        plt.xlabel("PCA 维度 1")
        plt.ylabel("PCA 维度 2")
        plt.legend()
        plt.show()


def main():
    # 配置文件路径
    question_file = "questions.txt"

    # 加载和预处理问题
    loader = QuestionLoader(question_file)
    questions = loader.load_questions()
    processed_questions = loader.preprocess_questions(questions)

    # 聚类问题
    cluster = QuestionCluster()
    embeddings, labels, clustered_questions = cluster.cluster_questions(processed_questions, questions, False)

    # 可视化结果
    visualizer = ClusterVisualizer()
    top_n = 4
    visualizer.display_top_clusters(labels, clustered_questions, top_n=top_n)
    visualizer.plot_clusters_2d(embeddings, labels, top_n=top_n)


if __name__ == "__main__":
    main()

```



#### 问题文件：questions.txt

``` txt
公司保密制度
涨薪制度
今日营养食谱
本周营养食谱
主责部门是运营管理部的管理流程有哪些？
涨薪制度
带薪假期
组织架构图
差旅费报销流程
我还有几年退休啊
保密制度要求
差旅费报销流程
临时有事3小时，我应该请什么？
出差标准
本周食谱
包含法务制度得相关文件
涨薪制度
夏季上班时间、冬季上班时间
迟到多久算迟到，缺勤扣多少工资
公司有哪些保密制度要求？
要创建一个腾讯文档，可以通过以下几种方式实现
我想休婚假，能休多少天？怎么请假？
北京出差标准多少?
事假，病假工资怎么计算
信息安全管理办法
本周营养食谱
我工作了5年，我的年假有多少天？
婚假多少天？
组织架构
事假，病假……工资怎么计算
公司有哪些信息安全要求？
保密制度
新员工怎么培训的？
伙食补助标准多少？
食谱
今天营养食谱
今天的食谱
公司请假流程
请假流程？
今天吃什么？
请假流程
我想休婚假，能休多少天？怎么请假？
金投集团的合规管理重点是什么
发展战略怎么制定的？
公司请假流程
信息安全管理办法
今天吃吃什么
婚假有几天？
涨薪制度
涨薪制度，一年涨多少
怎么晋升啊？
金投集团的合规管理重点是什么
也可以从模板中选择一个适合的文档模板进行创建。
大连出差住宿标准
根据公司管理制度，我要去大连出差，住宿的标准是多少？
公司考勤制度
今日营养食谱
北京出差住宿标准
高旸还有几天年假
福利假有什么？
上班
出差补贴
早退和旷工有什么区别？
金投集团的合规管理重点是什么
作业指导书
我要去大连出差，住宿的标准是多少？
差旅费报销流程
本周食谱
夏季上班时间、冬季上班时间
公司请假流程
夏季上班时间、冬季上班时间
事假
员工福利有哪些
新员工怎么培训的？
缺勤和请假的区别
你能帮我做什么
去北京的差标多少？
出差流程
本周营养食谱
本周营养食谱
信息安全管理办法
怎么发文
公司请假流程
一个月能补卡几次
能迟到几分钟
福利
北京出差标准多少？
薪酬制度
本周营养食谱
本周营养食谱
公司请假流程
差旅费报销流程
事假，病假工资怎么计算
考勤制度
考勤
公积金缴纳
差旅费报销流程
公文写作
本周营养食谱
本周营养食谱
差旅费报销流程
信息安全管理办法
保密制度
我想休婚假，能休多少天？怎么请假？
你是谁
楼里可以抽烟吗？
```




## RAG评测方案

> RAG整个环节主要涉及到三部分内容：
>
> - Query：输入
>
> - Context：检索到的上下文
>
> - Response：LLM的回答
>
> - 这也是 RAG 整个过程中最重要的三元组，它们之间两两相互牵制。因此可以通过**检测三元组之间两两元素的相关度**，来评估RAG 应用的效果:
>
> - ![img](images/rag-eval-01.png) 
>



## 相关资料

> OpenAI的报告[Jarvis and Allard, 2023](https://arxiv.org/pdf/2312.10997v1.pdf)提到了用于优化大型语言模型（LLMs）的各种技术，包括RAG及其评估指标。
>
> 
>
> 论文名称：Benchmarking Large Language Models in Retrieval-Augmented Generation (在检索增强生成中对大型语言模型进行基准测试)
>
> 论文地址：https://arxiv.org/pdf/2309.01431.pdf
>
> 
>
> 论文名称：RAGAS: Automated Evaluation of Retrieval Augmented Generation (检索增强生成的自动评估)
>
> 论文地址：https://arxiv.org/pdf/2309.15217.pdf
>
> 
>
> ### 关键指标
>
> 1. **忠实度**:
>    - 这个指标强调模型生成的答案必须保持对给定上下文的忠实，确保答案与上下文信息一致，不偏离或矛盾。这方面的评估对于解决大型模型中的幻觉问题至关重要。
> 2. **答案相关性**:
>    - 这个指标强调生成的答案需要与提出的问题直接相关。
> 3. **上下文相关性**:
>    - 这个指标要求检索到的上下文信息尽可能准确和针对性强，避免无关内容。毕竟，处理长文本对LLMs来说成本高昂，过多的无关信息会降低LLMs利用上下文的效率。
>
> OpenAI的报告还提到了“上下文回忆”作为一个补充指标，用于衡量模型回答问题所需的所有相关信息的检索能力。这个指标反映了RAG检索模块的搜索优化水平。低回忆率表明可能需要优化搜索功能，如引入重新排序机制或微调嵌入，以确保检索到更相关的内容。
>
> ### 关键能力
>
> 1. **噪声鲁棒性**:
>    - 这项能力衡量模型处理与问题相关但不包含有用信息的噪声文档的效率。
> 2. **负面拒绝**:
>    - 当模型检索到的文档缺乏回答问题所需的知识时，模型应该正确地拒绝响应。在负面拒绝的测试设置中，外部文档只包含噪声。理想情况下，LLM应该发出“缺乏信息”或类似的拒绝信号。
> 3. **信息整合**:
>    - 这项能力评估模型是否能从多个文档中整合信息以回答更复杂的问题。
> 4. **反事实鲁棒性**:
>    - 这项测试旨在评估模型在接收到关于检索信息潜在风险的指令时，是否能识别并处理文档中已知的错误信息。反事实鲁棒性测试包括LLM可以直接回答的问题，但相关的外部文档包含事实错误。
>
> 这些能力的评估有助于理解RAG模型在处理复杂信息和不确定环境中的性能，从而为RAG的进一步发展和优化提供重要的参考依据。



## 常用的RAG指标

1、**答案相关性（Answer Relevancy）**：此指标的目标是`评估生成的答案`与`提供的问题`提示之间的`相关性`。

答案如果缺乏完整性或者包含冗余信息，那么其得分将相对较低。这一指标通过问题和答案的结合来进行计算，评分的范围通常在0到1之间，其中高分代表更好的相关性。

> **示例**
>
> **问题**：*健康饮食的主要特点是什么？*
>
> - **低相关性答案**：健康饮食对整体健康非常重要。
> - **高相关性答案**：健康饮食应包括各种水果、蔬菜、全麦食品、瘦肉和乳制品，为优化健康提供必要的营养素。



2、**忠实度（Faithfulness）**：这个评价标准旨在`检查生成的答案`在给定上下文中的`事实准确性`。

评估的过程涉及到答案内容与其检索到的上下文之间的比对。这一指标也使用一个介于0到1之间的数值来表示，其中更高的数值意味着答案与上下文的一致性更高。

> **示例**
>
> **问题**：*居里夫人的主要成就是什么？*
>
> *背景*：玛丽·居里（1867-1934年）是一位开创性的物理学家和化学家，她是第一位获得诺贝尔奖的女性，也是唯一一位在两个不同领域获得诺贝尔奖的女性。
>
> - **高忠实度答案**：玛丽·居里在物理和化学两个领域都获得了诺贝尔奖，使她成为第一位实现这一成就的女性。
> - **低忠实度答案**：玛丽·居里只在物理学领域获得了诺贝尔奖。



3、**上下文精确度（Context Precision）**：在这个指标中，我们评估所有在给定上下文中`与基准信息相关`的条目是否被`正确地排序`。

理想情况下，所有相关的内容应该出现在排序的前部。这一评价标准同样使用0到1之间的得分值来表示，其中较高的得分反映了更高的精确度。



4、**答案正确性（Answer Correctness）**：该指标主要用于`测量生成的答案`与`实际基准答案`之间的`匹配程度`。

这一评估考虑了基准答案和生成答案的对比，其得分也通常在0到1之间，较高的得分表明生成答案与实际答案的一致性更高。

> **示例：**
>
> ***基本事实**：埃菲尔铁塔于 1889 年在法国巴黎竣工。*
>
> - ***答案正确率高***：埃菲尔铁塔于 1889 年在法国巴黎竣工。
> - ***答案正确率低***：埃菲尔铁塔于 1889 年竣工，矗立在英国伦敦。



## 评估方法

> 评估RAG有效性主要有两种方法：`独立评估`和`端到端评估`
>
> - 独立评估包括对检索模块和生成（读取/合成）模块的评估。
> - 端到端评估可以分为人工评估和使用`LLM的自动评估`。
>
> 在RAG评估框架领域，RAGAS和ARES是相对较新的框架。这些评估的核心焦点是三个主要指标：答案的忠实度、答案相关性和上下文相关性。



## 测试流程

> 收集RAG系统中的数据 --> 制作测试数据集 --> 使用RAGAS工具项目针对数据集进行评估 --> 进行指标可视化

1. 既然是创造数据集，就要先知道数据集的格式是什么？

   - > 假设你已经成功构建了一个RAG 系统，并且现在想要评估它的性能。为了这个目的，你需要一个评估数据集，该数据集包含以下列：
     >
     > - question（问题）：想要评估的RAG的问题
     > - ground_truths（真实答案）：问题的真实答案
     > - answer（答案）：RAG 预测的答案
     > - contexts（上下文）：RAG 用于生成答案的相关信息列表

2. 确定的数据集的格式，才能正确的填充数据内容。

   - > RAGAS 需要使用 HuggingFace 标准的 dataset 的格式，因此我们可以根据现有的格式去构建自己的数据集
     >
     > ```json
     > // 将您的数据加载到字典中
     > {
     >     "question": ["法国的首都是什么？", "《哈利波特》的作者是谁？", "水的沸点是多少？"],
     >     "ground_truths": ["巴黎", "J.K.罗琳", "100度摄氏度"],
     >     "answer": ["巴黎", "J.K.罗琳", "100度摄氏度"],
     >     "contexts": [
     >         ["巴黎是法国的首都。"],
     >         ["J.K.罗琳写了《哈利波特》。"],
     >         ["水在海平面下沸腾的温度是100摄氏度。"]
     >     ]
     > }
     > ```

3. 使用RAGAs系统进行评估

   - > **Ragas**（Retrieval-Augmented Generation Assessment,`检索增强生成评估`）是 LLM 支持的最大开源系统之一，它提供
     >
     > - 基于文档生成测试数据的方法
     >
     > - 基于不同指标的评估，用于一对一和端到端地评估检索和生成步骤。
     >
     > - 返回结果
     >
     >   - ``` json
     >     评估结果 / Evaluation Results:
     >         
     >     回答相关性：     0.5080443833601872
     >     回答相似度：     0.9999999999999999
     >     上下文精确度：   0.9999999999
     >     上下文实体召回率：0.9999999900000002
     >     忠实度：        1.0
     >     ```

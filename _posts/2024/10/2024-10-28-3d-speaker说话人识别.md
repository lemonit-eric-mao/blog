

## 下载项目

1. 下载项目

   ```bash
   git clone https://github.com/modelscope/3D-Speaker.git
   
   cd 3D-Speaker
   ```

## 安装依赖

2. 安装依赖环境

   ```bash
   conda create -n 3d-speaker python==3.10.12
   
   conda activate 3d-speaker
   
   pip install -r requirements.txt
   ```

> 待验证点：
>
> - 音频格式：只wav?    --> 可以其他格式，已测试aac
> - 音频采样率：只16K？  -->  只支持16KHZ的音频采样
> - 聚类算法
> - 1.5s和0.75滑动间隔
> - 视频的使用



## 说话人识别案例

3. 进入到说话人分类目录，可解决“谁在何时说话”的问题

   ```bash
   cd egs/3dspeaker/speaker-diarization
   ```

4. 查看run_audio.sh脚本思路

   ```bash
   脚本整体分为6个步骤：
   
   准备输入音频（Stage 1）
   下载示例音频文件和参考 RTTM 文件，并记录其路径到相应列表。
   
   语音活动检测（VAD）（Stage 2）
   运行 VAD 脚本处理输入音频，生成语音活动检测结果的 JSON 文件。
   
   准备子段信息（Stage 3）
   将 VAD 输出转换为子段信息，生成相应的 JSON 文件。
   
   提取说话人嵌入（Stage 4）
   使用预训练模型提取音频中的说话人嵌入特征，并保存到指定目录。
   
   聚类与输出结果（Stage 5）
   对提取的嵌入进行聚类，生成最终的 RTTM 文件。
   
   计算最终指标（Stage 6）
   计算分离性能指标（如 DER），以评估说话人分离的效果。
   ```

5. 安装`说话人分类`所需的依赖

   ```bash
   pip install -r requirements.txt
   
   # 如果没有ffmpeg请安装
   sudo apt install ffmpeg
   ```

   
## 说话人识别每个步骤拆解

### 第一、二步

6. 修改`run_audio.sh`脚本的stage、stop_stage,一步步了解实现机制

   ```bash
   # 定义了处理的起始和结束阶段
   stage=1
   #stop_stage=6
   stop_stage=2
   ```

7. 执行脚本中的第一步和第二步：进行样例文件的下载 --> 生成音频和rttm的文件列表 --> 进行语音活动检测（VAD）--> 生成vad.json数据文件

   ```bash
   bash run_audio.sh
   
   # 中间会提示各种依赖未安装，按照提示进行安装
   pip install addict datasets simplejson sortedcontainers
   
   # 第一步：准备输入音频,下载示例音频和参考 RTTM 文件，并将它们的路径写入列表，生成的文件内容如下：
   # 下载文件的保存路径在：examples下:
   └─$ ls examples/
   2speakers_example.rttm  2speakers_example.wav  refrttm.list  wav.list
   
   # 生成的文件内容:
   wav.list>>
   examples/2speakers_example.wav
   
   refrttm.list>>
   examples/2speakers_example.rttm
   
   
   # 第二步：接收第一步生成的wav.list文件，进行语音活动检测（VAD） 使用 VAD 脚本处理音频，输出语音活动检测结果到 JSON 文件
   
   # 模型VAD检测输出数据：
   {'key': '2speakers_example', 'value': [[5240, 29010], [29290, 37360], [37640, 67570], [67860, 78980]]}
   
   # VAD模型数据处理后输出文件的保存路径在 exp/json目录下：
   └─$ ls exp/json/
   vad.json
   
   # 输出vad.json内容如下：
   {
     "2speakers_example_5.24_29.01": {
       "file": "examples/2speakers_example.wav",
       "start": 5.24,
       "stop": 29.01,
       "sample_rate": 16000
     },
     "2speakers_example_29.29_37.36": {
       "file": "examples/2speakers_example.wav",
       "start": 29.29,
       "stop": 37.36,
       "sample_rate": 16000
     },
     ...
   }
   ```

### 第三步

8. 修改`run_audio.sh`脚本的stage、stop_stage，执行步骤3：将上一步的VAD的音频段json,拆解为更细粒度的子段（按照开始时间每次偏移默认0.75秒，每段时长1.5秒）

   ```bash
   # 修改内容
   stage=3
   stop_stage=3
   
   bash run_audio.sh
   └─$ bash run_audio.sh
   run_audio.sh Stage3: Prepare subsegments info...
   [INFO]: Generate sub-segmetns...
   [INFO]: Subsegments json is prepared in exp/json/subseg.json
   
   # 第三步：接收第二部的vad.json，将vad的每一段声音片段拆解更细粒度的子段（按照开始时间每次偏移默认0.75秒，每段时长1.5秒）
   # 拆分后的subseg.json内容如下：
   └─$ ls exp/json/
   subseg.json  vad.json
   
   {
     "2speakers_example_5.24_6.74": {
       "file": "examples/2speakers_example.wav",
       "start": 5.24, # vad的第一段的开始时间
       "stop": 6.74,  # 拆分成每段时长1.5秒的子段
       "sample_rate": 16000
     },
     "2speakers_example_5.99_7.49": {
       "file": "examples/2speakers_example.wav",
       "start": 5.99, # 接着上一段开始时间偏移0.75秒
       "stop": 7.49,  # 本子段的持续时长1.5秒
       "sample_rate": 16000
     },
     ...
   }
   ```
### 第四步
9. 修改`run_audio.sh`脚本的stage、stop_stage，执行步骤4：根据给定的模型和配置，按照子段json信息提取说话人嵌入特征，并将embedding信息输出到指定目录

   ```bash
   # 修改内容
   stage=4
   stop_stage=4
   
   bash run_audio.sh
   └─$ bash run_audio.sh
   run_audio.sh Stage4: Extract speaker embeddings...
   2024-10-21 15:03:46,089 - modelscope - INFO - Use user-specified model revision: v1.0.0
   [INFO]: Start computing embeddings...
   [WARNING]: The number of threads exceeds the number of files.
   [WARNING]: The number of threads exceeds the number of files.
   [WARNING]: The number of threads exceeds the number of files.
   [WARNING]: The number of threads exceeds the number of files.
   [WARNING]: The number of threads exceeds the number of files.
   [WARNING]: The number of threads exceeds the number of files.
   [WARNING]: The number of threads exceeds the number of files.
   
   # 生成的embedding结果文件
   └─$ ls exp/embs/
   2speakers_example.pkl
   
   embedding文件的内容为：
   {
     'embeddings': [[第一个子段向量],[第二个子段向量],...]，
     'times': [[第一子段开始时间,结束时间],[第二子段开始时间,结束时间],...]
   }
   
   
   # 如果出现以下报错信息：可以参考：https://github.com/pytorch/pytorch/issues/121834
   # 原因是torch版本问题，建议升级为2.3.1及以上版本
   /data/anaconda3/envs/3d-speaker/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
     return F.conv2d(input, weight, bias, self.stride,
   ```


### 第五步

10. 修改`run_audio.sh`脚本的stage、stop_stage，执行步骤5：根据给定配置和wav列表以及embedding信息，聚类与输出RTTM结果,对提取的嵌入进行聚类，生成 RTTM 文件

    ```bash
    # 修改内容
    stage=5
    stop_stage=5
    
    bash run_audio.sh
    └─$ bash run_audio.sh
    run_audio.sh Stage5: Perform clustering and output sys rttms...
    [WARNING]: The number of threads exceeds the number of files
    [WARNING]: The number of threads exceeds the number of files
    [WARNING]: The number of threads exceeds the number of files
    [WARNING]: The number of threads exceeds the number of files
    [WARNING]: The number of threads exceeds the number of files
    [INFO] Start clustering...
    [WARNING]: The number of threads exceeds the number of files
    [WARNING]: The number of threads exceeds the number of files
    
    # 第五步的操作，将第四步生成的每个子段的embedding信息进行聚类，生成一个聚类数组，代表了每一个子段音频的说话人标识：
    labels =  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1
               1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0
               0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1]
    
    # 然后将第四步的embedding信息中的 times 时间列表和 labels 说话人标签进行组合，生成音频时间和标签的列表
    seg_list = [([5.24, 6.74], 0), ([5.99, 7.49], 0), ([6.74, 8.24], 0), ... , ([29.29, 30.79], 1), ([30.04, 31.54], 1), ...]
    
    # 之后通过时间的逻辑，对连续的音频时间进行合并，生成和第二步中VAD.json文件相同的时间段信息
    new_seg_list = [['2speakers_example', 5.24, 29.01, 1],    ['2speakers_example', 29.29, 37.36, 2],   ['2speakers_example', 37.64, 40.265, 2],                       ['2speakers_example', 40.265, 46.265, 1], ['2speakers_example', 46.265, 53.765, 2], ['2speakers_example', 53.765, 67.57, 1],                       ['2speakers_example', 67.86, 69.735, 1],  ['2speakers_example', 69.735, 78.98, 2]]
    
    # 最后生成RTTM文件信息
      TYPE       File ID       Channel ID  Turn Onset  Turn Duration                  Speaker Name
    SPEAKER 2speakers_example      0          5.24         23.77         <NA> <NA>          1        <NA> <NA>
    SPEAKER 2speakers_example      0          29.29        8.07          <NA> <NA>          2        <NA> <NA>
    SPEAKER 2speakers_example      0          37.64        2.62          <NA> <NA>          2        <NA> <NA>
    SPEAKER 2speakers_example      0          40.27        6.00          <NA> <NA>          1        <NA> <NA>
    SPEAKER 2speakers_example      0          46.27        7.50          <NA> <NA>          2        <NA> <NA>
    SPEAKER 2speakers_example      0          53.77        13.80         <NA> <NA>          1        <NA> <NA>
    SPEAKER 2speakers_example      0          67.86        1.88          <NA> <NA>          1        <NA> <NA>
    SPEAKER 2speakers_example      0          69.73        9.25          <NA> <NA>          2        <NA> <NA>
    ```
    

### 第六步（评估）

11. 根据提供的参考RTTM文件,计算分离的说话人性能指标，如 DER（说话人错误率）

```bash
# 修改内容
stage=6
stop_stage=6

bash run_audio.sh
└─$ bash run_audio.sh
run_audio.sh Stage6: Get the final metrics...
Computing DER...
2024-10-22 11:22:31,031 - INFO: Concatenating individual RTTM files...
2024-10-22 11:22:31,054 - INFO: MS: 0.838138, FA: 0.130056, SER: 0.534674, DER: 1.502868

# 参数介绍

在语音识别和说话人识别中，这些指标的含义如下：

MS (Missed Speech)：指在识别过程中未能检测到的实际说话内容，反映系统遗漏的语音信息。

FA (False Alarm)：指系统错误地检测到说话内容，即系统认为有说话但实际上没有，表示误报的情况。

SER (Speaker Error)：指在说话人识别中，错误识别说话人的比例，显示系统对说话人的识别准确性。

DER (Diarticulation Error Rate)：综合考虑MS、FA和SER的一个指标，衡量识别系统的整体性能。
```





## RTTM 文件说明

> RTTM文件格式介绍：https://github.com/nryant/dscore/tree/824f126ae9e78cf889e582eec07941ffe3a7d134?tab=readme-ov-file#rttm

RTTM 表示 Rich Transcription Time Marked 的缩写，富转录时间标记 (RTTM) 文件是以空格分隔的文本文件，每行包含一个发言轮次，每一行包含十个字段。

- **类型**（Type）：段落类型；始终为 “SPEAKER”
- **文件 ID**（File ID）：文件名；录音文件名（不含扩展名），如 `rec1_a`
- **通道 ID**（Channel ID）：通道（1索引）所对应的通道号；始终为 1
- **发声开始时间**（Turn Onset）：发声开始的时间，以录音开始时间为基准，单位为秒
- **发声持续时间**（Turn Duration）：发声的持续时间，单位为秒
- **转录字段**（Orthography Field）：始终为 `<NA>`
- **说话者类型**（Speaker Type）：始终为 `<NA>`
- **说话者名称**（Speaker Name）：该段落中说话者的名称；在每个文件范围内应该是唯一的
- **置信度分数**（Confidence Score）：系统对信息正确性的置信度（概率）；始终为 `<NA>`
- **信号预瞻时间**（Signal Lookahead Time）：始终为 `<NA>`







报错信息

```bash
执行bash run_audio.sh 提示依赖未安装的问题
# 中间会提示各种依赖未安装，按照提示进行安装
pip install addict datasets simplejson sortedcontainers
```


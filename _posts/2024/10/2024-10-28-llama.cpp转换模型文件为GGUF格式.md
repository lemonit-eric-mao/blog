> **参考资料**
>
> - llama.cpp项目Git：https://github.com/ggerganov/llama.cpp
> - ollama的Modelfile配置文档：https://github.com/ollama/ollama/blob/main/docs/modelfile.md
> - ollama仓库地址：https://ollama.com/library







## llama.cpp的作用

1. 运行`.gguf`格式的模型文件，提高推理速度：详情参考[安装llama.cpp二进制文件](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#basic-usage),然后查看[快速开始](https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md#quick-start)进行推理；本文不做介绍

2. 提供了将`.bin、.safetensor`模型文件转化为`.gguf`格式的脚本文件



## 对huggingface的模型文件进行转换

### 快速开始

1. 下载`llama.cpp`项目并进入项目目录(使用[releases](https://github.com/ggerganov/llama.cpp/releases)版本`llama.cpp-b3974`)

   ```bash
   # 下载并解压
   wget https://github.com/ggerganov/llama.cpp/archive/refs/tags/b3974.zip
   unzip b3974.zip
   
   # 进入到项目目录
   cd llama.cpp-b3974
   ```

2. 创建python虚拟环境并安装依赖

   ```bash
   # 创建虚拟环境
   conda create -n llama-cpp python==3.10.12
   
   # 安装依赖
   conda activate llama-cpp
   pip install -r requirements.txt
   ```

3. 执行模型格式转换

   ```bash
   # 在项目根目录创建一个 llm 文件夹，用于存放转换后的模型文件
   mkdir llm
   
   # 执行转换脚本，此处只列出使用到的参数说明，详细说明见后文
   # 路径参数：
   #    model 包含模型文件的目录
   # 其他参数：
   #	 --outfile 转换后模型文件输出目录
   # 	 --outtype 输出格式，可选值：`f32`, `f16`, `bf16`, `q8_0`, `tq1_0`, `tq2_0`, `auto`
   # 	 --model-name 输出的模型文件名的前缀，整个文件名为{model-name}-{outtype}.gguf
   python convert_hf_to_gguf.py /data/LLM/Qwen/Qwen2.5-7B-Instruct --outfile ./llm --outtype f16  --model-name Qwen2.5-7B-Instruct
   ```

4. 生成ollama模型构建文件Modelfile

   ```bash
   # 定义模型构建文件Modelfile，将gguf格式的模型导入到ollama中
   # 	 ollama的Modelfile配置文档可见：https://github.com/ollama/ollama/blob/main/docs/modelfile.md
   # 	 其中TEMPLATE可以参考https://ollama.com/library 中对应模型的template信息
   
   vim Qwen2.5_7B_Modelfile
   
   FROM /data/LLM/llm-tools/llama.cpp-b3974/llm/Qwen2.5-7B-Instruct-F16.gguf
   
   TEMPLATE """{{- if .Messages }}
   {{- if or .System .Tools }}<|im_start|>system
   {{- if .System }}
   {{ .System }}
   {{- end }}
   {{- if .Tools }}
   
   # Tools
   
   You may call one or more functions to assist with the user query.
   
   You are provided with function signatures within <tools></tools> XML tags:
   <tools>
   {{- range .Tools }}
   {"type": "function", "function": {{ .Function }}}
   {{- end }}
   </tools>
   
   For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:
   <tool_call>
   {"name": <function-name>, "arguments": <args-json-object>}
   </tool_call>
   {{- end }}<|im_end|>
   {{ end }}
   {{- range $i, $_ := .Messages }}
   {{- $last := eq (len (slice $.Messages $i)) 1 -}}
   {{- if eq .Role "user" }}<|im_start|>user
   {{ .Content }}<|im_end|>
   {{ else if eq .Role "assistant" }}<|im_start|>assistant
   {{ if .Content }}{{ .Content }}
   {{- else if .ToolCalls }}<tool_call>
   {{ range .ToolCalls }}{"name": "{{ .Function.Name }}", "arguments": {{ .Function.Arguments }}}
   {{ end }}</tool_call>
   {{- end }}{{ if not $last }}<|im_end|>
   {{ end }}
   {{- else if eq .Role "tool" }}<|im_start|>user
   <tool_response>
   {{ .Content }}
   </tool_response><|im_end|>
   {{ end }}
   {{- if and (ne .Role "assistant") $last }}<|im_start|>assistant
   {{ end }}
   {{- end }}
   {{- else }}
   {{- if .System }}<|im_start|>system
   {{ .System }}<|im_end|>
   {{ end }}{{ if .Prompt }}<|im_start|>user
   {{ .Prompt }}<|im_end|>
   {{ end }}<|im_start|>assistant
   {{ end }}{{ .Response }}{{ if .Response }}<|im_end|>{{ end }}
   """
   ```

5. 执行ollama create 将模型导入 ollama服务

   ```bash
   # 构建模型
   ollama create Qwen2.5-7B-Instruct -f Qwen2.5_7B_Modelfile
   
   # 在ollama中测试
   ollama run Qwen2.5-7B-Instruct
   ```

   





### `convert_hf_to_gguf.py`参数说明

该脚本用于将 Hugging Face 模型转换为 GGUF 文件。

#### 参数列表

| 参数                         | 类型   | 描述                                                         |
| ---------------------------- | ------ | ------------------------------------------------------------ |
| `model <路径>`               | 路径   | 包含模型文件的目录                                           |
| `--vocab-only`               | 布尔值 | 仅提取词汇表                                                 |
| `--outfile <路径>`           | 路径   | 输出文件的路径；默认为输入路径，`{ftype}` 将被输出类型替换   |
| `--outtype <类型>`           | 字符串 | 输出格式 - 使用 `f32` 表示 float32，`f16` 表示 float16，`bf16` 表示 bfloat16，`q8_0` 表示 Q8_0，`tq1_0` 或 `tq2_0` 表示三元，`auto` 表示根据第一个加载的张量类型选择最高保真度的 16 位浮点类型 |
| `--bigendian`                | 布尔值 | 模型在 Big Endian 计算机上执行                               |
| `--use-temp-file`            | 布尔值 | 在处理过程中使用临时文件库（在内存不足时有帮助，防止进程被杀死） |
| `--no-lazy`                  | 布尔值 | 通过在写入之前计算所有输出来使用更多 RAM（在懒惰评估出现问题时使用） |
| `--model-name <字符串>`      | 字符串 | 模型的名称                                                   |
| `--verbose`                  | 布尔值 | 提高输出详细程度                                             |
| `--split-max-tensors <整数>` | 整数   | 每个分割的最大张量数                                         |
| `--split-max-size <字符串>`  | 字符串 | 每个分割的最大大小 N(M \| G)                                 |
| `--dry-run`                  | 布尔值 | 仅打印分割计划并退出，不写入任何新文件                       |
| `--no-tensor-first-split`    | 布尔值 | 不将张量添加到第一个分割（默认禁用）                         |
| `--metadata <路径>`          | 路径   | 指定作者身份元数据覆盖文件的路径                             |
---
title: "K8S 常见问题与解决方案"
date: "2019-12-02"
categories: 
  - "k8s"
---

## 污点问题，所有Pod都调度到master节点中运行，而新部署的Pod会直接调度失败

> k8s正常情况下创建Pod，如果没有容忍污点，它会被驱逐到没有污点的机器上，但是我新创建了一个测试Pod发现竟然调度失败，说是没有容忍Master，所以我就想到，是不是所有的节点都被人为的，误操作都打上了master：NoSchedule污点，才导致调度失败， 我一查，果然是这样 至于为什么有一些程序确创建在了master中，是因为添加了容忍度，在k8s中如果添加了容忍度，即使不允许调度，最后也是会被调度器接受的

* * *

* * *

* * *

## calico 老版本Bug

### 我做了什么？

> 在k8s中使用calico的注解为pod分配固定IP段时，使用了`自定义ippool`，在pod经过几轮删除后有概率出现多个pod具有相同IP的问题
> 
> - 出现问题的版本为：
>     - calico：3.17.1
>     - kubernetes：1.20.4

### 异常情况如下

```shell
## Pod信息如下
NAME                       READY   STATUS    RESTARTS   AGE   IP           NODE             NOMINATED NODE   READINESS GATES
my-nginx-c44cdb6b6-6rlqf   1/1     Running   0          29m   10.101.0.0   k8s2-worker-05   <none>           <none>
my-nginx-c44cdb6b6-rzfln   1/1     Running   0          36m   10.101.0.1   k8s2-worker-02   <none>           <none>
my-nginx-c44cdb6b6-fmxpw   1/1     Running   0          29m   10.101.0.2   k8s2-worker-01   <none>           <none>
my-nginx-c44cdb6b6-s9xds   1/1     Running   0          32m   10.101.0.2   k8s2-worker-05   <none>           <none>
my-nginx-c44cdb6b6-tbwb4   1/1     Running   0          36m   10.101.0.4   k8s2-worker-02   <none>           <none>
my-nginx-c44cdb6b6-hqkpc   1/1     Running   0          34m   10.101.0.5   k8s2-worker-03   <none>           <none>
my-nginx-c44cdb6b6-xgrhc   1/1     Running   0          34m   10.101.0.6   k8s2-worker-05   <none>           <none>
my-nginx-c44cdb6b6-ds79t   1/1     Running   0          29m   10.101.0.7   k8s2-worker-05   <none>           <none>

```

**在github上提交了该问题，得到的回复如下(中英对照)：**

> Calico 3.17 is ancient, and we've made many improvements to the IPAM sub-system in releases since then - specifically in Calico v3.23 and v3.24, we fix a number of bugs that manifested with symptoms just like this. Could you try running Calico v3.25 and see if you hit the same issues? Please re-open a new issue if you see problems on modern versions of Calico. Calico 3.17已经非常老旧了，我们在之后的版本中对IPAM子系统进行了许多改进——特别是在Calico v3.23和v3.24中，我们修复了许多与此类似的问题。请尝试运行Calico v3.25，并查看是否会遇到相同的问题。如果您在现代版本的Calico中遇到问题，请重新打开一个新问题。

### 解决方法

> - 将对应环境的版本更新为：
>     - calico：3.25.0
>     - kubernetes：1.26.0 然后进行测试，该问题没有再次出现

* * *

* * *

* * *

## 创建k8s初始化配置文件

```shell
kubeadm config print init-defaults
```

* * *

* * *

* * *

## 为Kubernetes中的`Service`、`Pod`指定固定IP

### 创建新的IP池

```shell
## 创建配置文件
cat > white-list-ippool.yaml <<  ERIC

apiVersion: crd.projectcalico.org/v1
kind: IPPool
metadata:
  name: white-list-ipv4-ippool
spec:
  # 必须写blockSize，才能在下面划分子网
  # 范围 20 ~ 32
  blockSize: 27
  # 在这才可以划分子网只留5个IP地址范围
  # k8s
  cidr: 10.102.0.0/27
  ipipMode: Never
  natOutgoing: true

ERIC


kubectl apply -f white-list-ippool.yaml

```

#### 查看详细内容

```shell
kubectl get ippool default-ipv4-ippool white-list-ipv4-ippool -o json | jq '.items[] | {apiVersion,name:.metadata.name,kind,spec}'

{
  "apiVersion": "crd.projectcalico.org/v1",
  "name": "default-ipv4-ippool",
  "kind": "IPPool",
  "spec": {
    "blockSize": 26,
    "cidr": "10.100.0.0/16",       # 默认k8s中自动调度的网段
    "ipipMode": "Never",
    "natOutgoing": true,
    "nodeSelector": "all()",
    "vxlanMode": "CrossSubnet"
  }
}
{
  "apiVersion": "crd.projectcalico.org/v1",
  "name": "white-list-ipv4-ippool",
  "kind": "IPPool",
  "spec": {
    "blockSize": 27,
    "cidr": "10.102.0.0/27",       # 自己添加的网段
    "ipipMode": "Never",
    "natOutgoing": true
  }
}

```

### 编写测试程序

```shell
## 测试限制Pod子网
cat > conf-range.yaml << ERIC

apiVersion: v1
kind: Service
metadata:
  labels:
    app: my-app
  name: my-app
spec:
  # 为Service指定固定IP
  clusterIP: 10.96.97.98
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: my-app

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
      annotations:
        # 使用注解为Pod批量IP池
        cni.projectcalico.org/ipv4pools: '["white-list-ipv4-ippool"]'
    spec:
      containers:
      - name: my-container
        image: nginx:1.21.1

ERIC


kubectl apply -f conf-range.yaml

```

### 查看效果

```shell
┌──(root@k8s1-master 15:22:59) - [/data/siyu.mao/white_list]
└─# kubectl get ippools,svc,deploy,po -o wide

## ippool
NAME                                                  AGE
ippool.crd.projectcalico.org/default-ipv4-ippool      149d
ippool.crd.projectcalico.org/white-list-ipv4-ippool   18m


## service
NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE    SELECTOR
service/kubernetes   ClusterIP   10.96.0.1     <none>        443/TCP   149d   <none>
service/my-app       ClusterIP   10.96.97.98   <none>        80/TCP    32s    app=my-app


## deployment
NAME                     READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS     IMAGES         SELECTOR
deployment.apps/my-app   3/3     3            3           32s   my-container   nginx:1.21.1   app=my-app


## pod
NAME                          READY   STATUS    RESTARTS   AGE   IP           NODE             NOMINATED NODE   READINESS GATES
pod/my-app-54b67758fc-qhhws   1/1     Running   0          32s   10.102.0.8   k8s1-worker-01   <none>           <none>
pod/my-app-54b67758fc-r2gbw   1/1     Running   0          32s   10.102.0.6   k8s1-worker-02   <none>           <none>
pod/my-app-54b67758fc-mckmg   1/1     Running   0          32s   10.102.0.7   k8s1-worker-03   <none>           <none>

```

## 解释

> 使用 Calico CNI 插件为 Kubernetes 集群中的 Pod 分配固定 IP 范围。下面是对这个配置的一些解释和补充说明：
> 
> 1. **IPPool 的配置** 在这个配置中，定义了一个名为 `white-list-ipv4-ippool` 的 IPPool，用来为 Pod 分配 IP 地址。其中：
>     - `blockSize: 27`：定义了每个子网的大小为 27，这意味着每个子网有 32 - 27 = 5 个 IP 地址可用。
>     - `cidr: 10.102.0.0/27`：定义了该 IPPool 的 IP 地址范围，这里是 10.102.0.0/27，表示使用 10.102.0.0 ~ 10.102.0.31 这 32 个 IP 地址范围作为 IPPool 的可用 IP 地址。
>     - `ipipMode: Never`：这个选项表示禁用 IPIP 封装，这个选项是可选的，不过可以提高性能。
>     - `natOutgoing: true`：这个选项表示启用 NAT，也是可选的。
> 2. **Deployment 的配置** 这个配置中定义了一个 Deployment，其中：
>     - `metadata.name: my-deployment`：定义了 Deployment 的名称。
>     - `spec.replicas: 3`：定义了该 Deployment 中 Pod 的副本数量为 3。
>     - `spec.selector.matchLabels.app: my-app`：定义了该 Deployment 选择器的标签，这里是 app: my-app。
>     - `template.metadata.labels.app: my-app`：定义了该 Pod 的标签，这里是 app: my-app。
>     - `template.metadata.annotations.cni.projectcalico.org/ipv4pools: '["white-list-ipv4-ippool"]'`：这个注释指定了要使用的 IP 地址池，这里是 `white-list-ipv4-ippool`，这样就会从这个 IPPool 中分配 IP 地址。
>     - `spec.containers.name: my-container`：定义了 Pod 中的容器名称。
>     - `spec.containers.image: nginx:1.21.1`：定义了容器使用的镜像。
> 
> 这个配置可以保证 Pod 只能使用 IPPool 中定义的 IP 地址范围，从而实现限制 Pod IP 地址范围的目的。

* * *

* * *

* * *

### **[大规模集群的注意事项](https://kubernetes.io/zh-cn/docs/setup/best-practices/cluster-large/ "大规模集群的注意事项")**

> - **Kubernetes v1.24 的标准配置**
>     - 每个节点的 Pod 数量不超过 `110`
>     - 节点数不超过 `5000`
>     - Pod 总数不超过 `150000`
>     - 容器总数不超过 `300000`

* * *

* * *

* * *

### 安装 **Kubernetes `v1.24`** **containerd `v1.6.4`** 时，**`master`** 节点启动后 **`NotReady`**

```ruby
## 问题
[root@master01 ~]# kubectl get nodes
NAME       STATUS     ROLES           AGE   VERSION
master01   NotReady   control-plane   53s   v1.24.0
[root@master01 ~]#


## 排查原因
[root@master01 ~]# kubectl describe nodes | grep error
    Ready            False   Wed, 25 May 2022 18:56:46 +0800   Wed, 25 May 2022 18:56:38 +0800   KubeletNotReady              container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized
[root@master01 ~]#

## 或者使用k9s查看 Describe
│ Conditions:
│   Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
│   ---- ------ ----------------- ------------------ ------ -------
│   MemoryPressure   False   Wed, 25 May 2022 18:56:46 +0800   Wed, 25 May 2022 18:56:38 +0800   KubeletHasSufficientMemory   kubelet has sufficient memory available
│   DiskPressure     False   Wed, 25 May 2022 18:56:46 +0800   Wed, 25 May 2022 18:56:38 +0800   KubeletHasNoDiskPressure     kubelet has no disk pressure
│   PIDPressure      False   Wed, 25 May 2022 18:56:46 +0800   Wed, 25 May 2022 18:56:38 +0800   KubeletHasSufficientPID      kubelet has sufficient PID available
│   Ready            False   Wed, 25 May 2022 18:56:46 +0800   Wed, 25 May 2022 18:56:38 +0800   KubeletNotReady              container runtime network notready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized
│ Addresses:
│   InternalIP:  10.250.119.20
│   Hostname:    master01

```

`message:Network plugin returns error: cni plugin not initialized` **[官方解决方案](https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/troubleshooting-cni-plugin-related-errors/#an-example-containerd-configuration-file "官方解决方案")**

> - 在 Kubernetes 上，containerd 运行时lo向 pod 添加了一个环回接口，作为默认行为。containerd 运行时通过 CNI 插件配置环回接口，loopback. 该插件作为具有该名称的发布包的loopback一部分分发 。 v1.6.0 及更高版本包括与 CNI v1.0.0 兼容的环回插件以及其他默认 CNI 插件。loopback 插件的配置由 containerd 内部完成，并设置为使用 CNI v1.0.0。这也意味着启动这个新版本时插件的版本必须是 v1.0.0 或更高版本 。containerdcnicontainerdloopbackcontainerd
>     
> - 以下 **bash** 命令生成示例 CNI 配置。在这里，配置版本的 1.0.0 值被分配给该cniVersion字段以在 containerd调用 CNI 桥接插件时使用。
>     

```ruby
cat << EOF | tee /etc/cni/net.d/10-containerd-net.conflist
{
 "cniVersion": "1.0.0",
 "name": "containerd-net",
 "plugins": [
   {
     "type": "bridge",
     "bridge": "cni0",
     "isGateway": true,
     "ipMasq": true,
     "promiscMode": true,
     "ipam": {
       "type": "host-local",
       "ranges": [
         [{
           "subnet": "10.88.0.0/16"
         }],
         [{
           "subnet": "2001:db8:4860::/64"
         }]
       ],
       "routes": [
         { "dst": "0.0.0.0/0" },
         { "dst": "::/0" }
       ]
     }
   },
   {
     "type": "portmap",
     "capabilities": {"portMappings": true}
   }
 ]
}
EOF

```

* * *

* * *

* * *

### keepalive 脑裂问题

如果keepalive启动时发现为 **`双master`**，恭喜中奖，发生脑裂了 通常情况下初始启动不会有这样的问题，可优先排查 vip(虚IP) 是否被其它主机占用 ping vip

```ruby
## 能ping通表示被占用
ping xxx.xxx.xxx.xxx
```

* * *

* * *

* * *

### 使用`calicoctl`切换网络模式

```ruby
## 下载 calicoctl
[root@master01 ~]# wget -P /usr/local/bin/ http://qiniu.dev-share.top/cni/calicoctl && chmod +x /usr/local/bin/calicoctl

[root@master01 ~]# calicoctl --allow-version-mismatch get ippool -o wide
NAME                  CIDR             NAT    IPIPMODE   VXLANMODE     DISABLED   SELECTOR
default-ipv4-ippool   192.169.0.0/16   true   Never      CrossSubnet   false      all()




## 切换网络模式
[root@master01 ~]# calicoctl --allow-version-mismatch apply  -f - << ERIC
apiVersion: projectcalico.org/v3
kind: IPPool
metadata:
  name: default-ipv4-ippool
spec:
  cidr: 192.169.0.0/16
  ipipMode: Never
  vxlanMode: Always
  natOutgoing: true
  disabled: false

ERIC


[root@master01 ~]# calicoctl --allow-version-mismatch get ippool -o wide
NAME                  CIDR             NAT    IPIPMODE   VXLANMODE   DISABLED   SELECTOR
default-ipv4-ippool   192.169.0.0/16   true   Never      Always      false      all()

[root@master01 ~]#  ping 192.169.5.1
PING 192.169.5.1 (192.169.5.1) 56(84) bytes of data.
64 bytes from 192.169.5.1: icmp_seq=1 ttl=63 time=0.899 ms
64 bytes from 192.169.5.1: icmp_seq=2 ttl=63 time=0.734 ms
64 bytes from 192.169.5.1: icmp_seq=3 ttl=63 time=0.683 ms
64 bytes from 192.169.5.1: icmp_seq=4 ttl=63 time=0.867 ms

```

* * *

* * *

* * *

### `kubelet nodes not sync`

```ruby
[root@master01 cloud]# journalctl -xeu kubelet
5月 25 15:44:10 master01 kubelet[8442]: E0525 15:44:10.922294    8442 kubelet.go:2419] "Error getting node" err="node \"master01\" not found"
5月 14 00:55:37 master01 kubelet[7821]: I0514 00:55:37.420655    7821 kubelet.go:449] kubelet nodes not sync

 原因一： ApiServer服务启动之前这个错误是正常的
 原因二： 环境变量 APISERVER_IP 地址写错
 原因三： kubelet需要的 k8s.gcr.io/pause 版本不对

```

* * *

* * *

* * *

### 无法删除 k8s 自定义资源类型(CRD)

**`kubectl patch crd/你的CRD -p '{"metadata":{"finalizers":[]}}' --type=merge`**

```ruby
## 例如删除： proxydefaults.consul.hashicorp.com
kubectl patch crd/proxydefaults.consul.hashicorp.com -p '{"metadata":{"finalizers":[]}}' --type=merge
customresourcedefinition.apiextensions.k8s.io/proxydefaults.consul.hashicorp.com patched

```

* * *

* * *

* * *

### 为 Pod 或容器配置安全性上下文

**使用`root`用户操作容器**

```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-one
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo-one
  template:
    metadata:
      name: demo-one
      labels:
        app: demo-one
    spec:
      serviceAccountName: demo-one
      containers:
        - name: demo-one
          image: curlimages/curl:7.77.0
          ports:
            - containerPort: 8080
          ## 配置后， 将以root用户权限进入容器
          securityContext:
            runAsUser: 0

```

* * *

* * *

* * *

### **[通过 HostAliases 增加额外条目](https://kubernetes.io/zh/docs/tasks/network/customize-hosts-file-for-pods/#%E9%80%9A%E8%BF%87-hostaliases-%E5%A2%9E%E5%8A%A0%E9%A2%9D%E5%A4%96%E6%9D%A1%E7%9B%AE "通过 HostAliases 增加额外条目")**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hostaliases-pod
spec:
  restartPolicy: Never
  hostAliases:
  - ip: "127.0.0.1"
    hostnames:
    - "foo.local"
    - "bar.local"
  - ip: "10.1.2.3"
    hostnames:
    - "foo.remote"
    - "bar.remote"
  containers:
  - name: cat-hosts
    image: busybox
    command:
    - cat
    args:
    - "/etc/hosts"

```

* * *

* * *

* * *

### [端口转发](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#port-forward "端口转发")

`kubectl -n 程序所属命名空间 port-forward svc/程序的Service名 --address 要对外暴露的地址 对外暴露的端口:程序的Service端口`

```ruby
kubectl -n flink port-forward svc/test-flink-cdc-rest --address 192.168.101.11  8082:8081
Forwarding from 192.168.101.11:8082 -> 8081

```

* * *

* * *

* * *

### [代理](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#proxy "代理")

`kubectl proxy [--port=PORT] [--www=static-dir] [--www-prefix=prefix] [--api-prefix=prefix]`

```ruby
kubectl proxy
```

* * *

* * *

* * *

### 直接获取yaml中的数据

```ruby
kubectl -n kube-system get svc kube-dns -o jsonpath={.spec.clusterIP}

kubectl -n kube-system get svc kube-dns -o template={{.spec.clusterIP}}

kubectl -n dhc-consul get secret consul-bootstrap-acl-token -o template={{.data.token}} | base64 -d

```

* * *

* * *

* * *

### 动态修改端口

```ruby
kubectl -n dhc-consul patch svc consul-ui -p '{
    "spec": {
        "ports": [
            {
                "nodePort": 30443,
                "port": 443,
                "protocol": "TCP",
                "targetPort": 8501
            }
        ],
        "type": "NodePort"
    }
}'

```

* * *

* * *

* * *

### **[改变默认 StorageClass](https://kubernetes.io/zh/docs/tasks/administer-cluster/change-default-storage-class/#%E6%94%B9%E5%8F%98%E9%BB%98%E8%AE%A4-storageclass "改变默认 StorageClass")**

`kubectl patch storageclass <your-class-name> -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'`

```ruby
[root@master01 ~]# kubectl get storageclass

NAME              PROVISIONER                  RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
rook-ceph-block   rook-ceph.rbd.csi.ceph.com   Delete          Immediate           true                   50d
[root@master01 ~]#


[root@master01 ~]# kubectl patch storageclass rook-ceph-block -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
storageclass.storage.k8s.io/rook-ceph-block patched
[root@master01 ~]#


[root@master01 ~]# kubectl get storageclass
NAME                        PROVISIONER                  RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
rook-ceph-block (default)   rook-ceph.rbd.csi.ceph.com   Delete          Immediate           true                   50d
[root@master01 ~]#

```

* * *

* * *

* * *

### 批量执行文件夹下所有文件

`kubectl apply -R`

```ruby
kubectl apply -R -f mssp-web/templates/
```

* * *

* * *

* * *

### `Get "http://127.0.0.1:10252/healthz": dial tcp 127.0.0.1:10252: connect: connection refused`

```ruby
[root@master01 ~]# kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS      MESSAGE                                                                                       ERROR
controller-manager   Unhealthy   Get "http://127.0.0.1:10252/healthz": dial tcp 127.0.0.1:10252: connect: connection refused
scheduler            Unhealthy   Get "http://127.0.0.1:10251/healthz": dial tcp 127.0.0.1:10251: connect: connection refused
etcd-0               Healthy     {"health":"true"}
[root@master01 ~]#
```

### 解决方案

```ruby
sed -i s/'- --port=0'/'#- --port=0'/g /etc/kubernetes/manifests/kube-controller-manager.yaml
sed -i s/'- --port=0'/'#- --port=0'/g /etc/kubernetes/manifests/kube-scheduler.yaml

## 所有主节点都要重启 kubelet
systemctl restart kubelet.service

kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE             ERROR
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-0               Healthy   {"health":"true"}

```

* * *

* * *

* * *

### 查看 kube-proxy模式

```ruby
[root@master01 ~]# kubectl logs -n kube-system ds/kube-proxy
I0404 16:34:17.442199       1 node.go:172] Successfully retrieved node IP: 192.168.103.232
I0404 16:34:17.442379       1 server_others.go:142] kube-proxy node IP is an IPv4 address (192.168.103.232), assume IPv4 operation
W0404 16:34:17.712355       1 server_others.go:578] Unknown proxy mode "", assuming iptables proxy
I0404 16:34:17.712446       1 server_others.go:185] Using iptables Proxier.  # 当前是 iptables模式
I0404 16:34:17.712712       1 server.go:650] Version: v1.20.4
......
```

* * *

* * *

* * *

### 修改calico的网络模式 BGP、IPIP

**`注：` IPIP** 它解决的是在**同一k8s集群中**，各主机的ip地址段不同时，实现集群内的pod互通

* * *

### [参考官方资料](https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises#install-calico-with-kubernetes-api-datastore-50-nodes-or-less "参考官方资料")

### [参考资料](https://www.icode9.com/content-4-697463.html "参考资料")

### [参考资料](https://blog.csdn.net/u010771890/article/details/103224004 "参考资料")

* * *

### 第一种 使用 calico.yaml 部署

```yaml
## 官网下载安装文件
wget https://docs.projectcalico.org/manifests/calico.yaml

## 编辑 calico.yaml 文件
......
            # Enable IPIP 模式
            - name: CALICO_IPV4POOL_IPIP
              value: "Always"

            ## Enable BGP 模式
            #- name: CALICO_IPV4POOL_IPIP
            #  value: "Never"
......

## 执行
kubectl apply -f calico.yaml
```

* * *

### 第二种 使用 tigera-operator 部署

`官网下载 tigera-operator.yaml`

```ruby
cat > custom-resources.yaml << ERIC

apiVersion: operator.tigera.io/v1
kind: Installation
metadata:
  name: default
spec:
  # Configures Calico networking.
  calicoNetwork:
    # Note: The ipPools section cannot be modified post-install.
    ipPools:
    - blockSize: 26
      #  k8s初始化时设定pod的网段一致 podSubnet: 10.244.0.0/16
      cidr: 10.244.0.0/16
      # supported values: "IPIPCrossSubnet", "IPIP", "VXLAN", "VXLANCrossSubnet", "None"
      encapsulation: IPIP
      natOutgoing: Enabled
      nodeSelector: all()

ERIC

kubectl apply -f tigera-operator.yaml -f custom-resources.yaml
```

* * *

* * *

* * *

* * *

* * *

* * *

### calico 网络异常 `1`

```ruby
[root@master01 ~]# kubectl describe pod calico-node-c45tk -n calico-system

......

Events:
  Type     Reason     Age                    From               Message
  ---- ------ ---- ---- -------
  Normal   Scheduled  9m22s                  default-scheduler  Successfully assigned calico-system/calico-node-c45tk to master01
  Normal   Pulled     9m21s                  kubelet            Container image "docker.io/calico/pod2daemon-flexvol:v3.18.1" already present on machine
  Normal   Created    9m21s                  kubelet            Created container flexvol-driver
  Normal   Started    9m21s                  kubelet            Started container flexvol-driver
  Normal   Pulled     9m20s                  kubelet            Container image "docker.io/calico/cni:v3.18.1" already present on machine
  Normal   Created    9m20s                  kubelet            Created container install-cni
  Normal   Started    9m19s                  kubelet            Started container install-cni
  Normal   Pulled     9m17s                  kubelet            Container image "docker.io/calico/node:v3.18.1" already present on machine
  Normal   Created    9m16s                  kubelet            Created container calico-node
  Normal   Started    9m16s                  kubelet            Started container calico-node
  Warning  Unhealthy  4m18s (x30 over 9m8s)  kubelet            Readiness probe failed: calico/node is not ready: BIRD is not ready: Error querying BIRD: unable to connect to BIRDv4 socket: dial unix /var/run/calico/bird.ctl: connect: connection refused
[root@master01 ~]#
#-----------------------------------------------------------------
## 主机DNS无法解析，导致链接被拒绝， 检查集群中所有主机DNS
```

* * *

### calico 网络异常 `2`

```ruby
  Warning  Unhealthy  4m54s  kubelet            Readiness probe failed: 2021-04-01 09:09:43.096 [INFO][190] confd/health.go 180: Number of node(s) with BGP peering established = 0
calico/node is not ready: BIRD is not ready: BGP not established with 240.168.103.230,240.168.103.231

```

### 修改 calico.yaml 或 tigera-operator.yaml 文件，添加如下环境变量

```yaml
......
      containers:
        - name: tigera-operator
          image: quay.io/tigera/operator:v1.15.1
          ......
          env:
            ## 新增
            # Specify interface
            - name: IP_AUTODETECTION_METHOD
              # ens 根据实际网卡开头配置
              value: "interface=ens."
......
```

* * *

* * *

* * *

### headless svc 与 正常 svc 区别

```ruby
[root@master01 ~]# kubectl exec busybox-bc7c57467-tt57c -- nslookup svc-demo-four
Server:    10.222.0.10
Address 1: 10.222.0.10 kube-dns.kube-system.svc.cluster.local

Name:      svc-demo-four
### 正常的service，这里只显示它自己的ClusterIP地址 ###
Address 1: 10.222.48.203 svc-demo-four.default.svc.cluster.local
[root@master01 ~]#

# -------------------------------------------------------------------------- #

[root@master01 ~]# kubectl exec busybox-bc7c57467-tt57c -- nslookup svc-demo-two
Server:    10.222.0.10
Address 1: 10.222.0.10 kube-dns.kube-system.svc.cluster.local

Name:      svc-demo-two
### 使用headless的svc，这里显示的是Pod的IP地址 ###
Address 1: 10.244.2.46 10-244-2-46.svc-demo-two.default.svc.cluster.local
Address 2: 10.244.1.45 10-244-1-45.svc-demo-two.default.svc.cluster.local
[root@master01 ~]#
```

* * *

### [用 Pod 字段作为环境变量的值](https://kubernetes.io/zh/docs/tasks/inject-data-application/environment-variable-expose-pod-information/#%E7%94%A8-pod-%E5%AD%97%E6%AE%B5%E4%BD%9C%E4%B8%BA%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E7%9A%84%E5%80%BC "用 Pod 字段作为环境变量的值"), 传给Java应用

* * *

### application.yml

```yaml
spring:
  application:
    name: demo-one
  cloud:
    consul:
      host: ${consul-client-ip:123.177.23.151} # Consul 注册地址
      port: ${consul-client-port:8500} # Consul 端口
      discovery:
        instance-id: ${k8s.pod.name} # 应用程序在注册中心的唯一标识，如果标识重复注册信息会被替换； 所以k8s 部署时动态传入pod名
        prefer-ip-address: true
        enabled: true
        register: true # 注册自身到consul
        deregister: true # 服务停止时取消注册
        service-name: ${spring.application.name} # 注册到 Consul 的服务名，默认为 `spring.application.name` 配置项
        health-check-path: /actuator/health # 健康检查的接口，默认为 /actuator/health，由 Spring Boot Actuator 提供
        ip-address: ${spring.cloud.client.ip-address} # 访问本程序的地址
        port: ${MY_CONSUL_PORT} # 访问本程序的端口
management:
  endpoints:
    web:
      exposure:
        include: "*"
```

* * *

### deployment.yaml

```yaml
      containers:
        - name: demo-three
          env:
            - name: MY_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: MY_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name

            # 将Pod名称传给Java应用
            - name: k8s.pod.name
              value: '${MY_POD_NAME}'

            - name: consul-client-ip
              value: 10.249.58.11
            - name: consul-client-port
              value: '30850'
            - name: spring.cloud.client.ip-address
              value: 10.249.58.17
            - name: MY_CONSUL_PORT
              value: '30084'
          image: '10.249.58.10:8081/skywalking/demo-three:1.3'
          imagePullPolicy: Always
          ports:
            - containerPort: 18083
              protocol: TCP
```

* * *

* * *

* * *

### 部署k8s不关闭swap

kubernetes关闭swap主要是为了性能考虑,当然如果不想关闭swap，需要编辑`/etc/sysconfig/kubelet`, 添加`"KUBELET_EXTRA_ARGS=--fail-swap-on=false"`

```ruby
echo "KUBELET_EXTRA_ARGS=--fail-swap-on=false" > /etc/sysconfig/kubelet
```

* * *

* * *

* * *

### **[转载-声明式API](https://blog.csdn.net/kevinbetterq/article/details/104012293 "转载-声明式API")**

* * *

### **[磁盘爆满](https://www.bookstack.cn/read/kubernetes-practice-guide/troubleshooting-handle-disk-full.md "磁盘爆满")**

### **[官方-配置资源不足时的处理方式](https://kubernetes.io/zh/docs/concepts/configuration/pod-priority-preemption/ "官方-配置资源不足时的处理方式")**

- **什么情况下磁盘可能会爆满？**
  - kubelet 有 gc 和驱逐机制，通过 `--image-gc-high-threshold`、`--image-gc-low-threshold`、`--eviction-hard`、`--eviction-soft`、`--eviction-minimum-reclaim` 等参数控制 kubelet 的 gc 和驱逐策略来释放磁盘空间，如果配置正确的情况下，磁盘一般不会爆满。
  - 通常导致爆满的原因可能是配置不正确或者节点上有其它非 K8S 管理的进程在不断写数据到磁盘占用大量空间导致磁盘爆满。

- **磁盘爆满会有什么影响？**
  - 影响 K8S 运行我们主要关注 kubelet 和容器运行时这两个最关键的组件，它们所使用的目录通常不一样，kubelet 一般不会单独挂盘，直接使用系统磁盘，因为通常占用空间不会很大，容器运行时单独挂盘的场景比较多，当磁盘爆满的时候我们也要看 kubelet 和 容器运行时使用的目录是否在这个磁盘，通过 df 命令可以查看磁盘挂载点。
    
- **查看工作节点磁盘是否满了**

```shell
## 查看工作节点是是否被k8s自动标记了污点
kubectl describe nodes worker02 | grep Taints
Taints:             node.kubernetes.io/disk-pressure:NoSchedule

```

* * *

### **[自动补全](cka-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%e4%b8%80 "自动补全")**

* * *

* * *

* * *

### 挂起pod，排查应用程序中的问题

**因为k8s中 service、deploy与pod之间都是通过 label进行绑定的，所以要实现pod类似于挂起的功能，就要将他们解绑， 实现方式就是删除 `pod` 带有绑定关系的label**

```ruby
### 删除带有绑定关系的label
[root@master01 ~]# kubectl label pod my-nginx-845fc9f6b9-zrlns app-


[root@master01 ~]# kubectl get pod
NAME                        READY   STATUS              RESTARTS   AGE
my-nginx-845fc9f6b9-sdslw   0/1     ContainerCreating   0          12s           # 新生成的
my-nginx-845fc9f6b9-zrlns   1/1     Running             0          3d13h         # 原有的
[root@master01 ~]#
```

* * *

### **[k8s端口范围](https://kubernetes.io/zh/docs/concepts/services-networking/service/#nodeport "k8s端口范围")**， 修改端口取值范围

**默认端口的范围是 `30000-32767`**

### 在主节点上，修改 apiserver配置文件

```ruby
vim /etc/kubernetes/manifests/kube-apiserver.yaml

# 添加到如下位置就行了
...... 省略
- command:
   - kube-apiserver
   - --service-node-port-range=1-65535
...... 省略
```

* * *

### 直接删除kube-apiserver  pod 就行了 等待自动重启

```ruby
kubectl delete pod kube-apiserver -n kube-system
```

* * *

[原文链接](https://blog.csdn.net/fire_work/java/article/details/106214188 "原文链接")

* * *

* * *

* * *

### 强制删除

```ruby
NS_NAME=命名空间

kubectl get namespace $NS_NAME -o json | tr -d "\n" | sed "s/\"finalizers\": \[[^]]\+\]/\"finalizers\": []/" | kubectl replace --raw /api/v1/namespaces/$NS_NAME/finalize -f -

```

* * *

### 打标签

node、 deploy、 namespace 等都可以打标签 `kubectl label [node | deploy | namespace | ......] 名称 标签名(key=value)`

### **`1` 给node节点添加 label**

添加：`kubectl label nodes 节点名 标签名(key=value)`

```ruby
[root@k8s-master ~]# kubectl label nodes dev-eric eric=msy
```

删除: `kubectl label nodes 节点名 标签名(key-)`

```ruby
[root@k8s-master ~]# kubectl label nodes dev-eric eric-
```

查看: `kubectl get nodes --show-labels`

```ruby
[root@k8s-master ~]# kubectl get nodes --show-labels
```

* * *

### **`2` 操作容器**

**进入容器**

```ruby
[root@k8s-master ~]# kubectl exec -it pod名 -n 命名空间 bash
```

**导出文件**

```ruby
[root@k8s-master ~]# kubectl cp pod名:heap.bin ./ -n 命名空间 bash
```

**查看容器 log**

```ruby
[root@k8s-master ~]# kubectl logs --tail 1000 -f 容器名 -n 命名空间
```

**查看容器详细信息**

```ruby
[root@k8s-master ~]# kubectl describe pod 容器名 -n 命名空间
```

* * *

### **`3`** kubectl 常用命令

```ruby
# 查询所有 pod
[root@k8s-master deploy]# kubectl get pod -A -o wide
# 查询所有节点
[root@k8s-master deploy]# kubectl get nodes -o wide
# 查看k8s问题节点日志
[root@k8s-master deploy]# journalctl -f -u kubelet
# 查看命名空间
[root@k8s-master deploy]# kubectl get namespace
```

* * *

### **`4`** kubeadm 常用命令

```ruby
# 启动一个 Kubernetes 主节点
[root@k8s-master deploy]# kubeadm init
# 启动一个 Kubernetes 工作节点并且将其加入到集群
[root@k8s-master deploy]# kubeadm join
# 更新一个 Kubernetes 集群到新版本
[root@k8s-master deploy]# kubeadm upgrade
# 如果使用 v1.7.x 或者更低版本的 kubeadm 初始化集群，您需要对集群做一些配置以便使用 kubeadm upgrade 命令
[root@k8s-master deploy]# kubeadm config
# 管理 kubeadm join 使用的令牌
[root@k8s-master deploy]# kubeadm token
# 重新生成链接 Token
[root@k8s-master deploy]# kubeadm token create --print-join-command
# 查看未失效的 Token列表
[root@k8s-master deploy]# kubeadm token list
# 还原 kubeadm init 或者 kubeadm join 对主机所做的任何更改
[root@k8s-master deploy]# kubeadm reset
# 打印用于 'kubeadm init' 的默认 init 配置
# https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-config/#cmd-config-print-init-defaults
[root@k8s-master deploy]# kubeadm config print init-defaults
```

* * *

### **`5`** 伸缩容器命令

```ruby
# 查看在 021-deploy 里面启动了几个 pod
[root@k8s-master ~]# kubectl get deploy 021-deploy
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
021-deploy   1/1     1            1           15d
[root@k8s-master ~]#
# 缩减为 0 个 pod
[root@k8s-master ~]# kubectl scale deploy 021-deploy --replicas=0
deployment.extensions/021-deploy scaled
[root@k8s-master ~]#
[root@k8s-master ~]# kubectl get deploy 021-deploy
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
021-deploy   0/0     0            0           15d
[root@k8s-master ~]#

# 扩充到 5 个 pod
[root@k8s-master ~]# kubectl scale deploy 021-deploy --replicas=5
deployment.extensions/021-deploy scaled
[root@k8s-master ~]#
[root@k8s-master ~]# kubectl get deploy 021-deploy
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
021-deploy   5/5     5            5           15d
[root@k8s-master ~]#
```

* * *

### **`6`** 批量操作 Pod

> #### 列出所有【非Running】状态的Pod
> 
> ```ruby
> kubectl get pod --field-selector=status.phase!=Running -A
> ```
> 
> #### 删除所有【非Running】状态的Pod
> 
> ```ruby
> kubectl delete pod --field-selector=status.phase!=Running -A
> ```

* * *

> #### 列出所有【已完成】状态的Pod
> 
> ```shell
> kubectl get pod --field-selector=status.phase==Succeeded
> ```
> 
> #### 删除所有【已完成】状态的Pod
> 
> ```shell
> kubectl delete pod --field-selector=status.phase==Succeeded
> ```

* * *

### 强制删除 POD

```ruby
POD_NAME=Pod名

kubectl delete pods $POD_NAME --grace-period=0 --force

```

* * *

### **`7`** 查看kubectl 所有可用命令、简写命令 `kubectl api-resources`

```ruby
[root@master redis-k8s]# kubectl api-resources
NAME                              SHORTNAMES   APIGROUP                       NAMESPACED   KIND
configmaps                        cm                                          true         ConfigMap
endpoints                         ep                                          true         Endpoints
events                            ev                                          true         Event
limitranges                       limits                                      true         LimitRange
namespaces                        ns                                          false        Namespace
nodes                             no                                          false        Node
persistentvolumeclaims            pvc                                         true         PersistentVolumeClaim
persistentvolumes                 pv                                          false        PersistentVolume
pods                              po                                          true         Pod
podtemplates                                                                  true         PodTemplate
replicationcontrollers            rc                                          true         ReplicationController
resourcequotas                    quota                                       true         ResourceQuota
serviceaccounts                   sa                                          true         ServiceAccount
services                          svc                                         true         Service
daemonsets                        ds           apps                           true         DaemonSet
deployments                       deploy       apps                           true         Deployment
replicasets                       rs           apps                           true         ReplicaSet
statefulsets                      sts          apps                           true         StatefulSet
daemonsets                        ds           extensions                     true         DaemonSet
deployments                       deploy       extensions                     true         Deployment
ingresses                         ing          extensions                     true         Ingress
replicasets                       rs           extensions                     true         ReplicaSet
priorityclasses                   pc           scheduling.k8s.io              false        PriorityClass
storageclasses                    sc           storage.k8s.io                 false        StorageClass
...... 省略大部分

```

* * *

* * *

* * *

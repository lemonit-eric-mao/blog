---
title: "向量数据库调研"
date: "2023-12-13"
categories: 
  - "milvus"
---

# 向量数据库调研

## 整体流程

[![](http://qiniu.dev-share.top/image/milvus/milvus-00.jpg)](http://qiniu.dev-share.top/image/milvus/milvus-00.jpg)

## 选型

- 性能评估
- 符合业务
- 数据容量
- 数据容量增长对应的搜索速度
- 数据的形状。
    
    - 数据的形状是指开发者计划在向量数据库中存储和使用的向量数量和向量维度。
    - 在设计性能测试时，数据形状可以影响系统的性能。
- 灵活、可扩展
- 负载压测

## 向量数据库的使用情况是什么？

> 事实上，向量数据库在大型语言模型（LLM）的宣传开始之前就已经存在了。 最初，它们被应用于推荐系统中，因为它们可以`快速找到`给定查询的`相似对象`。 但是，由于它们可以`为大型语言模型提供长期记忆`，因此最近也被应用于问答应用程序中。

## 向量数据库是如何工作的？

> 向量数据库能够快速检索查询中的类似对象，因为它们已经预先计算过了。其基本概念被称为`近似最近邻`（Approximate Nearest Neighbor：`ANN`）搜索，它使用不同的算法来索引和计算相似性。 近似最近邻算法（Approximate Nearest Neighbor, ANN）则是**`一种通过牺牲精度来换取时间和空间的方式从大量样本中获取最近邻的方法`**，并以其存储空间少、查找效率高等优点引起了人们的广泛关注。 严格地讲，ANN 是一种在NN 搜索过程中允许少量误差的算法。 http://www.cs.umd.edu/~mount/ANN/ [![](http://qiniu.dev-share.top/image/milvus/milvus-01.gif)](http://qiniu.dev-share.top/image/milvus/milvus-01.gif)

## 对比ES

| 特点 | 倒排索引（ES数据库） | 向量数据库 |
| --- | --- | --- |
| 数据类型和查询类型 | 用于文本搜索和信息检索领域，处理文本文档或半结构化文本数据，支持全文搜索、分析和相关性排名等查询类型 | 用于处理数值向量数据，支持相似性搜索、聚类、分类等查询类型 |
| 数据存储结构 | 使用倒排索引数据结构，将文档中的词语映射到文档ID或位置信息上，以便进行文本搜索和相关性计算 | 使用专门的数据结构来存储向量数据，以便快速检索和计算向量之间的相似度 |
| 查询方法 | 查询涉及词语匹配、相关性计算和分词等文本处理操作 | 查询涉及向量之间的距离计算，以找到最相似的向量 |
| 应用领域 | 用于搜索引擎、文本检索、日志分析、内容推荐等需要处理文本数据的应用领域 | 用于图像搜索、人脸识别、语音识别、推荐系统中的向量召回等需要处理数值向量数据的应用领域 |

# Milvus

> Milvus 于 2019 年创建，其目标只有一个：存储、索引和管理由深度神经网络和其他机器学习 (ML) 模型生成的大量[嵌入向量](https://milvus.io/docs/overview.md#Embedding-vectors)。 作为专门为处理输入向量查询而设计的数据库，它能够对万亿规模的向量进行索引。与现有的关系数据库主要处理遵循预定义模式的结构化数据不同，Milvus 是自下而上设计的，旨在处理从[非结构化数据](https://milvus.io/docs/overview.md#Unstructured-data)转换而来的嵌入向量。 **视频资料：**https://www.bilibili.com/video/BV1aF411k7u2/?spm\_id\_from=333.337.search-card.all.click&vd\_source=9aecca5c508878bfdf0acab334276c39

## Milvus 工作流程

[![](http://qiniu.dev-share.top/image/milvus/milvus-02.jpg)](http://qiniu.dev-share.top/image/milvus/milvus-02.jpg)

## Milvus 架构

[![](http://qiniu.dev-share.top/image/milvus/milvus-03.jpg)](http://qiniu.dev-share.top/image/milvus/milvus-03.jpg)

#### Milvus 架构的概览

> 1. **Vector Search Libraries**：
>     - Milvus 构建在一些流行的向量搜索库之上，包括 Faiss、Annoy、HNSW 等。这些库提供了高效的向量索引和搜索算法，用于处理密集向量数据集的相似性搜索。
> 2. **目标**：
>     - Milvus 的设计目标是用于在包含数百万、数十亿甚至数万亿向量的密集向量数据集上执行相似性搜索。它适用于需要高度扩展性和性能的场景。
> 3. **功能**：
>     - Milvus 提供了多种高级功能，包括数据分片、数据持久性、流数据摄入、向量和标量数据之间的混合搜索等。这些功能使其适用于各种检索场景，并可以根据需求进行优化。
> 4. **部署建议**：
>     - 建议使用 Kubernetes 部署 Milvus，以实现最佳的可用性和弹性。Kubernetes 是一个容器编排工具，可以用于管理和自动化应用程序的部署和扩展。
> 5. **架构**：
>     - Milvus 采用了共享存储架构，具有存储和计算分离以及水平可伸缩性。它包括四个独立的层级：访问层、协调服务、工作节点和存储。这些层级在扩展或灾难恢复时相互独立。
> 
> 根据Milvus的架构概述和Faiss的知识对比这两者：
> 
> 1. **用途**：
>     - Milvus：Milvus是一个专门用于向量检索的向量数据库，旨在处理大规模的向量数据集，并提供了数据分片、数据持久性、流数据摄入、混合搜索等高级功能。
>     - Faiss：Faiss是一个向量索引库，专注于高效的向量相似性搜索。它是一个库而不是一个完整的数据库，开发者可以使用它来构建自己的向量检索引擎。
> 2. **架构**：
>     - Milvus：Milvus采用共享存储架构，具有存储和计算分离以及水平可伸缩性。它包括多个层级，以支持高可用性和灵活性。
>     - Faiss：Faiss是一个库，通常需要与其他数据存储系统集成使用，因此它没有自己的存储和计算架构。
> 3. **功能**：
>     - Milvus：Milvus提供了更多的功能，包括数据分片、数据持久性、流数据摄入、混合搜索等。它还支持标量数据和向量数据的混合查询。
>     - Faiss：Faiss专注于向量索引和相似性搜索算法，不提供数据库功能，仅专注于向量的检索。
> 4. **部署**：
>     - Milvus：建议使用Kubernetes部署Milvus以实现最佳的可用性和弹性。
>     - Faiss：Faiss通常需要开发者自行集成到其应用程序中，没有特定的部署建议。

### 架构升级

[![](http://qiniu.dev-share.top/image/milvus/milvus-04.png)](http://qiniu.dev-share.top/image/milvus/milvus-04.png)

### Upsert

> 如果不存在就插入`Insert`，如果存在就更新`Updata` **实现原理**：
> 
> - 底层实现原理也是采用了**`先删除 后插入`**的做法，但是，帮助了用户解决了一致性的问题，保证了都是在同一个事务里面进行处理
>     
> - Milvus是`WAL架构`,如果Delete太多会影响到`查询性能`和`稳定性`的，因为删除数据是在`Compaction`之后才进提交的，在这之前都是`标记删除`。
>     
> - 所以很多用户会出现说为什么删除数据之后磁盘空间还没有释放，是因为删除的数据`只是加了一行删除记录`，需要等到`Compaction`之后才会`标记删除`，然后在等一次GC之后被`标记删除`的数据才会被删掉，至少要等`24小时`。当然这个参数是可以调整的。
> 
> **`所以 Upsert不要滥用`** [![](http://qiniu.dev-share.top/image/milvus/milvus-05.png)](http://qiniu.dev-share.top/image/milvus/milvus-05.png)

### RangeSearch

> **场景**：
> 
> - 绝大部分的向量数据库都只提供了`Top k`这样的查询能力，但是有些用户反应 Top k 并不是我想要的，我想要的是**`在某一个距离范围内的数据都返回给我`**，我不知道这个距离范围内有多少个，我都查出来，当成一个聚类的用法来使用
>     
> - 这里要注意`radius`和`range_filter`的使用
>     
> - [![](http://qiniu.dev-share.top/image/milvus/milvus-06.png)](http://qiniu.dev-share.top/image/milvus/milvus-06.png)
>     
>     - #### radius
>         
>     - 表示半径范围内的数据都会拿出来
>         
>     - #### range\_filter
>         
>     - 表示范围之外的我就不要了，它与`radius`配合使用
>         

### Iterator

> **场景**：
> 
> - 比如说要查询`Top 10万`的结果，这无论是对 Milvus还是对客户端来说都是非常大的，所以需要分批次进行返回。

### ScaerNN Index

> [![](http://qiniu.dev-share.top/image/milvus/milvus-07.png)](http://qiniu.dev-share.top/image/milvus/milvus-07.png)

### MMap

- Linux MMap，在Linux系统中，对于成本敏感的应用程序，通常需要加载大量数据以提高性能。为了实现这一目标，数据首先会被写入磁盘，然后在需要时通过MMap将硬盘上的数据映射到内存中以进行搜索。这样做的好处是磁盘可以容纳更多数据，但由于磁盘和内存之间的数据交换会引入一些弹性和性能损失。

### Growing索引

![milvus-08](http://qiniu.dev-share.top/image/milvus/milvus-08.png)

- 因为数据是分为流式数据和持久化好的数据，在之前流式数据是没有索引的，持久化好的数据是建好索引的，所以之前流式数据检索速度慢，在新的版本里对流式数据增加了索引能力。
- 图中的每一个点就是一个Record，"Record是1" 表示向量近似检索的质量非常高，即查询向量与某个数据库中的向量非常相似，相似度接近100%。
- 横坐标是写入的频率，也就是写入多少行；纵坐标是每秒查询数
- 通常情况下，QPS 数值越大表示系统的查询处理速度越快，系统能够更有效地处理更多的查询或请求。而 "Record" 的值越接近 1 表示查询结果与查询向量越相似，也就是查询的准确度越高。所以，一般情况下，高 QPS 和接近 1 的 "Record" 值都被认为是良好的指标。

### 优化复杂过滤条件性能

> - 因为`Milvus`是`标量`、`向量`混合查询，它是先查一遍标量，把要过滤的数据的ID记下来，然后在交给向量的索引去检索。
>     
> - 在之前的版本里，如果标量里面过滤的数据太多，交给索引的时候就会出现扫描的数据极具变大，新的版本里进行了优化
>     

## 稳定性-新的 Load Balancer

- A1.SDK发送请求到proxy的scheduler
- A2.Scheduler 找LB Policy选择最佳的QN
- A3.LB Policy 找当前的Balancer方案，选择一个最佳的QN
- A4.Look Aside Blancer返回最佳QN
- A5.LB Policy 返回最佳QN
- A6.Scheluder向QN发起查询
- A7.QN返回查询结果
- A8.Proxy返回查询结果

[![](http://qiniu.dev-share.top/image/milvus/milvus-09.png)](http://qiniu.dev-share.top/image/milvus/milvus-09.png)

移出 CentOS 7 支持，目前支持 Ubuntu

* * *

* * *

* * *

# 调研 Milvus

### 索引相关

> 再数据库迁移的过程中发现，相同的2个Milvus数据库迁移后查询出的结果不一样，怀疑这是影响检索数据质量的关键，要验证

1. 先`确认`能否`修改索引参数`？
    
    - 修改索引的相关问题确认，Milvus团队回复，2.3版本不支持显示修改索引需要使用python SDK 进行修改，修改索引的功能需要等到2.4版本，版本发布时间大约在2024年2月份左右
    - ChatChat-0.2.8`不支持动态修改`Milvus索引参数，已经与ChatChat团队沟通，下一版本`0.2.9`会添加此功能。
2. `确认索引的重新创建`，是否会对`查询质量`有关？
    
    - 尝试使用`Attu`(阿图)图形化工具，对向量索引进行删除、重建，然后使用相同的检索向量进行检索，发现检索到的`结果没有任何变化`。
    - 从结果上可以确认，**`向量索引`的`删除`与`重建`不会影响到检索结果，但会影响到`检索速度`**。
3. 确认文件覆盖上传会不会对向量数据有影响？
    
    - 从工具中可以看到，它并不会覆盖之前的向量，它是又`新增了相同的向量`，但主键不同。
    - 从结果上向量的差异并不大
4. 相同的文件分别上传到faiss和milvus向量结果的相似度是否相同？
    
    - 从实际结果来看，差距非常小
    - ```
        0.7144690155982971
        0.7144688963890076
        ```
        
5. 使用相同的文件分别上传到相同的2个milvus数据库中检索结果是否一致？
    
    - 从实际结果来看，完全一致。

### 分区、分段相关

- `Partition`只为增强检索性能设计
- `Segment`是Milvus`自动创建`的用于保存插入数据的数据文件。

#### **场景**

> `问题`： 假如我有`图像`、`文本`和`音频`3种不同类型的文件要存入数据库中
> 
> 1. 存放在一个collection里面创建 图像、文本和音频 3个Partition
> 2. 创建图像、文本和音频3个Collection，把数据存放在不同的Collection里面
> 
> 我想通过这个例子了解Milvus 对Partition的设计理念和推荐的做法是什么？ `社区回答`： embedding的维度不同的话，只能用不同collection来做
> 
> * * *
> 
> `问题`： 接着问一个场景，公司有3个部门，期望每个部门的资料数据是隔离，然后还有一个监管部门它希望能够监控每个部门的数据，这种场景下我怎么做才好？ 一个Collection 3个Partition？ 还是3个Collection? `社区回答`： 推荐用3个collection `问题`： 那监管部门它希望能够一次检索每个部门的数据，这个我应该如何实现呢？ `社区回答`： 使用collection，每次就只能搜一个collection。使用partition，一次就可以搜多个，具体用哪种方式，根据你自己的应用场景来决定哈。 `问题`： 我这种场景大概率会使用partition这个做法了，因为现阶段经验尚浅，不知道我这个做法会不会有什么隐患，比如后期每个部门的数据量变大，是不是就没有再调优的空间了。或者有哪些我没有想到的，希望您能指导一下。 `社区回答`： 我觉得还是用3个collection。如果用同一张表，就意味着这三个部门的向量维度必须得一样，使用相同的索引。 如果三个部门的向量维度不一样，就必须得用三张表。如果想搜三个部门的数据，那就每张表各搜一次，然后自己把结果根据自己的规则合并一下吧 这个一般是用RBAC吧，在Milvus官方文档里搜RBAC 【[将 RBAC 与数据库结合使用](https://milvus.io/docs/manage_databases.md#Use-the-RBAC-with-database)】 【[多租户策略](https://milvus.io/docs/multi_tenancy.md#Multi-tenancy-strategies)】
> 
> |  | 数据隔离方式 | 搜索性能 | 最大租户数量 |
> | :-- | :-- | :-- | --- |
> | 面向数据库的 | 强 | 强 | 64 |
> | 所有租户共用一个集合 | 弱 | 中等 | 不适用 |
> | 每个租户一个集合 | 强 | 强 | 65536 |
> | 每个租户一个分区 | 中等 | 强 | 4096 |
> | 基于分区键的 | 中等 | 强 | 不适用 |
> 
> `问题`： 一开始想的也是使用3个Collection的做法，但是没想好如何满足同时检索的需求，然后去看了Partition，看完以后感觉与关系型数据库的设计还是有些不同的用法， 关系数据库会受表的约束，Partition受到向量维度的影响 如果我现在从技术实现难易度的角度来考虑，还是多个Partition比较容易实现，但是未来如果文件过大要如果优化的问题，我还不能解决，这是我的顾虑 所以现在更倾向使用3个Collection的做法，虽然技术上实现会有些难度，我还是认为应该使用技术复杂换取程序稳定运行 当然，如果多Partition的顾虑是多余的，有其它的办法来解决，那就很好选择了。 `社区回答`： 只能用3个Collection， 因为Milvus权限控制的最小粒度是Collection级别
> 
> * * *
> 
> `问题`： 请教一个问题，segment大小更改后，原collection需要重新导入数据？ `社区回答`： 改大，重启之后它会把小的合成大的，只要满足合并条件的就能合。 改小，重启之后，原先的segment不会有变化，但对新的segment会按小的size来生成。 没必要要重导数据。
